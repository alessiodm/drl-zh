{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Based Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-Based Reinforcement Learning (MBRL) is a paradigm within RL where an agent first learns a\n",
    "model of the environment's dynamics. This learned model, often denoted as $p(s_{t+1},r_tâˆ£s_t, a_t)$,\n",
    "approximates the real world by predicting the next state $s_{t+1}$ and reward $r_t$ given the\n",
    "current state $s_t$ and an action $a_t$. The agent can then use this internal model to simulate\n",
    "experiences and plan its actions, often |without needing to interact with the actual environment.\n",
    "\n",
    "Even when direct interaction with an environment is possible, MBRL offers several key advantages\n",
    "that make it a powerful approach. By learning a model, an agent can overcome limitations of\n",
    "real-world interaction and unlock more sophisticated decision-making capabilities.\n",
    "\n",
    "- **Sample Efficiency**: Interacting with the real world can be slow, expensive, or dangerous. MBRL\n",
    "  agents can use their learned model to generate vast amounts of simulated data, drastically\n",
    "  reducing the number of real-world samples needed to learn an effective policy.\n",
    "\n",
    "- **Planning and Deliberation**: The learned model enables the use of powerful planning algorithms\n",
    "  (e.g., Monte Carlo Tree Search). The agent can \"look ahead\" and simulate the outcomes of different\n",
    "  action sequences to find an optimal plan before executing a single action in reality.\n",
    "\n",
    "- **Safety and Risk Management**: Before trying actions in the real world, an agent can use its\n",
    "  model to predict and avoid potentially catastrophic outcomes. This is critical for applications\n",
    "  like robotics or autonomous vehicles where mistakes are costly.\n",
    "\n",
    "- **Transfer Learning**: A model captures the underlying physics of an environment. This knowledge\n",
    "  can often be transferred or fine-tuned for new tasks within the same environment, allowing the\n",
    "  agent to adapt more quickly than learning a new policy from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers.vector import NormalizeObservation\n",
    "\n",
    "from util.rl_algos import SAC, AgentSAC, ReplayBuffer\n",
    "from util.gymnastics import DEVICE, gym_simulation, init_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Environment\n",
    "\n",
    "Let's experiment with model-based reinforcement learning using the MuJoCo\n",
    "[Half-Cheetah](https://gymnasium.farama.org/environments/mujoco/half_cheetah/) environment. Let's\n",
    "start with some utility functions.\n",
    "\n",
    "We use the [vectorized version](https://gymnasium.farama.org/api/vector/) of the environment to\n",
    "speed up and stabilize training. Moreover, we\n",
    "[normalize the observations](https://gymnasium.farama.org/api/vector/wrappers/#gymnasium.wrappers.vector.NormalizeObservation),\n",
    "which is critical for learning an effective dynamics model of the environment as required by MBPO.\n",
    "Because of that, we need to make sure to store and reuse the running mean and standard deviation\n",
    "statistics.\n",
    "\n",
    "**NOTE:** This notebook is computationally intensive, expect $O(hours)$ of GPU training for both the\n",
    "model-free (SAC) and especially the MBPO training. Consider lowering the `SOLVED_SCORE` below to get\n",
    "faster experimentation loops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"HalfCheetah-v5\"\n",
    "SOLVED_SCORE = 5999.0  # State of the art algorithms often go beyond this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vectorized_env(num_envs=15):\n",
    "    \"\"\"Utility method to create the vectorized environment with observation normalization.\"\"\"\n",
    "    env = gym.make_vec(ENV_NAME, num_envs=num_envs)\n",
    "    env = NormalizeObservation(env)\n",
    "    return init_random(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(agent: AgentSAC = None, obs_rms: np.ndarray = None):\n",
    "    \"\"\"Utility to run an agent simulation restoring normalization statistics.\"\"\"\n",
    "    sym_env = gym.wrappers.NormalizeObservation(gym.make(ENV_NAME, render_mode=\"rgb_array_list\"))\n",
    "    if obs_rms is not None:\n",
    "        sym_env.obs_rms = obs_rms\n",
    "    sym_env.update_running_mean = False\n",
    "    return gym_simulation(sym_env, agent, max_t=150 if agent is not None else 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(text: str):\n",
    "    \"\"\"Extremely naive utility to overwrite the same console line for logging in this notebook.\"\"\"\n",
    "    padding = 100 - len(text)\n",
    "    print(f\"\\r{text}{' ' * padding}\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the state / action dimensions, and run a simulation of a random agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ENV = gym.make(ENV_NAME)\n",
    "STATE_SIZE = TEST_ENV.observation_space.shape[0]\n",
    "ACTION_SIZE = TEST_ENV.action_space.shape[0]\n",
    "\n",
    "write(f\"State size: {STATE_SIZE}\\n\")\n",
    "write(f\"Action size: {ACTION_SIZE}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random agent simulation.\n",
    "run_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with SAC\n",
    "\n",
    "Let's first train a _model-free_ SAC agent. The SAC algorithm and agent implementations are provided\n",
    "as part of the utility code, but you already have built them as part of the actor-critic notebook :)\n",
    "\n",
    "We will hopefully be able to train the same agent in way less episodes (i.e., better sample\n",
    "efficiency) using MBPO at the end of this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sac_agent():\n",
    "    \"\"\"Creates a SAC agent, used in both SAC and MBPO training.\"\"\"\n",
    "    return AgentSAC(\n",
    "        STATE_SIZE,\n",
    "        ACTION_SIZE,\n",
    "        sample_size=512,\n",
    "        max_norm=1.0,\n",
    "        lr_actor=1e-3,\n",
    "        lr_critic=1e-3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't want to run the full SAC training, you can load scores from a prerun, and comment the\n",
    "# SAC training (and simulation) below. We will need the scores for plotting later on.\n",
    "with open(\"solution/mbrl_sac_scores.pkl\", \"rb\") as file:\n",
    "    sac_scores = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solved in ~1h30m minutes GPU\n",
    "sac_env = make_vectorized_env()\n",
    "sac_agent = make_sac_agent()\n",
    "sac_scores = SAC(sac_env, sac_agent, solved_score=SOLVED_SCORE).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_simulation(sac_agent, sac_env.obs_rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Based Policy Optimization (MBPO)\n",
    "\n",
    "Modern MBRL can be roughly divided into two major approaches, which differ in how they use the\n",
    "learned model.\n",
    "\n",
    "- **Dyna-Style Algorithms (Background Planning)**: The core idea is to the learned model to generate\n",
    "  more data to train a model-free RL algorithm. The model acts as a data augmentation engine.\n",
    "- **Planning-Based Algorithms (Decision-Time Planning)**: Use the learned model to plan or search\n",
    "  for the best possible actions at each and every timestep. The model acts as an internal simulator\n",
    "  for decision-making.\n",
    "\n",
    "In this lesson, we will put these concepts into practice by implementing a simplified version of\n",
    "Model-Based Policy Optimization (MBPO), a popular Dyna-style and effective MBRL algorithm\n",
    "([paper here](https://arxiv.org/abs/1906.08253), and a\n",
    "[reference implementation](https://github.com/Xingyu-Lin/mbpo_pytorch)).\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "The vanilla model-based RL algorithm can be described as for $N$ epochs: (1) collect data under the\n",
    "current policy, (2) learn dynamics model from past data, (3) improve the policy by using the learnt\n",
    "dynamics model (either via backprop through time or using the model as a simulator). You can see at\n",
    "a very high-level how the MBPO algorithm reflects those steps:\n",
    "\n",
    "<div style=\"width: 50%\">\n",
    "  <img src=\"assets/13_MBRL_MBPO_algorithm.png\">\n",
    "  <br>\n",
    "  <small></small>\n",
    "</div>\n",
    "\n",
    "In particular, MBPO works by creating a cycle between real interaction, model learning, and policy\n",
    "improvement. The agent maintains **two separate replay buffers**: one for real experiences collected\n",
    "from the environment and another for imaginary experiences generated by a model. First, an\n",
    "**ensemble of dynamics models** is trained on the real data to predict next states and rewards while\n",
    "capturing uncertainty. Then, the algorithm performs **short, imaginary rollouts** by sampling states\n",
    "from the real buffer, using the current policy to choose actions, and using a randomly selected\n",
    "model from the ensemble to predict the outcomes. These simulated transitions populate the model\n",
    "buffer. Finally, a powerful model-free agent (SAC in our case), is trained on mixed batches of data\n",
    "drawn from both the real and model buffers, allowing it to learn with far greater sample efficiency\n",
    "than from real interactions alone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Function\n",
    "\n",
    "The reward function _should be_ part of the dynamics model. But in this notebook, we **manually\n",
    "define the reward function**, a common and practical approach when the reward can be calculated\n",
    "directly from the state. This simplifies the agent's task, allowing the dynamics model to focus\n",
    "solely on predicting the next state.\n",
    "\n",
    "However, in many real-world problems, this isn't possible. An agent may only have partial\n",
    "observations and must learn the reward function alongside the dynamics. This introduces the critical\n",
    "challenge of **reward hacking**, where the agent exploits flaws in its learned reward model to\n",
    "achieve high scores that don't reflect true task success, a risk that is especially high in\n",
    "environments with sparse or complex rewards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_fn(action: np.ndarray, next_state: np.ndarray) -> float:\n",
    "    \"\"\"Reward function for HalfCheetah-v5.\n",
    "\n",
    "    from: https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/mujoco/half_cheetah_v5.py\n",
    "    \"\"\"\n",
    "    # TODO: The x_velocity is the 9th component (at index 8) of next_state\n",
    "    x_velocity = None\n",
    "    # TODO: action_penalty is 0.1 (ctrl_cost_weight) times the sum of the squared action components.\n",
    "    action_penalty = None\n",
    "    # TODO: The reward is the x_velocity minus the action_penalty\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running tests for reward_fn...\")\n",
    "\n",
    "action1 = np.array([0.0, 0.0, 0.0])\n",
    "state1 = np.array([0, 0, 0, 0, 0, 0, 0, 0, 5.0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "expected_reward1 = 5.0\n",
    "assert np.isclose(reward_fn(action1, state1), expected_reward1), \"Test Case 1 Failed\"\n",
    "print(\"âœ… Test Case 1 Passed\")\n",
    "\n",
    "action2 = np.array([1.0, -2.0, 0.5])\n",
    "state2 = np.array([0, 0, 0, 0, 0, 0, 0, 0, -2.0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "expected_reward2 = -2.525\n",
    "assert np.isclose(reward_fn(action2, state2), expected_reward2), \"Test Case 2 Failed\"\n",
    "print(\"âœ… Test Case 2 Passed\")\n",
    "\n",
    "action3 = np.array([-1.0, -1.0, -1.0, -1.0])\n",
    "state3 = np.zeros(17)\n",
    "expected_reward3 = -0.4\n",
    "assert np.isclose(reward_fn(action3, state3), expected_reward3), \"Test Case 3 Failed\"\n",
    "print(\"âœ… Test Case 3 Passed\")\n",
    "\n",
    "print(\"ðŸŽ‰ All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_env_dataset(env: gym.vector.VectorEnv, env_dataset: ReplayBuffer, num_samples=150_000):\n",
    "    \"\"\"Initializes the environment dataset by interacting randomly with the environment.\"\"\"\n",
    "    # TODO: Clear the env_dataset.\n",
    "    # ...\n",
    "    # TODO: reset the gym vectorized environment.\n",
    "    states, _ = None\n",
    "    for _ in range(num_samples // env.num_envs):\n",
    "        # TODO: Sample a random action from the vectorized environment.\n",
    "        actions = None\n",
    "        # TODO: Perform a step using that action.\n",
    "        next_states, rewards, terms, truncs, _ = None\n",
    "        # TODO: Compute dones as logical or between terminations and truncations\n",
    "        dones = None\n",
    "        for i in range(env.num_envs):\n",
    "            # TODO: Add the tuple state, action, reward, next_state, done to the env_dataset.\n",
    "            pass\n",
    "        # TODO: Crucial! Set state to the next_state for the next iteration!\n",
    "        states = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env_dataset = ReplayBuffer()\n",
    "init_env_dataset(make_vectorized_env(), test_env_dataset, num_samples=1020)\n",
    "\n",
    "assert len(test_env_dataset) == 1020, f\"Incorrect number of samples: {len(test_env_dataset)}\"\n",
    "\n",
    "env_dataset_sample = test_env_dataset.sample(3)\n",
    "assert env_dataset_sample[0].shape == (3, 17), \"Incorrect state shape\"\n",
    "assert env_dataset_sample[1].shape == (3, 6), \"Incorrect action shape\"\n",
    "assert env_dataset_sample[2].shape == (3, 1), \"Incorrect reward shape\"\n",
    "assert env_dataset_sample[3].shape == (3, 17), \"Incorrect next_state shape\"\n",
    "assert env_dataset_sample[4].shape == (3, 1), \"Incorrect done shape\"\n",
    "\n",
    "print(\"ðŸŽ‰ All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Network\n",
    "\n",
    "The learnt dynamics model of the environment $f_{\\theta}$ is implemented as a neural-network which\n",
    "predicts the change in the _normalized_ state given the current (normalized) state and action:\n",
    "$\\hat{\\Delta}_{t+1} = f_{\\theta}(s_t, a_t)$. See [this paper](https://arxiv.org/abs/1708.02596) for\n",
    "an intuition of why it is better to predict deltas instead of the next states directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size=STATE_SIZE, action_size=ACTION_SIZE):\n",
    "        super().__init__()\n",
    "        # TODO: The network input is state + action and outputs the next state delta. Create three\n",
    "        #       linear layers with hidden dimension 256.\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # TODO: Concat state and action. Hint: use torch.cat and pay attention on the dimension!\n",
    "        #       First dimension (index 0) is always the batch dimension.\n",
    "        # ...\n",
    "        # TODO: Forward through the network to compute the `delta`\n",
    "        # ...\n",
    "        delta = None\n",
    "        # TODO: Also compute the next_state as: state + delta\n",
    "        new_state = None\n",
    "        return new_state, delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Model\n",
    "\n",
    "MBPO utilizes an ensemble of probabilistic dynamics models rather than relying on a single one. This\n",
    "approach is crucial for managing model uncertainty; a single learned model might be confidently\n",
    "incorrect about the environment's physics, leading the agent to exploit these inaccuracies for\n",
    "illusory gains. To ensure the models in the ensemble are diverse, they are trained using\n",
    "**bootstrapping**.\n",
    "\n",
    "While this technique traditionally involves creating separate, fixed datasets for each model by\n",
    "_sampling with replacement_ from the main experience buffer, we implement a simplified version for\n",
    "efficiency: each model in the ensemble is trained on the full dataset, and diversity is achieved by\n",
    "having each model sample different, random mini-batches during each training step. This ensures each\n",
    "model sees a unique sequence of data, which effectively approximates the bootstrapping process.\n",
    "\n",
    "Let's create the `EnsembleModel` first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, n_models=7, state_size=STATE_SIZE, action_size=ACTION_SIZE, lr=3e-4):\n",
    "        super().__init__()\n",
    "        self.n_models = n_models\n",
    "        # TODO: Create an ensemble of n_models ModelNetwork. Hint: use nn.ModuleList.\n",
    "        self.models = None\n",
    "        # TODO: Create n_models separate optimizers in a list.\n",
    "        self.optimizers = [None]\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # TODO: Compute the deltas from all the models.\n",
    "        models_deltas = None\n",
    "        return models_deltas\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, state, action):\n",
    "        # TODO: Select a random ModelNetwork from the ensemble.\n",
    "        selected_model_in_ensamble = None\n",
    "        # TODO: Use the selected model to perform the prediction.\n",
    "        next_state, delta_pred = None\n",
    "        return next_state, delta_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then write the training loop, using holdouts for cross-validation and a _patience_ threshold\n",
    "to interrupt the training when no more improvements are made.|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predictive_model(\n",
    "    model: EnsembleModel,\n",
    "    env_dataset: ReplayBuffer,\n",
    "    n_epochs=100,\n",
    "    mini_batch_size=256,\n",
    "    gradient_steps=390,\n",
    "    holdout_ratio=0.1,\n",
    "    patience=5,\n",
    "):\n",
    "    # TODO: Make sure to set the model in training mode.\n",
    "    # ...\n",
    "    # TODO: Get all samples from the env_dataset\n",
    "    all_samples = None\n",
    "    # TODO: Get the total number of samples. Hint: use the shape.\n",
    "    total_samples = None\n",
    "    # TODO: Compute the number of holdouts as the product between total_samples and holdout_ratio.\n",
    "    n_holdout = None\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_since_improvement = 0\n",
    "\n",
    "    # TODO: Split data into training and validation tensors for easier indexing\n",
    "    train_tensors = None\n",
    "    val_samples = None\n",
    "    train_states, train_actions, _, train_next_states, _ = None\n",
    "    n_train_samples = None\n",
    "\n",
    "    write(f\"Performing model rollout... training model...\")\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        for _ in range(gradient_steps):\n",
    "            # Instead of one sequential batch, we train each model on its own random batch\n",
    "            for i in range(model.n_models):\n",
    "                # TODO: Create a bootstrap sample (mini_batch_size random indices). Hint: use\n",
    "                #       np.random.randint\n",
    "                indices = None\n",
    "\n",
    "                # TODO: select the various training batches using the indices.\n",
    "                state_batch = None\n",
    "                action_batch = None\n",
    "                next_state_batch = None\n",
    "\n",
    "                # TODO: Predict with the specific model\n",
    "                _, delta_pred = None\n",
    "                delta_target = None\n",
    "\n",
    "                # TODO: Calculate loss and update the specific model's optimizer. Use mse_loss.\n",
    "                loss = None\n",
    "                # TODO: Perform an optimization step.\n",
    "                # ...\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "        # Validation loss\n",
    "        with torch.no_grad():\n",
    "            # TODO: Unpack the cross-validation samples.\n",
    "            val_state, val_action, val_next_state, _, _ = None\n",
    "            # TODO: Get the delta target as val_next_state - val_state\n",
    "            val_target = None\n",
    "            # TODO: Run the prediction using the ensemble.\n",
    "            val_preds = None\n",
    "            # TODO: Adjust val_target dimensionality to match val_preds\n",
    "            val_target = None\n",
    "            # TODO: Compute MSE loss.\n",
    "            val_loss = None\n",
    "\n",
    "        epoch_loss /= gradient_steps\n",
    "        write(f\"Model Epoch {epoch} | Train Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            # TODO: save the best_val_loss, and reset the epochs_since_improvement to zero.\n",
    "            pass\n",
    "        else:\n",
    "            # TODO: Increment epochs_since_improvement, and break if it is greater than patience.\n",
    "            pass\n",
    "\n",
    "    write(f\"Model training complete (up to {epoch} epochs). Final Val Loss: {val_loss:.4f}\")\n",
    "    # TODO: Put model back into eval() mode.\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test whether some training at least happens :)\n",
    "train_predictive_model(EnsembleModel().to(DEVICE), test_env_dataset, n_epochs=2)\n",
    "print(\"\\nâœ… Training ran with no runtime errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Model Rollout\n",
    "\n",
    "Let's implement the _k-step model rollout_ using the predictive model to populate the simulated\n",
    "experiences buffer. Let's make sure to use normalized observations when feeding the model, and\n",
    "unnormalize when calculating rewards and storing in the buffer.\n",
    "\n",
    "Note that in MBPO short model rollouts are crucial to mitigate the accumulation of prediction errors\n",
    "from the imperfect learned dynamics model, which prevents the policy from exploiting these\n",
    "inaccuracies over long horizons. In our training, we keep $k=1$, though you can experiment with\n",
    "adaptive approaches that increase it as training proceeds and the model is more robust and accurate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_step_model_rollout(\n",
    "    predictive_model: EnsembleModel,\n",
    "    agent: AgentSAC,\n",
    "    model_dataset: ReplayBuffer,\n",
    "    start_state: torch.Tensor,  # Dimension (1, STATE_SIZE)\n",
    "    obs_rms_mean: np.ndarray,\n",
    "    obs_rms_var: np.ndarray,\n",
    "    k_steps=1,\n",
    "):\n",
    "    # TODO: Set the current state to the start_state.\n",
    "    current_state = None\n",
    "    # TODO: Convert running mean / std into tensors.\n",
    "    obs_rms_mean_t = None\n",
    "    obs_rms_var_t = None\n",
    "\n",
    "    for _ in range(k_steps):\n",
    "        # TODO: Convert the current state into numpy() and get a single action from the agent.\n",
    "        state_np = None\n",
    "        action_np = None\n",
    "\n",
    "        # TODO: Convert the action into a tensor. Make sure to add the batch dimension (unsqueeze).\n",
    "        action = torch.from_numpy(action_np).float().unsqueeze(0).to(DEVICE)\n",
    "        # TODO: Call the predictivel model step(...)\n",
    "        next_state, _ = None\n",
    "\n",
    "        # TODO: Compute the unnormalized next state using obs_rms_var_t and obs_rms_mean_t.\n",
    "        unnormalized_next_state = None\n",
    "        # TODO: Compute the reward invoking the manually written reward function.\n",
    "        reward = None\n",
    "        done = False  # We never terminate with HalfCheetah.\n",
    "\n",
    "        # Convert state and next_state back to numpy, making sure to remove the batch dimension!\n",
    "        state_to_add = None\n",
    "        next_state_to_add = None\n",
    "        # TODO: Add the tuple (state, action, reward, next_state, done) to the model_dataset\n",
    "        # ...\n",
    "\n",
    "        current_state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MBPO Algorithm\n",
    "\n",
    "Finally, let's code the MBPO algorithm! In this implementation, we make a couple of slight\n",
    "modifications from the algorithm described in the paper:\n",
    "\n",
    "1.  We use global timesteps, not epochs directly; both the predictive model training _and_ the\n",
    "    k-step model rollout happen once every `train_model_freq`;\n",
    "2.  We clear the model dataset after predictive model training, to ensure the policy is always\n",
    "    learning from rollouts generated by the most up-to-date dynamics model;\n",
    "3.  The start state for the k-step model rollout is sampled from the real environment, rather than\n",
    "    uniformly from the env dataset (to have always \"true\" start states).\n",
    "4.  We update the policy parameters using a mixture of model / synthetic and real data.\n",
    "\n",
    "And now we are ready to go!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MBPO(\n",
    "    env: gym.vector.VectorEnv,\n",
    "    agent: AgentSAC,\n",
    "    n_episodes=900,\n",
    "    n_env_steps=1_000,\n",
    "    n_model_rollouts=400,\n",
    "    n_grad_updates=25,\n",
    "    train_model_freq=250,\n",
    "    train_model_epochs=10,\n",
    "    policy_train_batch_size=256,\n",
    "    real_ratio=0.25,\n",
    "    k_steps=1,\n",
    "    solved_score=SOLVED_SCORE,\n",
    "):\n",
    "    assert real_ratio < 1.0, \"1.0 would mean degenerate into SAC. Implement tweaks as exercise :)\"\n",
    "\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    num_envs = env.num_envs\n",
    "    total_timesteps = n_episodes * n_env_steps\n",
    "    episode_scores = np.zeros(num_envs, dtype=np.float32)\n",
    "\n",
    "    states, _ = env.reset()\n",
    "    env_dataset = ReplayBuffer()\n",
    "    model_dataset = ReplayBuffer(1e6)\n",
    "    predictive_model = EnsembleModel().to(DEVICE)\n",
    "\n",
    "    # TODO: ALGO: Initialization: call init_env_dataset. Initial exploration matters a lot!\n",
    "    # ...\n",
    "\n",
    "    for global_step in range(total_timesteps // num_envs):\n",
    "        if global_step % train_model_freq == 0:\n",
    "            # TODO: ALGO: Train predictive model on env_dataset via maximum likelihood.\n",
    "            #       Hint: call train_predictive_model :)\n",
    "            # ...\n",
    "\n",
    "            # TODO: Clear the model dataset.\n",
    "            # ...\n",
    "            # TODO: Get obs_rms.mean and obs_rms.var from the environment (thanks to the wrapper).\n",
    "            obs_rms_mean = None\n",
    "            obs_rms_var = None\n",
    "\n",
    "            for model_rollout in range(n_model_rollouts):\n",
    "                # TODO: Pick the start state randomly from `states`.\n",
    "                start_state_np = None\n",
    "                # TODO: Convert it to a tensor, adding the batch dimension (unsqueeze).\n",
    "                rollout_state = None\n",
    "\n",
    "                # TODO: ALGO: Perform k-step model rollout!\n",
    "                # ...\n",
    "                write(f\"Rollout {model_rollout} completed...\")\n",
    "\n",
    "        # TODO: ALGO: Take action in environment, according to policy; add to the env_dataset.\n",
    "        actions = None\n",
    "        next_states, rewards, truncateds, terminateds, _ = None\n",
    "        dones = None\n",
    "        for i in range(env.num_envs):\n",
    "            # TODO: Add to env_dataset here.\n",
    "            pass\n",
    "\n",
    "        # ALGO: Update policy parameters on model data (and percentage of real data).\n",
    "        write(f\"Step {global_step} in progress + gradient updates...\")\n",
    "        for _ in range(n_grad_updates):\n",
    "            # TODO: Compute model_batch_size using the real_ratio.\n",
    "            model_batch_size = None\n",
    "            # TODO: Compute env_batch_size as: policy_train_batch_size - model_batch_size\n",
    "            env_batch_size = None\n",
    "            # TODO: Sample the batch part from the env_dataset.\n",
    "            env_batch = None\n",
    "            # TODO: Sample the batch part from the model_dataset.\n",
    "            model_batch = None\n",
    "            # TODO: Compute the full batch concatenating env + model batches.\n",
    "            batch = None\n",
    "            # TODO: Call agent.learn!\n",
    "            # ...\n",
    "\n",
    "        # TODO: CRITICAL! Set states to next_states!\n",
    "        states = None\n",
    "        episode_scores += rewards\n",
    "\n",
    "        # No need to check for termination; HalfCheetah-v5 only truncates at the time limit, and\n",
    "        # vectorized environments handle the automatic reset. We check it only for logging.\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                finished_score = episode_scores[i]\n",
    "                scores.append(finished_score)\n",
    "                scores_window.append(finished_score)\n",
    "                episode_scores[i] = 0.0\n",
    "                avg_score = np.mean(scores_window)\n",
    "                num_episodes_done = len(scores)\n",
    "                write(\n",
    "                    (\n",
    "                        f\"Episode {num_episodes_done}\\tAverage Score: {avg_score:.2f}\"\n",
    "                        + (\"\\n\" if num_episodes_done % env.num_envs == 0 else \"\")\n",
    "                    ),\n",
    "                )\n",
    "                if avg_score >= solved_score:\n",
    "                    write(\n",
    "                        f\"\\nEnvironment solved in {num_episodes_done} episodes!\"\n",
    "                        + f\"\\tAverage Score: {avg_score:.2f}\"\n",
    "                    )\n",
    "                    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Train Our MBPO Agent!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbpo_env = make_vectorized_env()\n",
    "mbpo_agent = make_sac_agent()\n",
    "mbpo_scores = MBPO(mbpo_env, mbpo_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_simulation(mbpo_agent, mbpo_env.obs_rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison\n",
    "\n",
    "We can see the better sample efficiency of MBPO vs. SAC!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(sac_scores, mbpo_scores, window_size=20):\n",
    "    \"\"\"Utility method to plot.\"\"\"\n",
    "    sac_series = pd.Series(sac_scores, name=\"SAC\")\n",
    "    mbpo_series = pd.Series(mbpo_scores, name=\"MBPO\")\n",
    "    sac_running_avg = sac_series.rolling(window=window_size).mean()\n",
    "    sac_running_std = sac_series.rolling(window=window_size).std()\n",
    "    mbpo_running_avg = mbpo_series.rolling(window=window_size).mean()\n",
    "    mbpo_running_std = mbpo_series.rolling(window=window_size).std()\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.set_theme(style=\"darkgrid\")\n",
    "    plt.plot(sac_running_avg.index, sac_running_avg, label=\"SAC\", color=\"purple\")\n",
    "    plt.fill_between(\n",
    "        sac_running_avg.index,\n",
    "        sac_running_avg - sac_running_std,\n",
    "        sac_running_avg + sac_running_std,\n",
    "        alpha=0.2,\n",
    "        color=\"purple\",\n",
    "    )\n",
    "    plt.plot(mbpo_running_avg.index, mbpo_running_avg, label=\"MBPO\", color=\"orange\")\n",
    "    plt.fill_between(\n",
    "        mbpo_running_avg.index,\n",
    "        mbpo_running_avg - mbpo_running_std,\n",
    "        mbpo_running_avg + mbpo_running_std,\n",
    "        alpha=0.2,\n",
    "        color=\"orange\",\n",
    "    )\n",
    "    plt.title(\"Smoothed Training Curves with Standard Deviation\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Return/Score\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(sac_scores, mbpo_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "While MBPO is a powerful and foundational algorithm, the field of Model-Based Reinforcement Learning\n",
    "is vast. The key differences often lie in how the learned model is usedâ€”either for data augmentation\n",
    "(like MBPO) or for active planningâ€”and how model errors are handled.\n",
    "\n",
    "Here are several influential algorithms and how their core ideas differ from MBPO:\n",
    "\n",
    "- **PETS (Probabilistic Ensembles with Trajectory Sampling)**: Instead of augmenting a replay\n",
    "  buffer, PETS uses the model for active decision-time planning by simulating thousands of future\n",
    "  trajectories to pick the best action at every step, a method known as Model Predictive Control\n",
    "  (MPC).\n",
    "\n",
    "- **ME-TRPO (Model-Ensemble Trust Region Policy Optimization)**: Like MBPO, it uses a model ensemble\n",
    "  to train a policy, but it replaces the standard policy update (like SAC) with the more stable and\n",
    "  conservative TRPO algorithm to prevent performance collapse from model errors.\n",
    "\n",
    "- **MB-MPO (Model-Based Meta-Policy Optimization)**: This approach treats each learned model in an\n",
    "  ensemble as a separate task and uses meta-learning to find a policy that can quickly adapt to any\n",
    "  of their different dynamics, making it highly robust to model uncertainty.\n",
    "\n",
    "- **The Dreamer Family (DreamerV2, DreamerV3)**: This state-of-the-art method learns a world model\n",
    "  in a compact latent space (not the raw state space) and learns the policy entirely within this\n",
    "  imagined \"dream,\" making it incredibly effective for complex, image-based environments like video\n",
    "  games.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlzh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
