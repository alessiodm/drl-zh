{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with Human Feedback\n",
    "\n",
    "[Large Language Models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) have been at the\n",
    "forefront of the present AI summer, making the generative AI field explode with innovation in recent\n",
    "years.\n",
    "\n",
    "LLMs are trained on vast amount of data in a self-supervised fashion (i.e., next token prediction),\n",
    "but they require lots of fine-tuning for them to be \"user-friendly\" and behave / respond in a way\n",
    "that humans consider appropriate (e.g., do not\n",
    "[hallucinate](<https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)>)).\n",
    "\n",
    "Fine tuning LLMs generally happens in two stages:\n",
    "\n",
    "- **Supervised Fined-Tuning (SFT):** The pre-trained model is fine-tuned on a smaller, high-quality\n",
    "  dataset of labeled prompt-response pairs that demonstrate desired behaviors for specific tasks.\n",
    "  This helps the model become more useful and follow instructions.\n",
    "- **RLHF:** Human evaluators rank or provide preferences for different model outputs. The feedback\n",
    "  is used to train a \"reward model,\" which guides the LLM to generate responses that are highly\n",
    "  preferred by humans (e.g., addressing safety, nuance, creativity, etc.) Here is a\n",
    "  [seminal paper](https://arxiv.org/abs/1909.08593) on the topic.\n",
    "\n",
    "### RLHF Steps\n",
    "\n",
    "The starting point is to collect a \"preference dataset\", where human raters pick one LLM output\n",
    "preferred over another (or multiple ones). The preference dataset has tuples like:\n",
    "\n",
    "```\n",
    "(prompt, output_1, output_2, human_preference)\n",
    "```\n",
    "\n",
    "Then we use the preference dataset to train a _Reward Model_. The reward model is generally another\n",
    "LLM that takes a prompt and a completion, and outputs a _score_. Training the reward model is a hard\n",
    "task in and of itself, and in this notebook we will \"fake\" it using a hand-crafted reward function.\n",
    "\n",
    "At this point, we can introduce the reinforcement learning loop to fine-tune our original LLM! We\n",
    "need a second dataset containing many prompts (no completions!) which are analogous to \"environment\n",
    "steps\". Then, we feed the prompts to the model, which produces a completion, which is then scored\n",
    "via the reward model, and then weights are updated via PPO (or similar RL algorithm).\n",
    "\n",
    "![RLHF diagram](assets/14_RLHF_loop.excalidraw.png) <br><small>Reinforcement learning loop for LLM\n",
    "fine-tuning.</small>\n",
    "\n",
    "In this notebook, we will fine-tune a small language model to prefer telling stories about the\n",
    "animal kingdom!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary classes from Hugging Face transformers library\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    PreTrainedModel,\n",
    ")\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "\n",
    "# Import necessary classes from PEFT (Parameter-Efficient Fine-Tuning) library for LoRA\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from util.gymnastics import RLHF, DEVICE, init_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a [TinyStories](https://arxiv.org/abs/2305.07759) pretrained model that you can find on\n",
    "[HuggingFace](https://huggingface.co/roneneldan/TinyStories-33M). Moreover, we limit the output of\n",
    "the model to 60 tokens, to keep the generation relatively short and more efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language model used in this notebook.\n",
    "MODEL_NAME = \"roneneldan/TinyStories-33M\"\n",
    "\n",
    "# Output length for training and sampling.\n",
    "OUTPUT_LEN = 60\n",
    "\n",
    "# Sample prompt for tests and consistent for comparison across epochs.\n",
    "SAMPLE_PROMPT = \"Once upon a time\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "In order to be able to fine-tune a language-model, we need to understand the very basics of its\n",
    "architecture. In particular, at the foundation of modern LLMs is the _transformer architecture_,\n",
    "described in the popular paper [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "We will use the [Hugging Face transformers API](https://huggingface.co/docs/transformers/index).\n",
    "Reading the documentation before proceeding with the notebook is highly-recommended, because the\n",
    "notebook assumes familiarity with such APIs and concepts.\n",
    "\n",
    "First thing first, let's write a function to get a tokenizer for our model. A _tokenizer_ transforms\n",
    "text into the corresponding tensor tokens' numerical IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokenizer(model_name=MODEL_NAME) -> PreTrainedTokenizer:\n",
    "    # TODO: Load the tokenizer associated with the chosen model. The tokenizer converts text to\n",
    "    # numerical IDs and back. Hint: use AutoTokenizer.from_pretrained\n",
    "    tokenizer = None\n",
    "    # TODO: Some models don't have a default padding token; set it to the end-of-sequence token in\n",
    "    # case. This is important for batching (even if don't batch in this notebook) and consistent\n",
    "    # sequence handling.\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = None\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to retrain the entire model for performance reasons. So we will use the HF\n",
    "[PEFT](https://huggingface.co/docs/transformers/en/peft) (Parameter Efficient Fine Tuning) library\n",
    "to create\n",
    "[LoRA adapters](https://huggingface.co/docs/peft/conceptual_guides/adapter#low-rank-adaptation-lora).\n",
    "\n",
    "Low-Rank Adaptation (LoRA) is one of the most popular PEFT methods, and allows to train a smaller\n",
    "subset of parameters added to the model's fixed parameters to tweak its behavior.\n",
    "\n",
    "Because we will use PPO for training, we will have both an actor and a critic model. Hence, we will\n",
    "create two LoRA configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lora_configs(r=16, alpha=32, dropout=0.05) -> tuple[LoraConfig, LoraConfig]:\n",
    "    \"\"\"LoRA (Low-Rank Adaptation) allows fine-tuning only a small number of parameters, saving\n",
    "    memory and compute.\n",
    "\n",
    "    Args:\n",
    "        r (int): Rank of the LoRA decomposition matrices. Higher rank means more trainable\n",
    "                 parameters (more capacity but slower).\n",
    "        alpha (int): LoRA scaling factor, often set to 2*R. Balances the influence of LoRA\n",
    "                     adapters vs base weights.\n",
    "        dropout (float): Dropout probability within LoRA layers to prevent overfitting.\n",
    "    \"\"\"\n",
    "    # TODO: Specify which layers / modules within the base model to apply LoRA adapters to.\n",
    "    # Targeting attention projection layers (query, key, value) is common and effective. Hint:\n",
    "    # they are \"q_proj\", \"k_proj\", \"v_proj\", make a list :)\n",
    "    target_layers = None\n",
    "\n",
    "    # LoRA configuration specifically for the actor model.\n",
    "    actor_lora_config = LoraConfig(\n",
    "        task_type=None,  # TODO: Use the Causal Language Model task type.\n",
    "        # TODO: Use the parameters from the method arguments and the target_layers above\n",
    "        r=None,\n",
    "        lora_alpha=None,\n",
    "        lora_dropout=None,\n",
    "        target_modules=None,\n",
    "    )\n",
    "\n",
    "    # LoRA configuration specifically for the critic model.\n",
    "    critic_lora_config = LoraConfig(\n",
    "        task_type=None,  # TODO: Use the Sequence Classification task type.\n",
    "        r=None,\n",
    "        lora_alpha=None,\n",
    "        lora_dropout=None,\n",
    "        target_modules=None,\n",
    "    )\n",
    "    return actor_lora_config, critic_lora_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create the models used for training!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_models(\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    model_name=MODEL_NAME,\n",
    ") -> tuple[PreTrainedModel, PreTrainedModel, PreTrainedModel]:\n",
    "    \"\"\"Loads the pre-trained language models to use for RLHF fine-tuning.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - PreTrainedModel: the frozen reference model, used for KL-divergence computation and\n",
    "                               reference during training.\n",
    "            - PreTrainedModel: the \"actor\" model, i.e., the language model that is fine-tuned.\n",
    "            - PreTrainedModel: the \"critic\" model, i.e., the model for classification used to\n",
    "                               determine the \"values\" of the actor outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Load the base pre-trained language model for the Actor (policy network).\n",
    "    # Hint: Use AutoModelForCausalLM.from_pretrained and make sure to move to DEVICE!\n",
    "    base_actor_model = None\n",
    "\n",
    "    # TODO: Load the base pre-trained model, but configure it for sequence classification to act as\n",
    "    # critic (value network). Hint: use AutoModelForSequenceClassification.from_pretrained,\n",
    "    # `num_labels=1` makes it output a single continuous value (regression), suitable for predicting\n",
    "    # expected reward (value).\n",
    "    base_critic_model = None\n",
    "\n",
    "    # TODO: Ensure critic model knows the correct padding token ID from the tokenizer.\n",
    "    base_critic_model.config.pad_token_id = None\n",
    "\n",
    "    # TODO: Create a deep copy of the original *base* actor model as the reference model, before\n",
    "    # LoRA. This model serves as a fixed reference point for the KL divergence calculation.\n",
    "    # Hint: use copy.deepcopy and move to DEVICE!\n",
    "    ref_model = None\n",
    "    # TODO: Set the reference model to evaluation mode (disables dropout, etc.)\n",
    "    # ...\n",
    "    # TODO: Freeze all parameters of the reference model, so they don't get updated during training.\n",
    "    # ...\n",
    "\n",
    "    # TODO: Wrap the base actor and critic models with LoRA adapters using the defined configs.\n",
    "    # Hint: use the `make_lora_configs` method above, and HF `get_peft_model`, which modifies the\n",
    "    # model to insert LoRA layers and freezes the original weights.\n",
    "    actor_lora_config, critic_lora_config = None, None\n",
    "    actor_model = None\n",
    "    critic_model = None\n",
    "\n",
    "    return ref_model, actor_model, critic_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now put some pieces together and create a function that invokes a model and generates a\n",
    "completion to an input prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    llm_model: PreTrainedModel,\n",
    "    prompt: str,\n",
    "    max_length: int = OUTPUT_LEN,\n",
    ") -> tuple[torch.Tensor, str]:\n",
    "    \"\"\"Generates an output from the LLM model.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - Tensor: the model output token IDs.\n",
    "            - str: The decoded text.\n",
    "    \"\"\"\n",
    "    # TODO: Tokenize the current prompt text into numerical IDs for the model, truncating long\n",
    "    # prompts to avoid exceeding our specified max_length. Hint: use the tokenizer, and move to\n",
    "    # DEVICE :)\n",
    "    tokenized_prompt = None\n",
    "    # TODO: Get token IDs and attention mask (which indicates which tokens are real vs padding).\n",
    "    input_ids = None\n",
    "    attention_mask = None\n",
    "    # TODO: # Store the length of the prompt section (hint: get it from the input_ids shape).\n",
    "    prompt_len = None\n",
    "\n",
    "    # Generate a response using the actor model (with LoRA). Use torch.no_grad() to avoid any\n",
    "    # gradients calculation on generation (saving memory during inference).\n",
    "    with torch.no_grad():\n",
    "        # TODO: Call llm_model.generate. Make sure to set `do_sample` to True, the appropriate\n",
    "        # padding and end-of-sequence token IDs, as well as top_k=50, top_p=0.95, and temperature\n",
    "        # set to 0.7. Read up about those parameters in the documentation :)\n",
    "        outputs = None\n",
    "    # TODO: Extract only the generated token IDs (excluding the prompt).\n",
    "    response_ids = None\n",
    "    # TODO: Decode the response token IDs back into human-readable text. Hint: use the tokenizer!\n",
    "    response_text = None\n",
    "    # Return response_ids with first being the batch (of size 1 in this simple notebook).\n",
    "    return tokenized_prompt, response_ids.unsqueeze(0), response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the tokenizer and generate an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = make_tokenizer()\n",
    "ref_model, actor_model, critic_model = make_training_models(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, output_text = generate(tokenizer, ref_model, prompt=SAMPLE_PROMPT)\n",
    "print(f\"{SAMPLE_PROMPT}{output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Model\n",
    "\n",
    "As previously mentioned, we are going to use an hard-coded function as our \"pretend reward model\"\n",
    "for simplicity and to avoid to train a full-fledged reward model. While this works in our limited\n",
    "educational RLHF example, it is certainly not effective for proper fine-tuning and alignment:\n",
    "[reward hacking](https://en.wikipedia.org/wiki/Reward_hacking) being one of the typical reasons.\n",
    "\n",
    "But for now, let's write a reward function that encourages telling stories about animals!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animals_stories_reward(response: str) -> torch.Tensor:\n",
    "    \"\"\"Defines the reward signal used to guide the RL training.\n",
    "\n",
    "    This simple function rewards mentioning animal words and penalizes mentioning human words.\n",
    "    Note: Simple keyword-based rewards are easy for education but prone to \"reward hacking\".\n",
    "\n",
    "    Feel free to simplify or create your own reward function!!! :)\n",
    "    \"\"\"\n",
    "    # TODO: Preprocess the generated response: to lower case and get the list of words.\n",
    "    text_words = None\n",
    "    response_length = None\n",
    "\n",
    "    # TODO: Count occurrences of animal and human words and use them as scores. Use the provided\n",
    "    # RLHF.ANIMAL_WORDS and RLHF.HUMAN_WORDS.\n",
    "    animal_mentions = None  # Hint: this will be used below.\n",
    "    animal_score = None\n",
    "    human_score = None\n",
    "\n",
    "    # TODO: length_penalty is -1.0 if the response_length is less than 5 words.\n",
    "    length_penalty = None\n",
    "\n",
    "    # TODO: Penalize excessive repetition of animal words. Compute `animal_threshold` as the max\n",
    "    # between 5 and 0.15 * response_length\n",
    "    animal_threshold = None\n",
    "    excessive_animal_penalty = 0\n",
    "    if animal_score > animal_threshold:\n",
    "        excessive_animal_penalty = -0.5 * (animal_score - animal_threshold)\n",
    "\n",
    "    # TODO: Encourage a diversity of animal words. The `unique_animal_bonus` is 0.5 * the number\n",
    "    # of unique `animal_mentions`.\n",
    "    unique_animal_bonus = None\n",
    "\n",
    "    # TODO: Calculate final reward as:\n",
    "    #  1.2 * animal_score - 0.8 * human_score + length_penalty + excessive_penalty + unique_bonus\n",
    "    reward = None\n",
    "    # TODO: Return a scalar tensor of float32 type (on DEVICE!)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_animals_stories_reward():\n",
    "    response1 = \"The quick brown fox and a lion met a boy\"\n",
    "    expected_reward1 = 2.6\n",
    "    actual_reward1 = animals_stories_reward(response1)\n",
    "    assert torch.isclose(\n",
    "        actual_reward1, torch.tensor(expected_reward1)\n",
    "    ), f\"FAILED Case 1: Expected {expected_reward1}, but got {actual_reward1.item()}\"\n",
    "    print(\"✅ Test Case 1 (Standard) Passed!\")\n",
    "\n",
    "    response2 = \"a cat and dog\"\n",
    "    expected_reward2 = 2.4\n",
    "    actual_reward2 = animals_stories_reward(response2)\n",
    "    assert torch.isclose(\n",
    "        actual_reward2, torch.tensor(expected_reward2)\n",
    "    ), f\"FAILED Case 2: Expected {expected_reward2}, but got {actual_reward2.item()}\"\n",
    "    print(\"✅ Test Case 2 (Short Response) Passed!\")\n",
    "\n",
    "    response3 = \"cat dog lion tiger bear fox cat dog lion tiger bear fox\"  # 12 words\n",
    "    expected_reward3 = 13.9\n",
    "    actual_reward3 = animals_stories_reward(response3)\n",
    "    assert torch.isclose(\n",
    "        actual_reward3, torch.tensor(expected_reward3)\n",
    "    ), f\"FAILED Case 3: Expected {expected_reward3}, but got {actual_reward3.item()}\"\n",
    "    print(\"✅ Test Case 3 (Excessive Animals) Passed!\")\n",
    "\n",
    "    response4 = \"This is a simple sentence about nothing special\"\n",
    "    expected_reward4 = 0.0\n",
    "    actual_reward4 = animals_stories_reward(response4)\n",
    "    assert torch.isclose(\n",
    "        actual_reward4, torch.tensor(expected_reward4)\n",
    "    ), f\"FAILED Case 4: Expected {expected_reward4}, but got {actual_reward4.item()}\"\n",
    "    print(\"✅ Test Case 4 (No Keywords) Passed!\")\n",
    "\n",
    "\n",
    "test_animals_stories_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLHF Fine Tuning w/ PPO\n",
    "\n",
    "Let's start by writing a helper function that computes the actual probabilities of the model output,\n",
    "with respect to the actual response. We will use this repeatedly in the training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actual_log_probs(\n",
    "    logits_all: torch.Tensor, response_ids: torch.Tensor, prompt_len: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute the log probabilities of the model output logits w.r.t. the *actual* response.\n",
    "\n",
    "    Args:\n",
    "        logits_all (torch.Tensor): The model output logits of prompt + response. The shape should\n",
    "                                   be [batch, tokens, logits], where batch is always 1 in this\n",
    "                                   notebook, tokens are prompt+response tokens, logits ~50k (i.e.,\n",
    "                                   total number of tokens).\n",
    "        response_ids (torch.Tensor): The actual response token IDs. Shape should be:\n",
    "                                     [batch, response_tokens], in this notebook [1, 100].\n",
    "        prompt_len (int): How long is the prompt.\n",
    "    \"\"\"\n",
    "    # TODO: Get the log_probs from the logits using F.log_softmax (pay attention to dim!)\n",
    "    log_probs_all = None\n",
    "    # TODO: Define the start and end indices corresponding to the *generated response* tokens\n",
    "    # within the full sequence logits. We need logits from position `prompt_len - 1` to predict\n",
    "    # tokens at position `prompt_len`, up to the end.\n",
    "    gen_start = None\n",
    "    # TODO: Index of the second-to-last token (predicting the last token).\n",
    "    gen_end = None\n",
    "    # TODO: Get only the response log_probs from the `log_probs_all`.\n",
    "    response_log_probs = None\n",
    "    # TODO: The generated `response_ids` tensor needs correct shape for `gather`. Hint: use\n",
    "    # unsqueeze in the last dimension: (1, gen_len) -> (1, gen_len, 1)\n",
    "    gather_index = None\n",
    "    # TODO: Use `gather` to select the log probabilities corresponding to the *actual tokens\n",
    "    # generated* by the actor (remove last dim).\n",
    "    # Hint: (1, 100, 50k) gathered by (1, 100, 1) -> selects (i, j, index[i, j, 1]), i.e., the\n",
    "    # actual token logprob for each token. Squeeze the last dimension!\n",
    "    actual_log_probs = None\n",
    "    return actual_log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an AI-generated unit-test :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_actual_log_probs():\n",
    "    batch_size = 1\n",
    "    prompt_len = 3\n",
    "    response_len = 2\n",
    "    total_len = prompt_len + response_len\n",
    "\n",
    "    logits_all = torch.tensor(\n",
    "        [\n",
    "            [  # Batch 0\n",
    "                [0, 0, 0, 0, 0],  # Logits for token 1 (in prompt)\n",
    "                [0, 0, 0, 0, 0],  # Logits for token 2 (in prompt)\n",
    "                [1, 2, 3, 4, 5],  # Logits used to predict the 1st response token\n",
    "                [5, 4, 3, 2, 1],  # Logits used to predict the 2nd response token\n",
    "                [0, 0, 0, 0, 0],  # Logits for token after response (will be ignored)\n",
    "            ]\n",
    "        ],\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    response_ids = torch.tensor([[2, 0]], dtype=torch.int64)\n",
    "    logits_for_response = logits_all[:, prompt_len - 1 : total_len - 1, :]\n",
    "    expected_log_probs_slice = F.log_softmax(logits_for_response, dim=-1)\n",
    "    log_prob_token1 = expected_log_probs_slice[0, 0, 2]\n",
    "    log_prob_token2 = expected_log_probs_slice[0, 1, 0]\n",
    "\n",
    "    expected_output = torch.tensor([[log_prob_token1, log_prob_token2]])\n",
    "\n",
    "    actual_output = actual_log_probs(\n",
    "        logits_all=logits_all,\n",
    "        response_ids=response_ids,\n",
    "        prompt_len=prompt_len,\n",
    "    )\n",
    "\n",
    "    assert actual_output.shape == (\n",
    "        batch_size,\n",
    "        response_len,\n",
    "    ), f\"FAILED: Expected shape {(batch_size, response_len)}, but got {actual_output.shape}\"\n",
    "\n",
    "    assert torch.allclose(\n",
    "        actual_output, expected_output\n",
    "    ), f\"FAILED: Expected values {expected_output}, but got {actual_output}\"\n",
    "\n",
    "    print(\"✅ Test passed!\")\n",
    "\n",
    "\n",
    "test_actual_log_probs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally... let's write our RLHF training loop using PPO! You should already by familiar with\n",
    "PPO's general training structure and loss function - feel free to take a peek at the PPO notebook\n",
    "solution to refresh your memory!\n",
    "\n",
    "One important detail about training is that the reward (that comes from the reward model) is\n",
    "counter-balanced by preventing the new model distribution to steer away too much from the original\n",
    "model distribution. Intuitively, we still want our language model to produce text in a similar way\n",
    "it currently does (and not finding ways to maximize reward that would instead produce unintelligible\n",
    "text).\n",
    "\n",
    "In order to do that, we measure the difference between the \"reference\" model distribution and the\n",
    "model under training via\n",
    "[KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P \\| Q) = \\sum_i P(i) \\log \\frac{P(i)}{Q(i)}\n",
    "$$\n",
    "\n",
    "Which can be [efficiently approximated](http://joschu.net/blog/kl-approx.html) as:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P \\| Q) = E_{X \\sim P}\\left[ \\log P(X) - \\log Q(X) \\right]\n",
    "$$\n",
    "\n",
    "Understanding the mathematics behind these formulas would definitely be beneficial, but it is not\n",
    "necessary to still appreciate the overall RLHF training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rlhf_fine_tuning(\n",
    "    actor_model: PreTrainedModel,\n",
    "    critic_model: PreTrainedModel,\n",
    "    num_epochs=50,\n",
    "    n_epoch_updates=2,\n",
    "    max_length=OUTPUT_LEN,\n",
    "    actor_lr=5e-5,\n",
    "    critic_lr=1e-4,\n",
    "    kl_beta=0.1,\n",
    "    epsilon=0.2,\n",
    "    grad_clip_norm=1.0,\n",
    "    seed=42,\n",
    "):\n",
    "    \"\"\"Fine-tune an LLM using Reinforcement Learning w/ Human Feedback.\n",
    "\n",
    "    Args:\n",
    "        num_epochs (int): Number of times the training loop iterates over all prompts.\n",
    "        max_length (int): Maximum sequence length for generated responses (including prompt).\n",
    "                          Limits computational cost and output length.\n",
    "        actor_lr (float): Learning rate for the actor (policy) model optimizer.\n",
    "        critic_lr (float): Learning rate for the critic (value) model optimizer.\n",
    "        kl_beta (float): Strength of the KL divergence penalty.\n",
    "        epsilon (float): PPO clipping parameter; limits how much the policy changes in one update.\n",
    "        grad_clip_norm (float): Maximum gradients norm to avoid exploding gradients.\n",
    "        seed (int): Seed for deterministic training.\n",
    "    \"\"\"\n",
    "    # TODO: Create optimizers on the trainable parameters, i.e., LoRA weights.\n",
    "    actor_optimizer = None\n",
    "    critic_optimizer = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Dictionary of metrics to track during training.\n",
    "        metrics = {\n",
    "            \"raw_scores\": [],\n",
    "            \"rewards\": [],\n",
    "            \"kl_divs\": [],\n",
    "            \"ppo_losses\": [],\n",
    "            \"critic_losses\": [],\n",
    "        }\n",
    "\n",
    "        # TODO: Set models to training mode (enables dropout, LoRA updates, etc.)\n",
    "        # ...\n",
    "\n",
    "        # TODO: Shuffle prompts randomly each epoch for better generalization. Use RLHF.PROMPTS,\n",
    "        # and remember to .copy() them!\n",
    "        epoch_prompts = None\n",
    "        # TODO: Shuffle them.\n",
    "\n",
    "        for prompt in epoch_prompts:\n",
    "            # TODO: Generate Response (no grad). Hint: use our `generate` method.\n",
    "            tokenized_prompt, response_ids, response_text = None\n",
    "            # TODO: Store the length of the response section.\n",
    "            response_len = None\n",
    "            # TODO: Store the length of the prompt section.\n",
    "            prompt_len = None\n",
    "\n",
    "            # TODO: Concatenate prompt and response IDs to form the full sequence for model input.\n",
    "            full_ids = None\n",
    "            # TODO: Create the corresponding attention mask for the full sequence (prompt mask +\n",
    "            # ones for response b/c response has no padding). Hint: use torch.cat again.\n",
    "            full_attention_mask = None\n",
    "\n",
    "            # Calculate Metrics (Reward, KL, Value). Perform these calculations without tracking\n",
    "            # gradients as they are inputs to the loss functions.\n",
    "            with torch.no_grad():\n",
    "                # TODO: Get logits (raw model outs before activation) from the actor (PEFT model).\n",
    "                # The actor model output has two fields: `logits` and `past_key_values` (for caching\n",
    "                # attention computations).\n",
    "                actor_logits = None\n",
    "                # TODO: Calculate the log probs using the response_ids. Hint: use the\n",
    "                # `actual_log_probs` method implemented above.\n",
    "                actor_log_probs = None\n",
    "\n",
    "                # TODO: Get logits from the frozen reference model (original base model).\n",
    "                ref_logits = None\n",
    "                # TODO: Calculate the log probabilities using the response_ids.\n",
    "                ref_log_probs = None\n",
    "\n",
    "                # TODO: Calculate the log probability of the generated sequence under the old policy\n",
    "                # (current actor state). Sum the log probabilities across the sequence dimension\n",
    "                # (dim=1). Squeeze to get a scalar tensor.\n",
    "                log_probs_old = None\n",
    "\n",
    "                # TODO: Get the value prediction from critic (PEFT model) using the full sequence.\n",
    "                critic_outputs = None\n",
    "                # TODO: Compute the critic value from the critic outputs logits (squeeze)\n",
    "                critic_value = critic_outputs.logits.squeeze()\n",
    "\n",
    "                # TODO: Calculate the KL divergence between the actor and reference model\n",
    "                # distributions for the generated sequence. KL(P || Q) approx E[log P - log Q]\n",
    "                # (sample). Sum difference over sequence, get scalar.\n",
    "                kl_div = None\n",
    "\n",
    "                # TODO: Get the base reward based on the generated text using our reward function.\n",
    "                base_reward = None\n",
    "                # TODO: Calculate the final reward: base reward minus the KL penalty.\n",
    "                total_reward = None\n",
    "\n",
    "            # TODO: Calculate the advantage A(s,a) = R - V(s) using the final reward and the value\n",
    "            # estimate (scalar). Detach it (to be used as a constant target below).\n",
    "            advantage = None\n",
    "\n",
    "            for _ in range(n_epoch_updates):\n",
    "                # TODO: Perform a forward pass through the current actor model again with gradients\n",
    "                # enabled to get the log probabilities of the generated sequence under the current\n",
    "                # policy state.\n",
    "                actor_logits_new = None\n",
    "                actor_log_probs_new = None\n",
    "                # TODO: Sum on dim=1 and squeeze()\n",
    "                log_probs_new = None\n",
    "\n",
    "                # TODO: Calculate the probability ratio: ratio = exp(log_prob_new - log_prob_old).\n",
    "                # Use the detached log_probs_old as it represents the fixed policy for this step.\n",
    "                ratio = None\n",
    "\n",
    "                # TODO: Clip the ratio to the range [1 - epsilon, 1 + epsilon].\n",
    "                clipped_ratio = None\n",
    "\n",
    "                # TODO: The PPO loss is the negative of the surrogate objective: the minimum between\n",
    "                # ratio * advantage and clipped_ratio * advantage.\n",
    "                ppo_loss = None\n",
    "\n",
    "                # TODO: Update Actor (Policy Network). Remember to clip_grad_norm_\n",
    "                # ...\n",
    "\n",
    "                # TODO: Calculate the target value for the critic. Return = Advantage + V(s), which\n",
    "                # should approximate the total_reward.\n",
    "                returns = None\n",
    "\n",
    "                # TODO: Perform a forward pass through the current critic with gradients enabled to\n",
    "                # get value prediction for the current state.\n",
    "                critic_outputs_for_loss = None\n",
    "                # TODO: Squeeze the logits.\n",
    "                critic_value_for_loss = None\n",
    "\n",
    "                # TODO: Calculate the critic loss, MSE between predicted value and target return.\n",
    "                critic_loss = None\n",
    "\n",
    "                # TODO: Update Critic (Value Network). Remember to clip_grad_norm_\n",
    "                # ...\n",
    "\n",
    "            # Store metrics for this step.\n",
    "            metrics[\"raw_scores\"].append(base_reward)\n",
    "            metrics[\"rewards\"].append(total_reward.item())\n",
    "            metrics[\"kl_divs\"].append(kl_div.item())\n",
    "            metrics[\"ppo_losses\"].append(ppo_loss.item())\n",
    "            metrics[\"critic_losses\"].append(critic_loss.item())\n",
    "\n",
    "        # Epoch average metrics.\n",
    "        avg_raw_score = sum(metrics[\"raw_scores\"]) / len(metrics[\"raw_scores\"])\n",
    "        avg_final_reward = sum(metrics[\"rewards\"]) / len(metrics[\"rewards\"])\n",
    "        avg_kl_div = sum(metrics[\"kl_divs\"]) / len(metrics[\"kl_divs\"])\n",
    "        avg_ppo_loss = sum(metrics[\"ppo_losses\"]) / len(metrics[\"ppo_losses\"])\n",
    "        avg_critic_loss = sum(metrics[\"critic_losses\"]) / len(metrics[\"critic_losses\"])\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] \", end=\"\")\n",
    "        print(\n",
    "            f\"Reward: {avg_raw_score:.2f} | Reward (w/ KL): {avg_final_reward:.2f} | \"\n",
    "            + f\"KL Div: {avg_kl_div:.4f} | PPO Loss: {avg_ppo_loss:.4f} | \"\n",
    "            + f\"Critic Loss: {avg_critic_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Sample response for qualitative progress assessment.\n",
    "        actor_model.eval() # eval mode (e.g., disable dropouts)\n",
    "        sample_prompt = SAMPLE_PROMPT\n",
    "        _, _, sample_response = generate(tokenizer, actor_model, sample_prompt, max_length)\n",
    "        print(f\"[SAMPLE] {sample_prompt}{sample_response.replace('\\n', ' ')}\\n\")\n",
    "\n",
    "    print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training!\n",
    "\n",
    "During training we should be monitoring the KL divergence, which should be positive but not too\n",
    "large. Also the losses should trend downwards, but they can be very noisy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes ~15 minutes on a modern CPU/GPU (RTX 4090).\n",
    "rlhf_fine_tuning(actor_model, critic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how our model generates stories: hopefully, we will get a strong preference towards\n",
    "animals' stories!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, sample_response = generate(tokenizer, actor_model, SAMPLE_PROMPT)\n",
    "print(f\"[SAMPLE] {SAMPLE_PROMPT}{sample_response.replace('\\n', ' ')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-statistic\n",
    "\n",
    "Finally, let's compute an approximate statistic about how often the fine-tuned model uses words\n",
    "related to animals compared to the reference model. We should see some bias :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_model_count, ref_model_total, fine_tuned_count, fine_tuned_total = 0, 0, 0, 0\n",
    "\n",
    "for i in range(100):\n",
    "    print(f\"\\rSample: {i} ...\", end=\"\")\n",
    "    _, _, ref_model_text = generate(tokenizer, ref_model, SAMPLE_PROMPT)\n",
    "    _, _, fine_tuned_text = generate(tokenizer, actor_model, SAMPLE_PROMPT)\n",
    "    ref_model_text_words = ref_model_text.split()\n",
    "    fine_tuned_text_words = fine_tuned_text.split()\n",
    "    ref_model_total += len(ref_model_text_words)\n",
    "    fine_tuned_total += len(fine_tuned_text_words)\n",
    "    for word in RLHF.ANIMAL_WORDS:\n",
    "        ref_model_count += ref_model_text_words.count(word)\n",
    "        fine_tuned_count += fine_tuned_text_words.count(word)\n",
    "\n",
    "print(f\"\\rFined tuned: {(fine_tuned_count / fine_tuned_total) * 100:.1f}%\")\n",
    "print(f\"Reference model: {(ref_model_count / ref_model_total) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other / Most Recent Techniques\n",
    "\n",
    "Most recent techniques that represent good alternatives to RLHF are\n",
    "[DPO](https://arxiv.org/abs/2305.18290) and [GRPO](https://arxiv.org/abs/2402.03300). In particular,\n",
    "when you have verifiable tasks (either via script, or via another LLM!), GRPO works great and\n",
    "employs reward functions similar to this notebook's example. Deeplearning.AI has a great\n",
    "[introductory course on GRPO](https://learn.deeplearning.ai/courses/reinforcement-fine-tuning-llms-grpo),\n",
    "totally recommended!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlzh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
