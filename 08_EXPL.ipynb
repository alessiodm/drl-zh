{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration and Curiosity\n",
    "\n",
    "In Reinforcement Learning (RL), agents learn by maximizing a **reward signal**. This core idea,\n",
    "often called the _reward hypothesis_, means the design and frequency of rewards heavily influence\n",
    "how well an agent learns.\n",
    "\n",
    "However, we often face two major hurdles:\n",
    "\n",
    "- **Difficult Reward Design:** Creating informative reward functions (extrinsic rewards from the\n",
    "  environment) can be complex, especially for intricate tasks.\n",
    "- **Sparse Rewards:** Many environments offer rewards only rarely. Imagine a maze where the agent\n",
    "  only gets a reward (+1) at the exit and zero everywhere else. Without frequent feedback, the agent\n",
    "  might wander aimlessly or get stuck, failing to explore effectively and find the goal.\n",
    "\n",
    "When extrinsic rewards are scarce, standard exploration methods (like adding noise to actions) might\n",
    "not be enough. The agent needs an internal drive to explore novel situations, a form of _curiosity_.\n",
    "The overall reward is now given by:\n",
    "\n",
    "$\n",
    "r_t = e_t + i_t\n",
    "$\n",
    "\n",
    "I.e., the sum of the _extrinsic_ reward and an _intrinsic_ component.\n",
    "\n",
    "This notebook explores [Random Network Distillation (RND)](https://arxiv.org/abs/1810.12894), a\n",
    "powerful technique that provides the agent with such an intrinsic reward. RND achieved SoTA in\n",
    "Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods.\n",
    "\n",
    "**NOTE:** It is highly recommended to read and understand the paper before proceeding with the\n",
    "notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gymnasium as gym\n",
    "from minigrid.wrappers import ImgObsWrapper\n",
    "from gymnasium.wrappers.vector import RecordEpisodeStatistics\n",
    "\n",
    "from util.gymnastics import DEVICE, gym_simulation, init_random\n",
    "from util.rl_algos import (\n",
    "    BaseAgent,\n",
    "    TrajectorySegment,\n",
    "    collect_trajectory_segment,\n",
    "    compute_advantages_and_returns,\n",
    "    flatten_and_shuffle,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MiniGrid DoorKey Environment\n",
    "\n",
    "From the [documentation](https://minigrid.farama.org/environments/minigrid/DoorKeyEnv/): \"This\n",
    "environment is difficult, because of the sparse reward, to solve using classical RL algorithms. It\n",
    "is useful to experiment with curiosity or curriculum learning.\"\n",
    "\n",
    "Note that this environment can be solved without\n",
    "[memory](https://github.com/lcswillems/rl-starter-files/tree/317da04a9a6fb26506bbd7f6c7c7e10fc0de86e0?tab=readme-ov-file#add-memory),\n",
    "though I encourage you to explore Lucas Willemsâ€™ starter-files repo and memory-based solutions.\n",
    "\n",
    "**Something to think about:** Consider the Markovian property and partial observability of the\n",
    "state... In particular, how does the agent know if they have the key? They don't! They learnt a\n",
    "policy of states highly-likely to have a key, if they don't they go explore more. They may be\n",
    "learning that staying close to a wall means we have the key...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIGRID_ENV = \"MiniGrid-DoorKey-5x5-v0\"\n",
    "MINIGRID_ENV_8x8 = \"MiniGrid-DoorKey-8x8-v0\"\n",
    "MINIGRID_ENV_16x16 = \"MiniGrid-DoorKey-16x16-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation of the environment, to verify it loaded.\n",
    "# https://minigrid.farama.org/environments/minigrid/DoorKeyEnv/#\n",
    "gym_simulation(MINIGRID_ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Wrappers\n",
    "\n",
    "To simplify our training, we only use simplified image observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_state, _ = init_random(gym.make(MINIGRID_ENV)).reset()\n",
    "\n",
    "print(sample_state[\"image\"].shape)\n",
    "print(sample_state[\"direction\"])\n",
    "print(sample_state[\"mission\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minigrid_wrapper(env: gym.Env) -> gym.Env:\n",
    "    # Converts to image observations\n",
    "    env = ImgObsWrapper(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_state, _ = minigrid_wrapper(gym.make(MINIGRID_ENV)).reset()\n",
    "print(sample_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_minigrid_env(minigrid_env=MINIGRID_ENV, num_envs=32) -> gym.vector.VectorEnv:\n",
    "    \"\"\"Utility function to create a MiniGrid vectorized environment with the appropriate wrappers\"\"\"\n",
    "    env = gym.make_vec(minigrid_env, num_envs=num_envs, wrappers=[minigrid_wrapper])\n",
    "    env = RecordEpisodeStatistics(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C Algorithm\n",
    "\n",
    "We already implemented a simplified A2C algorithm in the actor-critic notebook. We will now power-up\n",
    "the A2C implementation using a vectorized environment and utilities previously introduced in the PPO\n",
    "notebook. We then will extend the implementation with RND, let's start!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    env: gym.vector.VectorEnv, agent: BaseAgent, rollout_size=16, log_every=100, solved_score=0.95\n",
    "):\n",
    "    \"\"\"The A2C training loop. We use the utility BaseAgent class for on-policy policy-gradient\n",
    "    algorithms just for convenience in this notebook.\"\"\"\n",
    "    n_step = 0\n",
    "    avg_score = 0.0\n",
    "    obs, _ = env.reset()\n",
    "    obs = torch.Tensor(obs).to(DEVICE)\n",
    "    while True:\n",
    "        # Collects a trajectory segment across multiple vectorized environments, making sure that\n",
    "        # we take intrinsic rewards into account. Feel free to check out the library implementation\n",
    "        # of `collect_trajectory_segment` which is very similar to the one you implemented already\n",
    "        # in the PPO notebook!\n",
    "        segment = collect_trajectory_segment(\n",
    "            env, agent, obs, rollout_size, with_intrinsic_values=True\n",
    "        )\n",
    "        # Make the agent learn via the just-collected trajectory.\n",
    "        stats = agent.learn(segment)\n",
    "        # Update the state... Easy to overlook!\n",
    "        obs = segment.next_start_obs\n",
    "        print(\n",
    "            f'Loss: {stats[\"loss\"]: .7f}, Instrinsic reward (avg): '\n",
    "            + f'{stats[\"rnd_reward_avg\"]: .7f}, RND Loss: {stats[\"rnd_loss\"]: .7f}\\r',\n",
    "            end=\"\",\n",
    "        )\n",
    "\n",
    "        n_step += 1\n",
    "        if n_step % log_every == 0:\n",
    "            avg_score = np.mean(env.return_queue)\n",
    "            print(f\"Global step: {n_step*rollout_size}, Average Score: {avg_score:.5f}\".ljust(100))\n",
    "\n",
    "        if avg_score > solved_score:\n",
    "            print(f\"Environment solved with avg_score: {avg_score:.5f}\".ljust(100))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Architectures\n",
    "\n",
    "The actor-critic network has a shared feature extractor, the policy head (outputting a probability\n",
    "distribution over actions), and two value heads: for extrinsic and intrinsic rewards respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_embedding(input_channels=3) -> nn.Sequential:\n",
    "    \"\"\"Shared layers for common feature extraction from the MiniGrid image observation.\"\"\"\n",
    "    return nn.Sequential(\n",
    "        # TODO: Create three 2x2 2D convolutional layers with 16, 32, 64 output channels. Use the\n",
    "        #       ReLU non-linearity, and remember to flatten to prepare feeding the linear layers!\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"Neural network for both policy (actor) and value function (critic).\"\"\"\n",
    "\n",
    "    def __init__(self, action_dim, hidden_dim=64):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # TODO: Create the shared feature extraction convolutional embedding layer.\n",
    "        self.features = None\n",
    "        # Actor head\n",
    "        self.policy_head = nn.Sequential(\n",
    "            # TODO: Add a linear layer, tanh non-linearity, and a second linear layer to output the\n",
    "            #       logits of the action probability distribution.\n",
    "        )\n",
    "        # Critic head (extrinsic)\n",
    "        self.value_extr_head = nn.Sequential(\n",
    "            # TODO: Add a linear layer, tanh non-linearity, and a second linear layer to output the\n",
    "            #       state value using extrinsic rewards.\n",
    "        )\n",
    "        # Critic head (intrinsic)\n",
    "        self.value_intr_head = nn.Sequential(\n",
    "            # TODO: Add a linear layer, tanh non-linearity, and a second linear layer to output the\n",
    "            #       state value using instrinsic rewards.\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Get the shared feature embedding.\n",
    "        x = None\n",
    "        # TODO: Get the policy logits using the policy head.\n",
    "        policy_logits = None\n",
    "        # TODO: Get the critic value (extrinsic)\n",
    "        value_extr = None\n",
    "        # TODO: Get the critic value (intrinsic)\n",
    "        value_intr = None\n",
    "        # TODO: Stack the extrinsic and intrinsic value in a single tensor.\n",
    "        values = None\n",
    "        # TODO: Get the Categorical distribution over the policy logits.\n",
    "        dist = None\n",
    "        return dist, values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentA2C(BaseAgent):\n",
    "    \"\"\"A2C agent using vectorized environment and trajectory segments.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, env, gamma=0.99, lr=0.0001, vloss_coeff=0.5, ent_coeff=0.01, max_grad_norm=0.5\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.vloss_coeff = vloss_coeff\n",
    "        self.ent_coeff = ent_coeff\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.model = ActorCritic(env.action_space[0].n).to(DEVICE)\n",
    "        self.optimizer = optim.RMSprop(self.model.parameters(), lr=lr)\n",
    "\n",
    "    @torch.no_grad\n",
    "    def eval(self, obs: torch.Tensor) -> tuple[torch.Tensor, ...]:\n",
    "        \"\"\"Evaluates the current observation, returning the action to take, action logprob, and\n",
    "        state value.\"\"\"\n",
    "        # TODO: Permute the observation. The input `obs` dimension is (batch, x, y, channels), but\n",
    "        #       PyTorch convolutional layers expect (batch, channels, x, y).\n",
    "        obs = None\n",
    "        # TODO: Get action distribution and state values calling the model.\n",
    "        dist, values = None\n",
    "        # TODO: Sample the action.\n",
    "        action = None\n",
    "        # TODO: Get the log probability.\n",
    "        logprob = None\n",
    "        return action, logprob, values\n",
    "\n",
    "    def learn(self, segment: TrajectorySegment) -> dict:\n",
    "        \"\"\"Single step training of the agent using a trajectory segment.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # TODO: Get the next_values. Hint: use eval.\n",
    "            _, _, next_values = None\n",
    "            # TODO: Compute the advantages and returns using the providede library function\n",
    "            #       `compute_advantages_and_returns`. Note that for basic A2C we need to keep only\n",
    "            #       the extrinsic component of the value tensors!\n",
    "            advantages, returns = None\n",
    "\n",
    "        # TODO: Flatten and shuffle observations from all environments. Hint: remember to permute\n",
    "        #       the observation; also, the `flatten_and_shuffle` function might help :)\n",
    "        obs, actions, advantages, returns = None\n",
    "\n",
    "        # Forward pass\n",
    "        # TODO: Call the model.\n",
    "        dist, values = None\n",
    "        # TODO: Get the logprobs.\n",
    "        logprobs = None\n",
    "        # TODO: Also, get the entropy. Hint: you may want to reduce using mean()\n",
    "        entropy = None\n",
    "\n",
    "        # Loss\n",
    "        # TODO: Compute the policy loss as the mean of negative product of logprobs and advantages.\n",
    "        policy_loss = None\n",
    "        # TODO: Compute the value loss as MSE between extrinsic values and returns.\n",
    "        value_loss = None\n",
    "        # TODO: Compute the total loss as the sum of policy loss plus value loss minus entropy,\n",
    "        #       scaled by the respective coefficients.\n",
    "        loss = None\n",
    "\n",
    "        # TODO: Run backprop optimizer step. Remember to clip the gradient!\n",
    "        # ...\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss.item(),\n",
    "            \"rnd_reward_avg\": 0.0,\n",
    "            \"rnd_loss\": 0.0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train A2C on a trivial grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = init_random(make_minigrid_env())\n",
    "agent_a2c = AgentA2C(env)\n",
    "train(env, agent_a2c, rollout_size=16, solved_score=0.91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_simulation(MINIGRID_ENV, agent_a2c, wrappers=[minigrid_wrapper])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Network Distillation\n",
    "\n",
    "RND encourages the agent to visit unfamiliar states. It measures novelty by seeing how accurately a\n",
    "_predictor_ network can guess the output of a _fixed, randomly initialized target network_ given the\n",
    "current state.\n",
    "\n",
    "- **Familiar states:** The predictor network learns to accurately predict the target's output.\n",
    "  Prediction error is low -> low intrinsic reward (boredom).\n",
    "- **Novel states:** The predictor network struggles to predict the target's output. Prediction error\n",
    "  is high -> high intrinsic reward (curiosity).\n",
    "\n",
    "This intrinsic reward motivates the agent to explore parts of the environment it hasn't seen often,\n",
    "helping it overcome sparse extrinsic rewards and learn more effectively. RND cleverly avoids issues\n",
    "found in other intrinsic motivation methods (like getting distracted by unpredictable but irrelevant\n",
    "things, e.g., a noisy TV screen).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RND Module\n",
    "\n",
    "Let's implement the RND module, i.e., the neural network used to output the instrinsic reward using\n",
    "a trainable predictor and a fixed target network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNDModule(nn.Module):\n",
    "    \"\"\"The Random Network Distillation module\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super(RNDModule, self).__init__()\n",
    "        # RND predictor network.\n",
    "        self.predictor = nn.Sequential(\n",
    "            # TODO: Shared convolutional embedding layer to start\n",
    "            # TODO: Linear layer of hidden_dim\n",
    "            # TODO: Extra layers in predictor, regularized via dropouts. We use dropouts instead of\n",
    "            #       other regularization techniques.\n",
    "            #       Hint: dropout(0.25), ReLU, linear, dropout(0l25), ReLU, linear.\n",
    "        )\n",
    "        # RND target network (non-trainable)\n",
    "        self.target = nn.Sequential(\n",
    "            # TODO: Shared convolutional embedding layer to start\n",
    "            # TODO: Linear layer of hidden_dim\n",
    "        )\n",
    "        # TODO: Freeze the target network. Hint: set `requires_grad` to False in the parameters.\n",
    "        # ...\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # TODO: Get the target output (make sure to detach())\n",
    "        target_output = None\n",
    "        # TODO: Get the predictor output.\n",
    "        predictor_output = None\n",
    "        # TODO: Compute the intrinsic reward for each step of the trajectory using MSE. Keep an eye\n",
    "        #       on dimensionality, reduction, and which dimension to compute the mean() of!\n",
    "        rnd_error = None\n",
    "        return rnd_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C w/ RND\n",
    "\n",
    "Let's now re-implement A2C integrating RND and intrinsic rewards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentA2CWithRND(BaseAgent):\n",
    "    \"\"\"A2C agent with RND using vectorized environment and trajectory segments.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        gamma=0.99,\n",
    "        lr=0.0001,\n",
    "        vloss_coeff=0.5,\n",
    "        ent_coeff=0.01,\n",
    "        max_grad_norm_ac=0.5,\n",
    "        # The following hyperparameters have been tuned for minigrid in these examples.\n",
    "        gamma_rnd=0.99,\n",
    "        eta_extr=100.0,\n",
    "        eta_intr=10.0,\n",
    "        max_grad_norm_rnd=0.01,\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.vloss_coeff = vloss_coeff\n",
    "        self.ent_coeff = ent_coeff\n",
    "        self.max_grad_norm_ac = max_grad_norm_ac\n",
    "        self.gamma_rnd = gamma_rnd\n",
    "        self.eta_extr = eta_extr\n",
    "        self.eta_intr = eta_intr\n",
    "        self.max_grad_norm_rnd = max_grad_norm_rnd\n",
    "\n",
    "        self.model = ActorCritic(env.action_space[0].n).to(DEVICE)\n",
    "        self.rnd_module = RNDModule().to(DEVICE)\n",
    "        self.optimizer = optim.RMSprop(\n",
    "            list(self.model.parameters()) + list(self.rnd_module.predictor.parameters()), lr=lr\n",
    "        )\n",
    "\n",
    "    @torch.no_grad\n",
    "    def eval(self, obs: torch.Tensor) -> tuple[torch.Tensor, ...]:\n",
    "        # TODO: Same implementation of the A2CAgent!\n",
    "        pass\n",
    "\n",
    "    def learn(self, segment: TrajectorySegment) -> dict:\n",
    "        batch_size = segment.obs.shape[0]\n",
    "        n_envs = segment.obs.shape[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # TODO: Get the next_values.\n",
    "            _, _, next_values = None\n",
    "            # TODO: Compute extrinsic advantages and returns.\n",
    "            advantages_extr, returns_extr = None\n",
    "            next_obs = None  # Hint: flatten and permute!\n",
    "            # TODO: Call the RNDModule to compute intrinsic rewards on the next observations. Note:\n",
    "            #       we use dropouts as regularization, so we need to call eval() before the forward\n",
    "            #       pass, and then train() again.\n",
    "            # ...\n",
    "            # In the original implementation, the intrinsic rewards are scaled by a rolling std.\n",
    "            # Here we don't do that, and use dropouts as regularizers for simplicity.\n",
    "            intr_rewards = None\n",
    "            # ...\n",
    "            # Compute intrinsic advantages and returns.\n",
    "            advantages_intr, returns_intr = None\n",
    "\n",
    "        # TODO: Flatten and shuffle observations from all environments, advantages, and returns.\n",
    "        obs, actions, advantages_extr, returns_extr = None\n",
    "        advantages_intr, returns_intr = None\n",
    "\n",
    "        # TODO: Compute advantages as combination of extrinsic and intrinsic. Remember to use the\n",
    "        #       appropriate `eta` coefficients.\n",
    "        advantages = None\n",
    "\n",
    "        # TODO: Forward pass like A2CAgent\n",
    "        dist, values = None\n",
    "        logprobs = None\n",
    "        entropy = None\n",
    "\n",
    "        # TODO: Compute policy and value loss (as sum of extrinsic and instrinsic losses)\n",
    "        policy_loss = None\n",
    "        value_extr_loss = None\n",
    "        value_intr_loss = None\n",
    "        value_loss = None\n",
    "\n",
    "        # RND. In the original implementation, observations are normalized here.\n",
    "        # Moreover, there is a masked update on the number of experience for regularization\n",
    "        # (which we don't do and use dropouts instead).\n",
    "        # TODO: Get the rnd_error and rnd_loss (as the mean of the error)\n",
    "        rnd_error = None\n",
    "        rnd_loss = None\n",
    "\n",
    "        # TODO: Compute the total loss.\n",
    "        loss = None\n",
    "\n",
    "        # TODO: Backprop (and clip grad norm!)\n",
    "        # ...\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss.item(),\n",
    "            \"rnd_reward_avg\": intr_rewards.mean().item(),\n",
    "            \"rnd_loss\": rnd_loss.item(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train A2C w/ RND on a trivial grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = init_random(make_minigrid_env())\n",
    "agent_rnd = AgentA2CWithRND(env)\n",
    "train(env, agent_rnd, solved_score=0.91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_simulation(MINIGRID_ENV, agent_rnd, wrappers=[minigrid_wrapper])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to shine: larger grids!\n",
    "\n",
    "Let's train our A2C w/ RND on a larger grid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniGrid-DoorKey-8x8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = init_random(make_minigrid_env(MINIGRID_ENV_8x8))\n",
    "agent_rnd_8 = AgentA2CWithRND(env)\n",
    "train(env, agent_rnd_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_simulation(MINIGRID_ENV_8x8, agent_rnd_8, wrappers=[minigrid_wrapper])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train just A2C, giving it some benefits with lower solved score threshold :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = init_random(make_minigrid_env(MINIGRID_ENV_8x8))\n",
    "agent_a2c_8 = AgentA2C(env)\n",
    "# Give A2C some benefits, and reduce the threshold for solved score :)\n",
    "train(env, agent_a2c_8, solved_score=0.91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_simulation(MINIGRID_ENV_8x8, agent_a2c_8, wrappers=[minigrid_wrapper])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniGrid-DoorKey-16x16\n",
    "\n",
    "Train A2C w/ RND on `MiniGrid-DoorKey-16x16`. It is going to take time, b/c lots of exploration is\n",
    "required before getting stable reward signals... but it will be solved!\n",
    "\n",
    "NOTE: This is computationally really intensive (128 parallel env)! Consider just running the\n",
    "pretrained simulation below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pretrained_simulation():\n",
    "    loaded_env = gym.make_vec(MINIGRID_ENV_16x16, wrappers=[minigrid_wrapper])\n",
    "    loaded_agent = AgentA2CWithRND(loaded_env)\n",
    "    loaded_params = torch.load(\"solution/a2c_rnd_16x16_weights.pth\", map_location=DEVICE)\n",
    "    loaded_agent.model.load_state_dict(loaded_params[\"a2c_params\"])\n",
    "    loaded_agent.rnd_module.load_state_dict(loaded_params[\"rnd_params\"])\n",
    "    return gym_simulation(MINIGRID_ENV_16x16, loaded_agent, wrappers=[minigrid_wrapper], seed=110)\n",
    "\n",
    "\n",
    "# Uncomment the following line to run the pretrained agent on the 16x16 grid.\n",
    "# run_pretrained_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = init_random(make_minigrid_env(MINIGRID_ENV_16x16, num_envs=128))\n",
    "agent_rnd_16 = AgentA2CWithRND(env, gamma=0.995)\n",
    "train(env, agent_rnd_16, rollout_size=48, log_every=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_simulation(MINIGRID_ENV_16x16, agent_rnd_16, wrappers=[minigrid_wrapper])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to try A2C on the same environment... wait as much as you like... but I would not\n",
    "recommend staying too long :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt bare A2C... it's not gonna work...\n",
    "env = init_random(make_minigrid_env(MINIGRID_ENV_16x16, num_envs=128))\n",
    "agent_a2c_16 = AgentA2C(env, gamma=0.995)\n",
    "train(env, agent_a2c_16, rollout_size=48, log_every=35, solved_score=0.91)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlzh2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
