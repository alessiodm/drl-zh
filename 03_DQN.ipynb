{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve Grid World, that's awesome: we have agents that learn! So let's have them learn how to\n",
    "play Atari video games, such as pong!\n",
    "\n",
    "But... Grid World has a state space of ~20... Atari games have a state space of ~$10^{17000}$ using\n",
    "pixels (consider that there are ~$10^{82}$ atoms in the observable universe). Tabular methods don't\n",
    "scale to such large spaces.\n",
    "\n",
    "To overcome this limitation, we will use function approximation and approximate the Q function using\n",
    "neural networks!\n",
    "\n",
    "**Prerequisites**: Neural Network fundamentals as well as PyTorch basic APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gymnasium and Atari Pong\n",
    "\n",
    "[Gymnasium](https://gymnasium.farama.org/index.html) is an API standard for reinforcement learning,\n",
    "with a vast collections of environments. Let's use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing, RecordVideo\n",
    "\n",
    "from util.gymnastics import DEVICE, init_random, plot_scores, show_gym_video_recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's learn Gymnasium! https://gymnasium.farama.org/environments/atari/pong/\n",
    "def gym_simulate(agent = None):\n",
    "    \"\"\"Runs an Atari pong game with our agent passed as input.\"\"\"\n",
    "    # We use pong deterministic b/c it is simpler to train.\n",
    "    # Actions: NOOP, FIRE, RIGHT, LEFT, RIGHTFIRE, LEFTFIRE\n",
    "    # TODO: Use gym.make to create \"PongDeterministic-v4\" environment. Also, pass the render_mode\n",
    "    #       'rgb_array_list' and frameskip=1 to make sure we can record the video :)\n",
    "    sim_env = None\n",
    "    # Initializes the random generators for determinism.\n",
    "    sim_env = init_random(sim_env)\n",
    "    # TODO: Add the preprocessor for Atari. This does black/white conversion, and other convenient\n",
    "    #       operations for learning (see documentation).\n",
    "    sim_env = None\n",
    "    # Adding the RecordVideo wrapper to be able to record videos.\n",
    "    sim_env = RecordVideo(sim_env, tempfile.tempdir, lambda i: i == 0)\n",
    "\n",
    "    # TODO: Reset the environment\n",
    "    init_position, _ = None\n",
    "    # TODO: Take the first step with action=1 that starts the game.\n",
    "    first_observation, _, _, _, _ = None\n",
    "    # TODO: To create the state, we stack two observations, that is to have a sense of time/velocity\n",
    "    #       for the pong game. Hint: use np.stack.\n",
    "    state = None\n",
    "\n",
    "    for _ in range(2_500):\n",
    "        # TODO: Call agent.act if the agent is specified, otherwise use action_space.sample() from\n",
    "        #       the gym environment to select a random action.\n",
    "        action = None\n",
    "        # TODO: Perform an environment step.\n",
    "        observation, _, terminated, truncated, _ = None\n",
    "        # TODO: Check for completion, if completed reset the environment.\n",
    "        if terminated or truncated:\n",
    "            observation, _ = None\n",
    "        # TODO: Update the state with the new stacked observations (the last and the new one)\n",
    "        state = None\n",
    "\n",
    "    # TODO: Remember to close the gym environment!\n",
    "\n",
    "    # Call a convenient utility function to show the video in the notebook as a GIF.\n",
    "    return show_gym_video_recording()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_simulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN\n",
    "\n",
    "### Q Function Approximation via Neural Network\n",
    "\n",
    "Instead of a \"lookup\" table, $Q$ is going to be a parameterized functions: $Q_\\theta(s,a)$. The set\n",
    "of parameters $\\theta$ corresponds to the parameters (e.g., weights) of the neural network trained\n",
    "to approximate $Q$.\n",
    "\n",
    "We cannot use the TD error $\\delta^{TD}$ to update $Q$ anymore, because we now have to optimize the\n",
    "set of parameters $\\theta$ instead. The target is still the following:\n",
    "$$\n",
    "target(s') = R(s, a, s') + \\gamma \\max_{a'} Q_{\\theta_{k}}(s', a')\n",
    "$$\n",
    "\n",
    "But now we use gradient descent to update the parameters $\\theta$ (in a sense still minimizing\n",
    "$\\delta^{TD}$):\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_k - \\alpha \\nabla_\\theta \n",
    "   \\biggl\\{ \\frac{1}{2} \\Bigl[ Q_\\theta(s,a) - target(s') \\Bigr] ^ 2 \\biggr\\}\n",
    "$$\n",
    "\n",
    "Luckily, we don't have to implement any neural networks or know automatic differentiation math\n",
    "ourselves thanks to modern frameworks like PyTorch, TensorFlow, JAX! We use PyTorch in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        # TODO: Recall that the state is 2 frames of 84x84 pixels, which translates into a 2x84x84\n",
    "        #       tensor. Let's run 2D convolutions to transform to 4x40x40 and then 16x9x9. To do\n",
    "        #       that, add two Conv2d layers with (out_channels, kernel_size, stride) to match those\n",
    "        #       dimensions. Hint: (size - kernel) / stride + 1\n",
    "\n",
    "        # TODO: Create two fully connected layers with 256 and action_size units.\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        # TODO: Forward through the convolutions (use ReLU non-linearity)\n",
    "        # TODO: Remember to flatten the tensor for the linear layers!\n",
    "        # TODO: Forward through linear layers (using ReLU again)\n",
    "        # TODO: The output is directly the output of the last linear layer, representing the\n",
    "        #       action value function.\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for neural network!\n",
    "init_random()\n",
    "\n",
    "test_net = QNetwork(action_size=6)\n",
    "\n",
    "fake_img = np.random.randn(2, 84, 84)\n",
    "fake_tensor = torch.from_numpy(fake_img).float()\n",
    "result = test_net.forward(fake_tensor).detach()\n",
    "\n",
    "expected_result = torch.tensor([[-0.2041,  0.0406, -0.0483,  0.0051,  0.0216,  0.0395]])\n",
    "assert torch.allclose(result, expected_result, atol=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "The algorith from the [2013 paper](https://arxiv.org/abs/1312.5602) is approximately the following:\n",
    "\n",
    "<div style=\"width: 50%\">\n",
    "  <img src=\"assets/03_DQN_algo_highlighted.png\">\n",
    "  <br>\n",
    "  <small></small>\n",
    "</div>\n",
    "\n",
    "Two techniques to highlight:\n",
    "\n",
    " * **Replay Buffer:** Helpful for data efficiency (data reused for many updates). Also, learning\n",
    "   from consecutive samples is inefficient because of the strong correlations. Using a replay buffer\n",
    "   and randomizing the samples breaks such correlations reducing variance of the updates. It helps\n",
    "   preventing _catastrophic forgetting_.\n",
    " * **Target Network:** We don't update the same network at every step, because the target would\n",
    "   effectively become non-stationary (e.g., dog chasing its own tail) and learning become unstable.\n",
    "   Instead, we keep the \"target\" stable for a while, and then (every _C_ steps) we update it to the\n",
    "   current parameters.\n",
    "\n",
    "We can see how the _blue_ box is mostly equivalent to Q Learning, but the _red_ part of the learning\n",
    "algorithm runs gradient descent on the Q network parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from typing import NamedTuple\n",
    "\n",
    "class Experience(NamedTuple):\n",
    "    \"\"\"An Experience stored in the replay buffer.\"\"\"\n",
    "    state:      np.array\n",
    "    action:     int\n",
    "    reward:     float\n",
    "    next_state: np.array\n",
    "    done:       bool\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"The replay buffer for DQN.\"\"\"\n",
    "    def __init__(self, buffer_size=int(1e4)):\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        # TODO: Append a new Experience in the memory.\n",
    "        pass\n",
    "\n",
    "    def sample(self, batch_size: int = 32):\n",
    "        # TODO: Select a new batch of experiences from the memory.\n",
    "        #       Hint: use np.arange and np.random.choice.\n",
    "        all_indices = None\n",
    "        selection = None\n",
    "        return self.unpack(selection)\n",
    "\n",
    "    def unpack(self, selection):\n",
    "        # TODO: Select the experiences from the memory (filtering out missing indexes).\n",
    "        experiences = None\n",
    "        states, actions, rewards, next_states, dones = None\n",
    "        # TODO: Convert the selections to PyTorch tensors, stacking the arrays.\n",
    "        #       Hint: use torch.from_numpy and np.stack/vstack (in case of scalars). Also, remember\n",
    "        #       to convert to the appropriate tensor type (float() or long())\n",
    "        states      = None\n",
    "        actions     = None\n",
    "        rewards     = None\n",
    "        next_states = None\n",
    "        dones       = None\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO: Returns the number of experiences in the replay buffer.\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer minimal test.\n",
    "test_buffer = ReplayBuffer()\n",
    "\n",
    "def fake_state():\n",
    "    return np.random.rand(4, 5)\n",
    "\n",
    "test_buffer.add(fake_state(), 10, 1.0, fake_state(), False)\n",
    "test_buffer.add(fake_state(), 11, 2.0, fake_state(), False)\n",
    "test_buffer.add(fake_state(), 12, 3.0, fake_state(), False)\n",
    "test_buffer.add(fake_state(), 13, 4.0, fake_state(), False)\n",
    "test_buffer.add(fake_state(), 14, 5.0, fake_state(), True)\n",
    "\n",
    "batch_size = 3\n",
    "state_shape = (batch_size, 4, 5)\n",
    "\n",
    "t_states, t_actions, t_rewards, t_next_states, t_dones = test_buffer.sample(batch_size)\n",
    "\n",
    "assert t_states.shape      == state_shape\n",
    "assert t_actions.shape     == (batch_size, 1)\n",
    "assert t_rewards.shape     == (batch_size, 1)\n",
    "assert t_next_states.shape == state_shape\n",
    "assert t_dones.shape       == (batch_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_episode(env: gym.Env):\n",
    "    \"\"\"Method to call to start a new episode for pong in DQN training.\"\"\"\n",
    "    state, _ = env.reset()\n",
    "    obs, _, _, _, _ = env.step(1) # Starts the game :)\n",
    "    return np.stack([state, obs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.gymnastics import epsilon_gen\n",
    "\n",
    "def train(env, agent, max_timesteps=int(1e6)) -> list[int]:\n",
    "    # Records all episode scores.\n",
    "    scores = []\n",
    "    # Tracks the current episode score.\n",
    "    score  = 0.0\n",
    "    # Tracks the current episode number.\n",
    "    n_episode = 1\n",
    "    # TODO: Create the epsilon generator. Start: 0.1, decay: 0.995, min: 0.01.\n",
    "    eps_gen = None\n",
    "\n",
    "    # TODO: Get the next epsilon.\n",
    "    epsilon = None\n",
    "    # TODO: Get the first state.\n",
    "    state = None\n",
    "    # Run DQN training for max_timesteps.\n",
    "    for t in range(max_timesteps):\n",
    "        # TODO: Select an action calling agent.act passing state and epsilon.\n",
    "        action = None\n",
    "        # TODO: Make a step in the environment with the selection action.\n",
    "        observation, reward, terminated, truncated, _ = None\n",
    "        # TODO: Build the new state (i.e., two stacked frames).\n",
    "        next_state = None\n",
    "        # Determines if the episode ended\n",
    "        done = terminated or truncated\n",
    "        # TODO: Call agent.step with (state, action, reward, next_state, done). That will take care\n",
    "        #       of collecting experiences and learning - we'll see later :)\n",
    "        # <HERE CALL AGENT.STEP>\n",
    "        # Prepares for the next iteration, updating score and state, and checking for completion.\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done: # Ends the episode\n",
    "            scores.append(score)\n",
    "            avg = np.mean(scores[-25:])\n",
    "            print(f'\\rEpisode {n_episode}\\tScore: {score:.2f}\\tT={t:6} (avg={avg:.2f})',\n",
    "                  end=\"\\n\" if n_episode % 25 == 0 else \"\")\n",
    "            score = 0.0\n",
    "            n_episode += 1\n",
    "            epsilon = next(eps_gen)\n",
    "            state = start_episode(env)\n",
    "\n",
    "    # Checkpoint the agent at the end of training, also save the scores for plotting.\n",
    "    agent.checkpoint()\n",
    "    np.savetxt(\"dqn_scores.csv\", np.asarray(scores, dtype=np.int16), delimiter=\",\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Agent that interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size=6, gamma: float = 0.99, tau: float = 1e-3, lr: float = 1e-4,\n",
    "                 batch_size: int = 32, learn_every: int = 4, update_target_every: int = 2,\n",
    "                 preload_file: str = None):\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.learn_every = learn_every\n",
    "        self.update_target_every = update_target_every\n",
    "        self.t_learn_step = 0\n",
    "        self.t_update_target_step = 0\n",
    "\n",
    "        # TODO: Create the replay buffer.\n",
    "        self.memory = None\n",
    "\n",
    "        # TODO: Create both the local and target networks.\n",
    "        self.qnetwork_local  = None\n",
    "        self.qnetwork_target = None\n",
    "        # TODO: Copy the weights of the local network in the target one. Hint use state_dict() and\n",
    "        #       load_state_dict methods.\n",
    "\n",
    "        # TODO: Remember to set the target network only in eval mode.\n",
    "\n",
    "        # Creating the optimizer.\n",
    "        self.optimizer = optim.RMSprop(self.qnetwork_local.parameters(), lr=self.lr)\n",
    "\n",
    "        # If we want to preload a saved network, we do it here.\n",
    "        if preload_file is not None:\n",
    "            print(f'Loading pre-trained model: {preload_file}')\n",
    "            self.qnetwork_local.load_state_dict(torch.load(preload_file, map_location=DEVICE))\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Tells the agent to make a step: record experience and possibly learn.\"\"\"\n",
    "        # TODO: Save experience in replay memory\n",
    "        # TODO: Update t_learn_step to determine when to learn (every \"learn_every\" time steps).\n",
    "        self.t_learn_step = None\n",
    "        if self.t_learn_step == 0:\n",
    "            # TODO: If enough samples are available in memory, get random subset and learn.\n",
    "            pass\n",
    "        # Update target network every \"update_target_every\" step.\n",
    "        self.t_update_target_step = (self.t_update_target_step + 1) % self.update_target_every\n",
    "        if self.t_update_target_step == 0:\n",
    "            self.soft_update_model_params(self.qnetwork_local, self.qnetwork_target, self.tau)\n",
    "\n",
    "    def act(self, state: np.array, eps=0.):\n",
    "        \"\"\"Makes the agent take an action for the state passed as input.\"\"\"\n",
    "        # TODO: Convert the state to a torch.Tensor\n",
    "        state = None\n",
    "        # TODO: Set the local network to eval, probe it to get the action values, and set it back\n",
    "        #       to training mode. Remember to use torch.no_grad().\n",
    "        action_values = None\n",
    "        # TODO: Perform epsilon-greedy action selection based on eps.\n",
    "        #       Hint: either np.argmax or random.choice across all actions.\n",
    "        return None\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Executes one learning step for the agent.\"\"\"\n",
    "        # TODO: Select a batch of experiences from the replay buffer\n",
    "        experiences = None\n",
    "        # TODO: Unpack them\n",
    "        states, actions, rewards, next_states, dones = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # TODO: Get the predicted action values of the *NEXT* states from the target model.\n",
    "            target_action_values = None # (batch_s, action_s)\n",
    "            # TODO: Select the max action value for each state:\n",
    "            #       Hint: https://pytorch.org/docs/stable/generated/torch.amax.html\n",
    "            max_action_values = None  # (batch_size, 1)\n",
    "            # TODO: Then, compute the Q _targets_ for the current states.\n",
    "            Q_targets = None # (batch_size, 1)\n",
    "\n",
    "        # TODO: Get the predicted Q values from local model...\n",
    "        predictions = None\n",
    "        # TODO: ...but choose only the action value that was selected in the experience replay.\n",
    "        Q_expected = None\n",
    "        # TODO: Compute loss. Hint: use the Huber Loss.\n",
    "        loss = None\n",
    "\n",
    "        # TODO: Minimize the loss. Hint: zero_grad the optim, backward on loss, step the optim.                \n",
    "\n",
    "    @staticmethod\n",
    "    def soft_update_model_params(src: torch.nn.Module, dest: torch.nn.Module, tau=1e-3):\n",
    "        \"\"\"Soft updates model parameters (θ_dest = τ * θ_src + (1 - τ) * θ_dest).\"\"\"\n",
    "        # TODO: For each dest parameter (get them via the parameters() function), update it with\n",
    "        #       the update-rule in the method description. Hint: use data.copy_ of the parameter.\n",
    "        pass\n",
    "\n",
    "    def checkpoint(self):\n",
    "        \"\"\"Save the QNetwork weights in a file.\"\"\"\n",
    "        torch.save(self.qnetwork_local.state_dict(), 'dqn_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training and Simulation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_simulation():\n",
    "    pretrained_agent = Agent(preload_file='solution/dqn_weights_pre.pth')\n",
    "    pretrained_scores = np.loadtxt(f'solution/dqn_scores_pre.csv', delimiter=',').astype(np.int16)\n",
    "    plot_scores(pretrained_scores)\n",
    "    return gym_simulate(pretrained_agent)\n",
    "\n",
    "# Uncomment the line below to watch the pretrained agent :)\n",
    "# pretrained_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gym.make(\"PongDeterministic-v4\", frameskip=1) as env:\n",
    "    env = init_random(env)\n",
    "    env = AtariPreprocessing(env)\n",
    "    agent = Agent(action_size=env.action_space.n)\n",
    "    scores = train(env, agent)\n",
    "\n",
    "plot_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_simulate(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where To Go From Here\n",
    "\n",
    " * Some tricks: e.g., acting only every other step and collect two steps help speeding up and\n",
    "   stabilizing the training.\n",
    " * Improvements: prioritized experience reply, double DQN, up to Rainbow.\n",
    " * Check out official implementations such as Stable Baselines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
