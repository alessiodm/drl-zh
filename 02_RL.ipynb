{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "Reinforcement learning algorithms solve MDP-based control problems having the agent interact with\n",
    "the environment (i.e., sampling) and learning from its experience. In order to do so, the agent will\n",
    "try to maximize the expected return, or _expected cumulative reward_ (reward hypothesis).\n",
    "\n",
    "## Type of Tasks\n",
    "\n",
    "Before we proceed, let's introduce some useful concepts. There are two types of RL tasks:\n",
    "\n",
    " * **Episodic:** The task has one or more end (or _terminal_) states. For example, our grid world\n",
    "   robot task ends either in the bomb or in the target.\n",
    " * **Continuous:** The task never ends, there are no terminal states. The agent keeps interacting\n",
    "   with the environment (e.g., stock trading agent).\n",
    "\n",
    "## Approaches\n",
    "\n",
    "Finally, there are two main approaches for solving an RL problem:\n",
    "\n",
    " * **Value-based:** The agent learns an optimal value function (e.g., $Q(s,a)$), and then derives\n",
    "   the optimal policy from it (see previous lecture). For now, we will focus on these methods.\n",
    " * **Policy-based:** The agent learns an optimal policy directly. We will learn about a popular\n",
    "   category of algorithms of this kind later on (i.e., _policy gradient_ methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility.\n",
    "from util.gymnastics import init_random\n",
    "init_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Updated Grid World\n",
    "\n",
    "To examine our RL algorithms, we will use a slightly more complicated version of our grid world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.gridworld import Grid, GridMDP, run_simulation, RANDOM_POLICY\n",
    "\n",
    "WORLD_GRID = Grid([\n",
    "    'EEEEE',\n",
    "    'EWTNG',\n",
    "    'SEEEW',\n",
    "])\n",
    "\n",
    "MDP = GridMDP(WORLD_GRID, gamma=0.9)\n",
    "\n",
    "run_simulation(MDP, RANDOM_POLICY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential Smoothing\n",
    "\n",
    "How do we refine the expected value of a variable `X` given a series of samples? One technique that\n",
    "we will use extensively is iteratively applying an\n",
    "[_exponential moving average_](https://en.wikipedia.org/wiki/Exponential_smoothing) of the samples.\n",
    "\n",
    "$$\n",
    "X_{t+1} =  (1 - \\alpha) X_t + \\alpha X_{t+1}\n",
    "$$\n",
    "\n",
    "Basically, the adjusted value after sampling is a \"blend\" of the old value adjusted by the scaled\n",
    "value of the new sample. $\\alpha$ is the scaling factor (or, _learning rate_). We can rewrite the\n",
    "above as:\n",
    "\n",
    "$$\n",
    "X_{t+1} =  X_t + \\alpha (X_{t+1} - X_t)\n",
    "$$\n",
    "\n",
    "Where the difference between the new and old value is effectively an error $\\delta$ scaled by the\n",
    "learning rate $\\alpha$. You will see why this formula is very important shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Samples 1M values from a normal distribution with mean pi.\n",
    "samples = np.random.normal(loc=3.14159265, scale=1.618033, size=(1_000_000,))\n",
    "\n",
    "def estimate_mean(samples, alpha=0.0001) -> float:\n",
    "    \"\"\"Approximates the expected value using exponential smoothing.\"\"\"\n",
    "    value = samples[0]\n",
    "    for sample in samples[1:]:\n",
    "        # TODO: Update the value using the exponential smoothing formula.\n",
    "        value = None\n",
    "    return value\n",
    "\n",
    "# This should print a value very close to 3.14 :)\n",
    "print(f'Estimated expected value: {estimate_mean(samples):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Methods\n",
    "\n",
    "Monte Carlo methods apply to episodic tasks. The intuition is that we can simulate entire episodes\n",
    "and compute the actual return with which we can then update our value function.\n",
    "\n",
    "$$\n",
    "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [G_t - Q(S_t, A_t)] \n",
    "$$\n",
    "\n",
    "Monte Carlo methods do not introduce _bias_ in the estimation (because they use the actual return at\n",
    "the end of the episode), but they might have high _variance_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.gridworld import GridEnv, State, Action\n",
    "from typing import Callable, TypeAlias\n",
    "\n",
    "# For simplicity, we define a policy as a function that returns an action given a state.\n",
    "# TODO: Define the 'Policy' typealias.\n",
    "Policy:  TypeAlias = None\n",
    "\n",
    "# An episode instead is a list of tuples (state, action, reward).\n",
    "# TODO: Define the 'Episode' typealias.\n",
    "Episode: TypeAlias = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our definition\n",
    "assert Policy == Callable[[State], Action]\n",
    "assert Episode == list[tuple[State, Action, float]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env: GridEnv, policy: Policy, max_t=10) -> Episode:\n",
    "    \"\"\"Generates an Monte Carlo episode in the Grid World environment GridEnv.\"\"\"\n",
    "    t = 0\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while t < max_t:\n",
    "        # TODO: select an action via the policy\n",
    "        action: Action = None\n",
    "        # TODO: get next_state, reward, done from the environment.\n",
    "        next, reward, done = None\n",
    "        # TODO: record the step in the episode list.\n",
    "        # TODO: Update state, time, and check for completion.\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test_env = GridEnv(MDP)\n",
    "test_policy = lambda _: Action.LEFT\n",
    "test_episode = generate_episode(test_env, test_policy, max_t=3)\n",
    "assert len(test_episode) == 3\n",
    "assert test_episode[0] == (State(0, 0), Action.LEFT, 0.0)\n",
    "assert test_episode[1] == (State(0, 0), Action.LEFT, 0.0)\n",
    "assert test_episode[2] == (State(0, 0), Action.LEFT, 0.0)\n",
    "\n",
    "test_policy = lambda s: Action.UP if s == State(3, 0) else Action.RIGHT\n",
    "test_episode = generate_episode(test_env, test_policy)\n",
    "assert len(test_episode) == 4\n",
    "assert test_episode[0] == (State(0, 0), Action.RIGHT, 0.0)\n",
    "assert test_episode[1] == (State(1, 0), Action.RIGHT, 0.0)\n",
    "assert test_episode[2] == (State(2, 0), Action.RIGHT, 0.0)\n",
    "assert test_episode[3] == (State(3, 0), Action.UP, -9.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But _how_ do we choose the actions in our Monte Carlo algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy and ε-greedy Policy\n",
    "\n",
    "A _greedy_ policy always chooses the action that maximizes the Q function in the current state.\n",
    "\n",
    "An _ε-greedy_ policy chooses any other action (other than the best action) with probability\n",
    "$\\frac{\\epsilon}{n_A}$, where $n_A$ is the number of available actions. Hence, if ε is `1`,\n",
    "the policy becomes the random policy; if ε is `0`, the policy is greedy instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.gridworld import QTable\n",
    "\n",
    "def greedy_policy(qtable: QTable) -> Policy:\n",
    "    \"\"\"Returns the greedy policy for the specified QTable.\"\"\"\n",
    "    # TODO: Return the policy lambda for action selection using the best action from QTable.\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the implementation\n",
    "state_0 = State(0, 0)\n",
    "state_1 = State(1, 0)\n",
    "test_qtable = QTable([state_0, state_1], list(Action))\n",
    "\n",
    "test_qtable[state_0, Action.DOWN]  = 0.5\n",
    "test_qtable[state_0, Action.LEFT]  = 1.5\n",
    "test_qtable[state_0, Action.RIGHT] = 0.8\n",
    "test_qtable[state_1, Action.UP]    = 0.1\n",
    "\n",
    "test_greedy_policy = greedy_policy(test_qtable)\n",
    "\n",
    "assert test_greedy_policy(state_0) == Action.LEFT\n",
    "assert test_greedy_policy(state_1) == Action.UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.gridworld import QTable\n",
    "\n",
    "def epsilon_greedy_policy(qtable: QTable, epsilon: float) -> Policy:\n",
    "    \"\"\"Returns the epsilon-greedy policy for the specified QTable.\"\"\"\n",
    "    def choose_action(state: State):\n",
    "        # TODO: The probability of the best action is (1 - epsilon + epsilon / nA), while the\n",
    "        #       other actions have probability (epsilon / nA). Compute those and choose the action.\n",
    "        #       Use np.random.choice for sampling :)\n",
    "        return None\n",
    "    return choose_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the implementation\n",
    "# Approximate tests... hopefully good enough to find big bugs :)\n",
    "state_0 = State(0, 0)\n",
    "state_1 = State(1, 0)\n",
    "test_qtable = QTable([state_0, state_1], list(Action))\n",
    "\n",
    "test_qtable[state_0, Action.DOWN]  = 0.5\n",
    "test_qtable[state_0, Action.LEFT]  = 1.5\n",
    "test_qtable[state_0, Action.RIGHT] = 0.8\n",
    "test_qtable[state_1, Action.UP]    = 0.1\n",
    "\n",
    "def probe_actions(policy, state) -> list[Action]:\n",
    "    return dict.fromkeys([policy(state) for _ in range(5_000)])\n",
    "\n",
    "test_egreedy_policy = epsilon_greedy_policy(test_qtable, epsilon=1.0)\n",
    "assert len(probe_actions(test_egreedy_policy, state_0)) == 4\n",
    "assert len(probe_actions(test_egreedy_policy, state_1)) == 4\n",
    "\n",
    "test_egreedy_policy = epsilon_greedy_policy(test_qtable, epsilon=0.0)\n",
    "assert len(probe_actions(test_egreedy_policy, state_0)) == 1\n",
    "assert len(probe_actions(test_egreedy_policy, state_1)) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This implementation technique (i.e., returning a function) is not optimal and definitely\n",
    "not efficient. But it serves well for learning purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decay Epsilon with Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_gen(eps_start=1.0, eps_decay=0.99999, eps_min=0.05):\n",
    "    \"\"\"Convenient generator function to generate and decy epsilon.\"\"\"\n",
    "    # TODO: eps starts at eps_start :)\n",
    "    eps = None\n",
    "    while True:\n",
    "        # TODO: Yield and update eps, selecting the max(eps * decay, eps_min).\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the implementation!\n",
    "eps = epsilon_gen(1.0, 0.5, 0.1)\n",
    "\n",
    "assert next(eps) == 1.0\n",
    "assert next(eps) == 0.5\n",
    "assert next(eps) == 0.25\n",
    "assert next(eps) == 0.125\n",
    "assert next(eps) == 0.1\n",
    "assert next(eps) == 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo(env: GridEnv, num_episodes, alpha=0.02, eps_start=1.0, start_q: QTable = None):\n",
    "    \"\"\"A Monte Carlo algorithm for reinforcement learning.\"\"\"\n",
    "    # TODO: Initialize the QTable (use start_q if provided, you'll see why later)\n",
    "    Q = None\n",
    "\n",
    "    # TODO: Prepare to generate epsilon creating the generator.\n",
    "    epsilon = None\n",
    "\n",
    "    # Iterate until we reached the maximum number of episodes for learning.\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        # TODO: Generate an episode by following epsilon-greedy policy.\n",
    "        policy = None\n",
    "        episode = None\n",
    "\n",
    "        # TODO: Unpack the episode in a tuple of (list[states], list[actions], list[rewards]).\n",
    "        #       Hint: use the zip function!\n",
    "        states, actions, rewards = None\n",
    "\n",
    "        # TODO: Conveniently compute the discounts first. We can do this b/c we can compute all the\n",
    "        #       expected returns at each timestep (having all the rewards).\n",
    "        #       The discounts are: [1, gamma, gamma^2, gamma^3, ...] for the length of the episode.\n",
    "        discounts = None\n",
    "\n",
    "        # For each step / transition in the environment, let's update the QTable according to the\n",
    "        # update rule of Monte Carlo methods defined above.\n",
    "        for t, state in enumerate(states):\n",
    "            # TODO: Get the action at timestep `t` and the current state-action value.\n",
    "            action = None\n",
    "            old_Q = None\n",
    "            # TODO: Compute the total return. Recall that:\n",
    "            #       G_0 = R_1 + gamma * R_2 + gamma^2 * R_3 + ... (R1 is found at index 0)\n",
    "            #       Hint: sum rewards _from_ `t` onward, while select discounts _until_ `t`. That is\n",
    "            #       because discounts always start from the beginning even if rewards \"shift\".\n",
    "            G_t = None\n",
    "            Q[state, action] = None\n",
    "\n",
    "        # Monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(f\"\\rEpisode {i_episode}/{num_episodes}.\", end=\"\")\n",
    "\n",
    "    # TODO: Determine the optimal policy  (i.e., the greedy policy on the computed QTable).\n",
    "    policy = None\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = GridEnv(MDP)\n",
    "\n",
    "# With minimal exploration\n",
    "biased_q = QTable(ENV.mdp.all_states, ENV.mdp.all_actions)\n",
    "biased_q[State(0, 0), Action.RIGHT] = 0.1\n",
    "biased_q[State(1, 0), Action.RIGHT] = 0.1\n",
    "biased_q[State(2, 0), Action.UP]    = 0.1\n",
    "\n",
    "minimal_exploration_policy, Q = monte_carlo(ENV, 100_000, eps_start=0.05, start_q=biased_q)\n",
    "run_simulation(ENV.mdp, minimal_exploration_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With exploration\n",
    "optimal_policy, Q = monte_carlo(ENV, 100_000, start_q=biased_q)\n",
    "run_simulation(ENV.mdp, optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration vs. Exploitation Tradeoff\n",
    "\n",
    "We can prove that MC converges to the optimal policy when these conditions are met:\n",
    "\n",
    " * Every state-action pair is visited infinitely many times; and\n",
    " * The policy converges to a policy that is greedy with respect to the action-value function Q.\n",
    "\n",
    "These are called the GLIE (Greedy in the Limit with Infinite Exploration) conditions, and guarantee\n",
    "that the agent continues exploring for all time steps, and grdually exploits more exploring less.\n",
    "\n",
    "Try to tune `alpha` and `epsilon` and see the effects on the performance of the algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference Methods\n",
    "\n",
    "Monte Carlo methods need to wait until the end of the episode to update our value estimates. Could\n",
    "we find a way to integrate knowledge earlier than that? Temporal Difference (TD) methods come to the\n",
    "rescue: they update the current value estimate based on the immediate reward and _another_ estimate.\n",
    "Hence, the update rule looks something like this:\n",
    "\n",
    "$$\n",
    "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\{[R_t + \\gamma V^{estimate}(S_{t+1})] - Q(S_t, A_t)\\}\n",
    "$$\n",
    "\n",
    "\n",
    "$R_t + \\gamma V^{estimate}(S_{t+1})$ is called the _TD target_. If our estimate was perfect, you can\n",
    "notice it is effectively equivalent to $G_t$. Then, the _TD error_ $\\delta^{TD}$ is:\n",
    "$TD^{target} - Q(S_t, A_t)$. The equation above can be simplified as:\n",
    "\n",
    "$$\n",
    "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\delta^{TD}\n",
    "$$\n",
    "\n",
    "Choosing the estimate of the value function is what differentiates various TD algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Learning\n",
    "\n",
    "Q Learning is probably the most popular TD algorithm. In Q Learning, we choose the estimate of the\n",
    "next value as: $\\max Q(S_{t+1}, A_{t+1})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env: GridEnv, num_episodes, alpha=0.02, max_t=10):\n",
    "    \"\"\"Runs Q Learning.\"\"\"\n",
    "    # TODO: Initialize the QTable, and the epsilon generator.\n",
    "    Q = None\n",
    "    epsilon = None\n",
    "    # Run for the maximum number of episodes passed as input.\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        t = 0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            # TODO: Select an action with an epsilon greedy policy using Q.\n",
    "            policy = None\n",
    "            action = None\n",
    "            # TODO: Interact with the environment\n",
    "            next_state, reward, done = None\n",
    "            # TODO: Store the current Q(s,a) value.\n",
    "            cur_value = None\n",
    "            # TODO: Determine the next_action using maxQ.\n",
    "            next_action = None\n",
    "            # TODO: Compute the TD target.\n",
    "            td_target = None\n",
    "            # TODO: Compute the TD error.\n",
    "            td_error  = None\n",
    "            # TODO: Update Q with the temporal-difference update rule.\n",
    "            Q[state, action] = None\n",
    "\n",
    "            # Update the state for the next cycle, and check for episode completion.\n",
    "            state = next_state\n",
    "            t = t + 1\n",
    "            if done or t >= max_t:\n",
    "                break\n",
    "        # Monitor and debugging messages.\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "    # TODO: Return the optimal policy as the greedy policy on Q.\n",
    "    policy = None\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_policy, Q = q_learning(ENV, 100_000)\n",
    "run_simulation(ENV.mdp, opt_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to converge, Q Learning needs to explore enough and eventually make the learning rate small\n",
    "enough, but not decrease it too quickly either... :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off-Policy vs. On-Policy\n",
    "\n",
    "Q Learning is an _off-policy_ algorithm: the policy function is learnt by estimating the value of\n",
    "the next state using a separate policy (i.e., the greedy policy, maximizing the current Q value)\n",
    "compared to the policy that is actually followed (i.e., $\\epsilon$-greedy).\n",
    "\n",
    "That is different from _on-policy_ learning, where we learn and refine the policy function using\n",
    "actions taken via our current followed and learnt policy $\\pi(a|s)$. See [this post](https://stats.stackexchange.com/questions/184657/what-is-the-difference-between-off-policy-and-on-policy-learning) for a more in-depth explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Methods Limitations\n",
    "\n",
    "Cannot scale to large state / action spaces. Discretization is one approach, but there are better ones :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### N-step Bootstrapping\n",
    "\n",
    "Sometimes neither Monte Carlo nor TD are the best fit. We should think them like the end of a\n",
    "spectrum: TD uses the immediate next reward, MC uses all (possibly infinite) rewards. Nothing\n",
    "prevents us to use any intermetiate number of rewards and then an estimate: this process is called\n",
    "_n-step bootstrapping_. More details in the Sutton and Barto book :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlzh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
