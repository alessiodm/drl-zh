{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions ðŸŽ‰\n",
    "\n",
    "Congratulations on making it to the end of **Deep Reinforcement Learning: Zero to Hero!** We've\n",
    "journeyed from the fundamentals of Markov Decision Processes to building cutting-edge algorithms\n",
    "like PPO. You've trained agents to play games, land on the moon, and much more. I hope this course\n",
    "has given you a solid foundation and the confidence to continue exploring the fascinating world of\n",
    "Deep RL.\n",
    "\n",
    "The field of RL is constantly evolving, with new ideas and architectures emerging all the time. In\n",
    "this final notebook, we'll take a quick look at some of the exciting, advanced topics that are\n",
    "pushing the boundaries of what's possible. This is not an exhaustive list, but rather a starting\n",
    "point for your continued learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Advanced Topics ðŸš€\n",
    "\n",
    "Here are some of the advanced topics and recent breakthroughs in the field that we didn't cover in\n",
    "this course. They represent the current frontiers of RL research and are worth exploring further.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline Reinforcement Learning\n",
    "\n",
    "Offline RL (or batch RL) deals with learning from a fixed dataset of interactions, without the\n",
    "ability to explore the environment further. This is a very practical setting, as it allows us to\n",
    "leverage large, pre-existing datasets (e.g., from human demonstrations, or logs from a previous\n",
    "policy). For a good overview, you can check out this\n",
    "[Reddit discussion on Offline RL](https://www.reddit.com/r/reinforcementlearning/comments/utnhia/what_is_offline_reinforcement_learning/).\n",
    "The main challenge is to learn a good policy without being able to correct for distributional shift.\n",
    "Here are a few key algorithms in this area:\n",
    "\n",
    "- **Conservative Q-Learning (CQL)**: A popular offline RL algorithm that learns a lower bound of the\n",
    "  true Q-function to avoid overestimating the value of out-of-distribution actions. You can find the\n",
    "  original implementation in this [CQL GitHub Repository](https://github.com/young-geng/CQL).\n",
    "- **Implicit Q-Learning (IQL)**: An offline RL method that avoids querying values of unseen actions,\n",
    "  instead learning by extracting the best actions implicitly from the dataset.\n",
    "- **TD3+BC**: A simpler approach to offline RL that extends the TD3 algorithm with a behavioral\n",
    "  cloning term to keep the policy close to the data distribution. A more recent approach,\n",
    "  [Flow Q-Learning (FQL)](https://seohong.me/projects/fql/), builds on these ideas for even better\n",
    "  performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dreamer\n",
    "\n",
    "Dreamer is a model-based reinforcement learning agent that learns a world model of the environment\n",
    "and uses it to \"dream\" or imagine future trajectories to learn a policy. This approach is very\n",
    "sample-efficient, as the agent can learn from imagined experience, which is much cheaper than\n",
    "real-world interaction. You can read more about it in Google's blog post,\n",
    "[Introducing Dreamer](https://research.google/blog/introducing-dreamer-scalable-reinforcement-learning-using-world-models/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Transformers\n",
    "\n",
    "Decision Transformers reframe reinforcement learning as a sequence modeling problem. Instead of\n",
    "learning a policy or a value function, a\n",
    "[Decision Transformer](https://sites.google.com/berkeley.edu/decision-transformer) uses a GPT-like\n",
    "architecture to predict future actions based on a desired return, past states, and actions. This is\n",
    "a powerful approach for offline RL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow-based RL\n",
    "\n",
    "This is a newer area of research that uses normalizing flows (a type of generative model) to\n",
    "represent the policy. Flow-based models can represent complex, multi-modal action distributions,\n",
    "which can be beneficial in many real-world tasks. A great example of this is the recent\n",
    "[Flow Q-Learning (FQL)](https://seohong.me/projects/fql/) work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-Learning\n",
    "\n",
    "Meta-learning, or \"learning to learn,\" aims to create agents that can quickly adapt to new tasks. In\n",
    "the context of RL, a meta-learning agent is trained on a distribution of tasks, and the goal is for\n",
    "it to generalize and learn a new, unseen task with very few samples. For a deep dive, check out this\n",
    "comprehensive [Survey of Meta-Reinforcement Learning](https://arxiv.org/abs/2301.08028).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robotics and Vision-Language-Action Models (VLAs)\n",
    "\n",
    "A very exciting application of RL is in robotics. Vision-Language-Action models (VLAs) are a recent\n",
    "development where large, pre-trained models are used to control robots. These models can understand\n",
    "natural language instructions, see the world through a camera, and take actions to complete complex\n",
    "tasks.\n",
    "\n",
    "- **Gemini VLA**: Google's VLA model that shows impressive capabilities in robotics, leveraging the\n",
    "  power of the Gemini family of models. Read more about it in their post,\n",
    "  [Gemini Robotics: Bringing AI into the Physical World](https://deepmind.google/discover/blog/gemini-robotics-bringing-ai-into-the-physical-world/).\n",
    "- **$\\pi0$**: A Vision-Language-Action flow model for general robot control, from the\n",
    "  [Open-Source Physical Intelligence Project](https://www.pi.website/blog/openpi). You can also read\n",
    "  about their [first open-source model](https://www.pi.website/blog/pi05) and a new fine-tuning\n",
    "  technique called [Knowledge Insulation](https://www.pi.website/research/knowledge_insulation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Words\n",
    "\n",
    "The journey of learning is a continuous one. The topics above are just a glimpse of the exciting\n",
    "research happening in Deep Reinforcement Learning. I encourage you to pick one that fascinates you,\n",
    "read the papers, and maybe even try to implement it yourself! The skills you've developed in this\n",
    "course have prepared you for this next step.\n",
    "\n",
    "Thank you for taking this course. Keep learning, keep experimenting, and I can't wait to see what\n",
    "you'll build. Good luck! ðŸ¤–\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlzh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
