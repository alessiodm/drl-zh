{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Methods\n",
    "\n",
    "Policy _based_ methods learn the optimal policy directly, without necessarily estimating a value\n",
    "function. Policy _gradient_ methods do that performing gradient ascent on the objective function.\n",
    "\n",
    "### Advantages\n",
    "\n",
    " * No need to store action-values.\n",
    " * Ability to learn a stochastic policy direcly.\n",
    " * Hence, no need to manually tune exploitation vs. exploration.\n",
    " * Effective in continuous action spaces (and high-dimensional state spaces).\n",
    " * They generally have good convergence properties.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    " * They might have high-variance.\n",
    " * Might converge to a local maximum.\n",
    " * Slower than other methods, and might take a long time to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from typing import Union\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from util.gymnastics import DEVICE, gym_simulation, init_random, plot_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cart Pole Environment\n",
    "\n",
    "Explore the [Gymnasium Cart Pole environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/),\n",
    "and run the simulation below!\n",
    "\n",
    "**IMPORTANT**: For this notebook, we are going to tweak the reward function of the environment. See\n",
    "the \"Reward Function\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `gym_simulation` is a convenient utility to run simulations on Gynmasium environment, so that we\n",
    "# don't have to repeat the same code in all notebooks. But it works basically the same as the one we\n",
    "# implemented for DQN :) Feel free to check the source code!\n",
    "\n",
    "gym_simulation(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for convenience, we hardcode the state and action sizes of the CartPole environment.\n",
    "STATE_SIZE  = 4\n",
    "ACTION_SIZE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Function\n",
    "\n",
    "The Cart Pole reward function returns `+1` reward at every timestep: the idea is that the longer the\n",
    "pole stays in the upright position the better. Such reward function does not encapsulate whether an\n",
    "action is good or bad, and it does not play well with some concepts we are going to analyze later\n",
    "in this notebook (such as normalization and custom baselines).\n",
    "\n",
    "For this reason, we \"adjust\" the reward taking into account angle and position of the pole as a\n",
    "heuristic to evaluate how good is the next state (to which the agent transitioned given its action)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function works for both single and vectorized environments and rewards.\n",
    "def adjust_reward(next_state, reward) -> Union[np.float64, np.array]:\n",
    "    angle = next_state[2] if len(next_state.shape) == 1 else next_state[:, 2]\n",
    "    position = next_state[0] if len(next_state.shape) == 1 else next_state[:, 0]\n",
    "    return reward - np.abs(angle) / 0.418 - np.abs(position) / 4.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Rule\n",
    "\n",
    "For one trajectory $\\tau$ (or episode), the neural networks weight can be updated according to:\n",
    "\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_k - \\alpha \\sum_{t=0} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) R(\\tau)\n",
    "$$\n",
    "\n",
    "We can interpret this as pushing up probabilities for action / states combinations when the return\n",
    "is high, and the other way around for low returns.\n",
    "\n",
    "This relationship is also interesting because only the policy function needs to be differentiable:\n",
    "the reward function might very well be discontinuous and sparse.\n",
    "\n",
    "For derivation, check the [Hugging Face Deep RL tutorial](https://huggingface.co/learn/deep-rl-course/unit4/pg-theorem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE\n",
    "\n",
    "<div style=\"width: 70%\">\n",
    "  <img src=\"assets/04_PG_reinforce.png\">\n",
    "  <br>\n",
    "  <small>Sutton & Barto 2022</small>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, hidden_units=16):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        # TODO: Create two fully connected / linear layers, input dimension STATE_SIZE, output\n",
    "        #       dimension ACTION_SIZE, and hidden units specified in the constructor.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Use ReLU as first non-linearity, and softmax for the output layer (so that we can\n",
    "        #       interpret this as a stochastic policy across action probabilities). Hint: make sure\n",
    "        #       to choose the right dimension for the softmax! Another hint: it should be the last\n",
    "        #       dimension of the tensor (to be able to work with single and batched updates).\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent that will work with both single and vectorized environments, keep that in mind when thinking\n",
    "# about dimensionality and how to operate on tensors in a consistent way!\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.policy = PolicyNetwork()\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2)\n",
    "\n",
    "    def sample_action(self, state: np.array) -> tuple[np.array, torch.Tensor]:\n",
    "        \"\"\"Samples an action for a state from the policy network.\"\"\"\n",
    "        # TODO: Convert the state to a PyTorch tensor.\n",
    "        state = None\n",
    "        # TODO: Get the action probabilities from the policy.\n",
    "        probs = None\n",
    "        # TODO: Create a Categorical distribution with those probabilities.\n",
    "        cdist = None\n",
    "        # TODO: Sample the action from the categorical distribution, and gets its logprob.\n",
    "        action = None\n",
    "        logprob = None\n",
    "        # TODO: Return the action and convert it to be used by Gymnasium (hint: use cpu().numpy()).\n",
    "        #       Also return the the log_prob for that action as well as second return output.\n",
    "        return None\n",
    "\n",
    "    def learn(self, log_probs: list[torch.Tensor], returns: Union[np.float64, np.array]):\n",
    "        \"\"\"Perform a step of learning (gradient ascent) on the policy network.\"\"\"\n",
    "        # TODO: To handle both single and vectorized environments, rewards can be either a scalar or\n",
    "        #       an array. Let's create a corresponding tensor first.\n",
    "        returns = None\n",
    "        # TODO: Stack the log_probs (hint: use torch.stack)\n",
    "        log_probs = None\n",
    "        # TODO: Compute the policy loss. Hint: gradient ascent.... so negative sign!\n",
    "        policy_loss = None\n",
    "        # TODO: Perform a backprop step via the optimizer.\n",
    "        pass\n",
    "    \n",
    "    @torch.no_grad\n",
    "    def act(self, state):\n",
    "        \"\"\"Convenient method for the agent to select an action during simulation.\"\"\"\n",
    "        action, _ = self.sample_action(state[np.newaxis, :])\n",
    "        return action[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def REINFORCE(env: gym.Env, agent, max_episodes=10_000, max_t=1_000, gamma=0.9999):\n",
    "    # Tracks the score for each episode.\n",
    "    #\n",
    "    # IMPORTANT: in this notebook we modified the reward function but effectively we still consider\n",
    "    # the score as the original reward function. In fact, we just track the latest timestamp of the\n",
    "    # episode (representing how long we were able to balance the pole). This is important to know to\n",
    "    # properly read the \"score\" value and its meaning (which still encapsulates learning quality).\n",
    "    scores = []\n",
    "    # Loop for max_episodes.\n",
    "    for i_episode in range(1, max_episodes + 1):\n",
    "        # Store episode rewards.\n",
    "        rewards = []\n",
    "        # Store episode log probabilities.\n",
    "        log_probs = []\n",
    "        # Start the episode in the initial state.\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        # TODO: Generate an episode following the policy, for T (max_t) timesteps.\n",
    "        for t in range(max_t):\n",
    "            # TODO: sample action and log probability.\n",
    "            action, log_prob = None\n",
    "            # TODO: perform a step in the environment.\n",
    "            state, reward, terminated, truncated, _ = None\n",
    "            # Storing the reward (adjusting it for the purpose of this notebook).\n",
    "            reward = adjust_reward(state, reward)\n",
    "            rewards.append(adjust_reward(state, reward))\n",
    "            # TODO: store reward and log probability.\n",
    "            # TODO: Check for episode completion.\n",
    "\n",
    "        # Compute discounted return.\n",
    "        # TODO: First, compute the discounts.\n",
    "        discounts = None\n",
    "        # TODO: Then compute the total discounted return as the sum of the discouted rewards.\n",
    "        R = None\n",
    "\n",
    "        # TODO: Perform a learning step on the agent calling the `learn` method.\n",
    "\n",
    "        # Track scores and print statistics.\n",
    "        scores.append(t)\n",
    "        avg_duration = np.mean(scores[-100:])\n",
    "        if i_episode % 100 == 0:\n",
    "            print(f'Episode {i_episode}\\tAverage duration: {avg_duration:.2f}')\n",
    "        if avg_duration >= 490.0: # Solved\n",
    "            print(f'Environment solved at episode {i_episode}\\Avg. duration: {avg_duration:.2f}')\n",
    "            break\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with init_random(gym.make('CartPole-v1')) as env:\n",
    "    agent = Agent()\n",
    "    scores = REINFORCE(env, agent)\n",
    "plot_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_simulation(\"CartPole-v1\", agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "\n",
    "### Use Future Rewards\n",
    "\n",
    "First thing to notice is that we are using all rewards at every timestep. But really, we should only\n",
    "consider future rewards: i.e., the rewards that are actually the consequences of our actions.\n",
    "\n",
    "$$\n",
    "g = \\sum_t R_t^{future}\\nabla_{\\theta} log\\pi_{\\theta}(a_t | s_t)\n",
    "$$\n",
    "\n",
    "### Collect Multiple Trajectories\n",
    "\n",
    "In every episode, we sample a single trajectory: the gradient update might not contain good data\n",
    "about our policy and the stochastic updates might be very _noisy_. Learning happens because in the\n",
    "long run we hope that all tiny signals accumulate and converge towards the optimal policy.\n",
    "\n",
    "How do we reduce noise? The simplest strategy is to collect more trajectories (with the current\n",
    "policy) at once! And Gymnasium vectorized environment `gym.vector.Env` serves exactly that purpose!\n",
    "\n",
    "#### Normalize Rewards\n",
    "\n",
    "When collecting multiple trajectories, a technique we can use is to normalize the rewards across\n",
    "the various trajectories: which roughly picks half actions to encourage / discourage, and keeps the\n",
    "gradient updates moderate.\n",
    "\n",
    "$$\n",
    "R_k \\leftarrow \\frac{R_k - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "The rewards distribution will also likely change during training, and a reward that might be good at\n",
    "the beginning might actually signal a bad trajectory in late stages of training. Normalization helps\n",
    "with such cases as well.\n",
    "\n",
    "### Baseline Subtraction\n",
    "\n",
    "The idea is to subtract to the reward a _baseline_ $b$, for example the average reward along all\n",
    "trajectories (What if every trajectory has _always_ positive returns?). In this case, things that\n",
    "are above average will push their probabilities to happen up while things below average will be\n",
    "penalized.\n",
    "\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_k - \\alpha \\sum_{t=0} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) [R(\\tau) - b(s_t)]\n",
    "$$\n",
    "\n",
    "We can do this because on expectation this extra subtracted term will have zero effect (as long as\n",
    "it does not depend on the action in _logprob_), but overall we'll get reduced variance (proof left\n",
    "as exercise and/or you can find it in the resources).\n",
    "\n",
    "#### Advantage Function\n",
    "\n",
    "This value that we multiply with the log-probability to \"reinforce\" or \"depress\" the corresponding\n",
    "actions is called the _advantage function_ and plays a critical role in state-of-the-art algorithms:\n",
    "\n",
    "$$\n",
    "A(*) = R(\\tau) - b\n",
    "$$\n",
    "\n",
    "It measures how better the selected action does compared to the _average_ in the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note how we use a vectorized environment: gym.vector.VectorEnv!\n",
    "def REINFORCE_v2(env: gym.vector.VectorEnv, agent, max_episodes=10_000, max_t=1_000, gamma=0.9999,\n",
    "                 with_normalization=True, with_baseline=True):\n",
    "    # Tracks the score for each episode. IMPORTANT: same remark as in REINFORCE above!\n",
    "    scores = []\n",
    "    # Loop for max_episodes.\n",
    "    for i_episode in range(1, max_episodes + 1):\n",
    "        # Store episode rewards, log probabilities, *AND* episode states for baseline computation!\n",
    "        rewards, log_probs, states = ([], [], [])\n",
    "        # Start the episode in the initial state.\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        # TODO: Generate an episode following the policy. Copy the code above :) But...\n",
    "        for t in range(max_t):\n",
    "            # ...pay attention! We are now using a vectorized environment! Hence, to determine\n",
    "            # episode completion we should check if `any()` is terminated or truncated!\n",
    "            pass\n",
    "\n",
    "        # Compute discounted _future_ returns.\n",
    "        # TODO: Compute the discounts and discounted rewards.\n",
    "        discounts = None\n",
    "        # TODO: Pay attention to dimensionality of rewards (trajectory_len, n_envs), and how to\n",
    "        #       broadcast discounts appropriately! Hint: use np.newaxis to add a dimension.\n",
    "        discounted_rewards = None\n",
    "        # TODO: Compute the future_returns. Hint: consider cumulative sums in reverse order :)\n",
    "        #       But again, pay attention in which axis to perform the cumulative sum!\n",
    "        future_returns = None\n",
    "\n",
    "        # If the `with_baseline` argument flag is enabled, subtract a baseline to future returns.\n",
    "        if with_baseline:\n",
    "            # TODO: Compute the state-dependent baseline. Feel free to explore different baselines!\n",
    "            #       As a suggestion, use the product of the angle and angular velocity: it provides\n",
    "            #       a proxy about how well we are contrasting the pole fall.\n",
    "            #       Hint: use the tracked `states` to compute the time dependent baseline.\n",
    "            baseline = None\n",
    "            # TODO: Subtract the baseline from future_returns.\n",
    "            future_returns = None\n",
    "\n",
    "        # If the `with_normalization` argument flag is enabled, normalize future returns.\n",
    "        # Note: normalize across multiple trajectories of the vectorized env for the same timestamp.\n",
    "        if with_normalization:\n",
    "            # TODO: Compute the mean. Hint: use np.mean on the proper axis.\n",
    "            returns_mean = None\n",
    "            # TODO: Compute the stdev. Hint: make sure the std is non zero; use np.std.\n",
    "            returns_std = None\n",
    "            # TODO: Normalize the future returns using mean and stdev.\n",
    "            future_returns = None\n",
    "\n",
    "        # TODO: Perform a learning step calling agent.learn(...)\n",
    "        # copy() for negative strides :(\n",
    "        #   https://discuss.pytorch.org/t/negative-strides-in-tensor-error/134287/2\n",
    "\n",
    "        # Track scores and print statistics\n",
    "        scores.append(t)\n",
    "        avg_duration = np.mean(scores[-100:])\n",
    "        if i_episode % 100 == 0:\n",
    "            print(f'Episode {i_episode}\\tAverage duration: {avg_duration:.2f}')\n",
    "        if avg_duration >= 490.0: # Solved\n",
    "            print(f'Environment solved at episode {i_episode}\\tAvg. duration: {avg_duration:.2f}')\n",
    "            break\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with init_random(gym.vector.make('CartPole-v1', num_envs=5)) as env:\n",
    "    agent_v2 = Agent()\n",
    "    scores_v2 = REINFORCE_v2(env, agent_v2)\n",
    "plot_scores(scores_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_simulation(\"CartPole-v1\", agent_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Can We Do Better?\n",
    "\n",
    "There are other improvements that can be applied:\n",
    "\n",
    "1. Actor-critic setup (with value function baseline) and advanced advantage estimation such as _GAE_\n",
    "  will improve learning.\n",
    "2. We are currently _discarding experiences_ after every learning step. That is because the policy\n",
    "  effectively changes. But we'll see that with importance sampling we can iterate on the same data\n",
    "  multiple times and learn in mini-batches!\n",
    "3. Techniques such as \"_trust region_\" and \"_loss clipping_\" will help against degeneration and keep\n",
    "  the policy learning along smooth gradient directions.\n",
    "\n",
    "Once we put all of these in place... we'll have PPO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### Meaning of Loss\n",
    "\n",
    "Note that the loss function used in policy gradient methods doesn't have the same meaning of the\n",
    "typical supervised learning setup. In particular, after that first step of gradient descent, there\n",
    "is no more connection to performance - which is determined by the average return.\n",
    "\n",
    "The loss function is only useful when evaluated at the current parameters to perform one step of\n",
    "gradient ascent. After that it loses its meaning and it's value shouldn't be used as a metric for\n",
    "performance.\n",
    "\n",
    "More details on [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#id14).\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "You can check how REINFORCE with future rewards + normalization + well crafted baseling performs\n",
    "better than the vanilla REINFORCE.\n",
    "\n",
    "Interesting to observe how we needed to tweak the reward function to reach these results: in fact,\n",
    "formulating the reward function properly is one of the major challenges in RL and an area of open\n",
    "research (e.g., curiosity driven agents).\n",
    "\n",
    "As an exercise, try removing the adjustment of rewards in this notebook and just keep the original\n",
    "Cart Pole `+1` per timestep reward. What's the effect on the algorithms' variants? Hint: check how\n",
    "catastrophic learning is with normalization enabled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "init_random()\n",
    "base, norm, all = ([], [], [])\n",
    "random_seeds = np.random.randint(3_141_592, size=10)\n",
    "for seed in random_seeds:\n",
    "    with init_random(gym.vector.make('CartPole-v1', num_envs=5), seed=int(seed)) as env:\n",
    "        print('Future rewards only:')\n",
    "        agent_v3 = Agent()\n",
    "        scores_v3 = REINFORCE_v2(env, agent_v3, with_normalization=False, with_baseline=False)\n",
    "        base.append(len(scores_v3))\n",
    "\n",
    "    with init_random(gym.vector.make('CartPole-v1', num_envs=5), seed=int(seed)) as env:\n",
    "        print('Future rewards + normalization:')\n",
    "        agent_v3_b = Agent()\n",
    "        scores_v3_b = REINFORCE_v2(env, agent_v3_b, with_normalization=True, with_baseline=False)\n",
    "        norm.append(len(scores_v3_b))\n",
    "\n",
    "    with init_random(gym.vector.make('CartPole-v1', num_envs=5), seed=int(seed)) as env:\n",
    "        print('Future rewards + normalization + baseline:')    \n",
    "        agent_v3_bn = Agent()\n",
    "        scores_v3_bn = REINFORCE_v2(env, agent_v3_bn, with_normalization=True, with_baseline=True)\n",
    "        all.append(len(scores_v3_bn))\n",
    "    print()\n",
    "\n",
    "x = np.arange(len(norm))\n",
    "plt.figure('Episode scores')\n",
    "plt.plot(x, base, label='Future rewards')\n",
    "plt.plot(x, norm, 'r', label='Future rewards + normalization')\n",
    "plt.plot(x, all, 'g', label='Future rewards + normalization + baseline')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlzh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
