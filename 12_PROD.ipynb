{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production-Ready Reinforcement Learning\n",
    "\n",
    "Let's time to bridge the gap between theoretical knowledge and real-world application. This notebook\n",
    "introduces the essential techniques and tools for making RL agents production-ready.\n",
    "\n",
    "### What We'll Cover:\n",
    "\n",
    "1.  **Code Structure & Refactoring**: How to structure code to be modular and reusable.\n",
    "2.  **Metrics & Monitoring**: Using Tensorboard to track losses, rewards, and gradients.\n",
    "3.  **Checkpointing**: Saving and loading models to resume training or for inference.\n",
    "4.  **Debugging & Troubleshooting**: Common techniques for when your RL agent isn't learning.\n",
    "5.  **The Importance of Multiple Seeds**: Ensuring your results are robust and reproducible.\n",
    "6.  **Scaling with Parallelization (Ray)**: Speeding up training with parallel environments.\n",
    "7.  **Automated Hyperparameter Tuning (Optuna)**: Finding the best hyperparameters automatically.\n",
    "8.  **Other Production Techniques**: A look at MLOps, CI/CD, and Safe RL.\n",
    "\n",
    "Let's dive in!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Setup\n",
    "\n",
    "First, let's import the necessary libraries. We'll be using PyTorch, Gymnasium, and several new\n",
    "libraries for our production techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "import ray\n",
    "import optuna\n",
    "import copy\n",
    "\n",
    "from util.gymnastics import gym_simulation, init_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNS_DIR = \"runs\"\n",
    "CHECKPOINTS_DIR = \"checkpoints\"\n",
    "\n",
    "# Set up a directory for our logs and models\n",
    "os.makedirs(RUNS_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n",
    "\n",
    "# Init random seed of environment.\n",
    "init_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note on Environment Choice: CartPole\n",
    "\n",
    "`CartPole` provides a **dense reward** (+1 for every timestep the pole is balanced), which allows\n",
    "our agent to learn quickly. This rapid feedback loop makes it much easier to see the effects of our\n",
    "tooling and techniques without waiting a long time for the agent to solve a complex exploration\n",
    "problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"CartPole-v1\"\n",
    "TEMP_ENV = gym.make(ENV_NAME)\n",
    "STATE_DIM = TEMP_ENV.observation_space.shape[0]\n",
    "ACTION_DIM = TEMP_ENV.action_space.n\n",
    "TEMP_ENV.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_simulation(ENV_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Modular Code Structure - An A2C Trainer\n",
    "\n",
    "A key principle of production-ready code is **modularity**. Instead of rewriting the training loop\n",
    "in every section, we will encapsulate the logic into a reusable `A2C_Trainer` class. This makes the\n",
    "code cleaner, easier to maintain, and less prone to errors.\n",
    "\n",
    "Our trainer will handle:\n",
    "\n",
    "- The Actor-Critic model and optimizer.\n",
    "- A single step of the A2C algorithm.\n",
    "- Logic for running a full episode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"The Actor-Critic neural network model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # TODO: Create a Sequential module for the actor: linear, relu, linear, softmax\n",
    "        self.actor = None\n",
    "        # TODO: Create a Sequential module for the critic: linear, relu, linear\n",
    "        self.critic = None\n",
    "\n",
    "    def forward(self, state):\n",
    "        # TODO: Pass the state through the actor to get the policy\n",
    "        policy = None\n",
    "        # TODO: Pass the state through the critic to get the value\n",
    "        value = None\n",
    "        return policy, value\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, state):\n",
    "        # TODO: Implement the act method for inference\n",
    "        # 1. Convert state to a tensor and add a batch dimension\n",
    "        # 2. Get the policy from the model\n",
    "        # 3. Return the action with the highest probability (use torch.argmax)\n",
    "        return 0  # Placeholder\n",
    "\n",
    "\n",
    "class A2C_Trainer:\n",
    "    \"\"\"A trainer class to encapsulate the A2C training logic.\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99, hidden_dim=128):\n",
    "        self.model = ActorCritic(state_dim, action_dim, hidden_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def train_step(self, log_probs, values, rewards):\n",
    "        \"\"\"Performs a single training update.\"\"\"\n",
    "        # TODO: Calculate discounted returns (R). Hint: Iterate backwards through rewards.\n",
    "        returns = []\n",
    "\n",
    "        # TODO: Convert returns and values to tensors. Squeeze values.\n",
    "        returns = None\n",
    "        values = None\n",
    "\n",
    "        # TODO: Calculate advantages. Detach values to stop gradients.\n",
    "        advantages = None\n",
    "\n",
    "        # TODO: Calculate actor loss.\n",
    "        actor_loss = None\n",
    "\n",
    "        # TODO: Calculate critic loss using Mean Squared Error.\n",
    "        critic_loss = None\n",
    "\n",
    "        # TODO: Calculate the total loss.\n",
    "        loss = None\n",
    "\n",
    "        # TODO: Perform backpropagation.\n",
    "        # 1. Zero gradients\n",
    "        # 2. Backward pass\n",
    "        # 3. Clip gradients\n",
    "        # 4. Optimizer step\n",
    "\n",
    "        return 0.0  # Placeholder for loss.item()\n",
    "\n",
    "    def run_episode(self, env, seed: int = None):\n",
    "        \"\"\"Runs a single episode and collects trajectories.\"\"\"\n",
    "        if seed is None:\n",
    "            seed = random.randrange(100_000)\n",
    "\n",
    "        state, _ = env.reset(seed=seed)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        log_probs, values, rewards = [], [], []\n",
    "\n",
    "        while not done:\n",
    "            # TODO: Get policy and value from the model.\n",
    "            policy, value = None, None\n",
    "\n",
    "            # TODO: Create a Categorical distribution and sample an action.\n",
    "            dist = None\n",
    "            action = None\n",
    "\n",
    "            # TODO: Step the environment with the sampled action.\n",
    "            next_state, reward, done, _, _ = env.step(0)  # Placeholder\n",
    "\n",
    "            # TODO: Store the log probability of the action, the value, and the reward.\n",
    "            # ...\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        return log_probs, values, rewards, total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Metrics and Monitoring with Tensorboard\n",
    "\n",
    "**Tensorboard** is a powerful visualization toolkit for understanding, debugging, and optimizing\n",
    "machine learning experiments. It allows us to log and visualize key metrics in real-time.\n",
    "\n",
    "We will track:\n",
    "\n",
    "- **Rewards**: The most important metric. Is our agent learning?\n",
    "- **Losses**: How well our model is learning to predict values and update its policy.\n",
    "- **Gradients**: The magnitude of our gradients. This helps diagnose issues like _vanishing_ or\n",
    "  _exploding_ gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_monitoring(seed, n_episodes=1000, lr=0.001, gamma=0.99):\n",
    "    env = gym.make(ENV_NAME)\n",
    "    # TODO: Create the trainer.\n",
    "    trainer = None\n",
    "\n",
    "    # Setup directories and Tensorboard writer\n",
    "    run_name = f\"a2c_seed_{seed}\"\n",
    "    os.makedirs(os.path.join(CHECKPOINTS_DIR, run_name), exist_ok=True)\n",
    "    # TODO: Create the SummaryWriter in RUNS_DIR/run_name.\n",
    "    writer = None\n",
    "\n",
    "    episode_rewards = []\n",
    "    best_avg_reward = -np.inf\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        # TODO: Run an episode with the trainer.\n",
    "        log_probs, values, rewards, total_reward = None\n",
    "        # TODO: Perform one training step.\n",
    "        loss = None\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "        # TODO: Log the total loss and episode reward to TensorBoard.\n",
    "        # Hint: use writer.add_scalar(\"Tag/Name\", value, global_step)\n",
    "\n",
    "        for name, param in trainer.model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                # TODO: Log the gradients for each parameter.\n",
    "                # Hint: use writer.add_histogram(f\"Gradients/{name}\", param.grad, episode)\n",
    "                pass\n",
    "\n",
    "        # Save the best model based on a moving average of rewards\n",
    "        if len(episode_rewards) > 100:\n",
    "            # TODO: Compute the average reward as the mean of the last 100 episodes.\n",
    "            avg_reward = None\n",
    "            # TODO: Log the moving_avg_reward.\n",
    "            # ..\n",
    "            if avg_reward > best_avg_reward:\n",
    "                best_avg_reward = avg_reward\n",
    "                best_ckpt_path = os.path.join(CHECKPOINTS_DIR, run_name, \"model_best.pth\")\n",
    "                # TODO: Save the model's state dictionary.\n",
    "                # Hint: use torch.save(trainer.model.state_dict(), path)\n",
    "\n",
    "    print(f\"Finished training for seed {seed}.\")\n",
    "    writer.close()\n",
    "    env.close()\n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "# Run the training for one seed\n",
    "rewards_seed_1 = train_with_monitoring(seed=42)\n",
    "\n",
    "# To view the logs, run this command in your terminal:\n",
    "# tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TensorBoard](assets/12_PROD_tensorboard.png) <br><small>TensorBoard UI. This is what you should\n",
    "see when multiple seeds are logged.</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Checkpointing (Saving & Loading Models)\n",
    "\n",
    "**Checkpointing** is the process of saving the state of your model during training. This is crucial\n",
    "for several reasons:\n",
    "\n",
    "- **Resuming Training**: If your training process is interrupted, you can resume from the last saved\n",
    "  checkpoint instead of starting over.\n",
    "- **Inference**: Once you have a trained model, you need to save it to use it later for making\n",
    "  predictions.\n",
    "- **Best Model**: You can save the model that achieved the best performance, which might not be the\n",
    "  one from the very last epoch.\n",
    "\n",
    "Our `train_with_monitoring` function already saves the best-performing model. Now, let's see how to\n",
    "load it and watch our trained agent perform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the best model saved from our previous run\n",
    "best_model_path = f\"{CHECKPOINTS_DIR}/a2c_seed_42/model_best.pth\"\n",
    "# Instantiate the model directly\n",
    "agent_model = ActorCritic(STATE_DIM, ACTION_DIM)\n",
    "# TODO: Load the saved weights from best_model_path.\n",
    "# Hint: use agent_model.load_state_dict(torch.load(path))\n",
    "\n",
    "# TODO: Set the model to evaluation mode. This is CRITICAL for inference.\n",
    "# Hint: use agent_model.eval()\n",
    "\n",
    "# Pass the model (which now has an .act method) to your simulation\n",
    "gym_simulation(ENV_NAME, agent_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Debugging and Troubleshooting in RL\n",
    "\n",
    "Debugging RL algorithms is notoriously difficult because the agent's behavior, the environment, and\n",
    "the learning algorithm are all tightly coupled. A bug might not cause a crash; instead, it might\n",
    "just lead to the agent not learning. Here are some common problems and techniques to debug them.\n",
    "\n",
    "#### Problem 1: The agent is not learning (flat reward curve).\n",
    "\n",
    "- **Check Your Environment**: Is the observation space correct? Is the `done` signal being triggered\n",
    "  appropriately?\n",
    "- **Learning Rate is Too High/Low**: A very high learning rate can cause the policy to become\n",
    "  chaotic, while a very low one can lead to painfully slow learning. This is where hyperparameter\n",
    "  tuning (Part 7) becomes essential.\n",
    "- **Bug in Reward Calculation**: Double-check your reward logic. In our A2C implementation, this is\n",
    "  the calculation of `returns` and `advantages`. An off-by-one error or a mistake in the `gamma`\n",
    "  application can kill learning.\n",
    "- **Check Action Distribution**: Your policy should not collapse to a single action too quickly. Log\n",
    "  the entropy of your policy distribution. If entropy drops to zero, the agent has stopped exploring\n",
    "  and may be stuck in a suboptimal policy.\n",
    "\n",
    "```python\n",
    "  # Inside the training loop, after creating the distribution:\n",
    "  entropy = dist.entropy().item()\n",
    "  writer.add_scalar('Policy/entropy', entropy, episode)\n",
    "```\n",
    "\n",
    "#### Problem 2: Training is very unstable (reward goes up and down wildly).\n",
    "\n",
    "- **High Learning Rate**: This is a classic symptom. The policy updates are too large, causing the\n",
    "  agent to overshoot good policies.\n",
    "- **Small Batch Size / High Variance**: In policy gradient methods, updates can have high variance.\n",
    "  In our simple A2C, each episode is one \"batch\". If episodes are very short, the gradient estimates\n",
    "  can be noisy. You can mitigate this by accumulating gradients over several episodes before\n",
    "  performing an optimizer step.\n",
    "- **Gradient Clipping**: Unstable training can lead to exploding gradients. We already added\n",
    "  `torch.nn.utils.clip_grad_norm_`, which is a standard technique to prevent this by capping the\n",
    "  magnitude of the gradients.\n",
    "- **Value Function is not learning**: If the critic (`value` network) provides poor estimates of the\n",
    "  state value, the `advantages` will be noisy, leading to unstable policy updates. Check the\n",
    "  `critic_loss` in Tensorboard. If it's not decreasing, there might be an issue with your critic's\n",
    "  architecture or learning rate.\n",
    "\n",
    "#### General Debugging Tips:\n",
    "\n",
    "- **Start Simple**: Always start with the simplest possible environment (like `CartPole`) and a\n",
    "  known, stable algorithm before moving to more complex problems.\n",
    "- **Sanity Check Model Outputs**: Before training, pass a dummy state through your model and check\n",
    "  the shapes and value ranges of the output policy and value. The policy should be a valid\n",
    "  probability distribution.\n",
    "- **Read the Paper**: If you are implementing an algorithm from a paper, read it carefully. Small\n",
    "  implementation details can make a huge difference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: The Importance of Multiple Seeds\n",
    "\n",
    "RL algorithms can be very sensitive to the random seed, which affects weight initialization and\n",
    "environment randomness. A great result on a single seed might just be luck. To get a reliable\n",
    "estimate of an agent's performance, you **must** run experiments with multiple seeds and analyze the\n",
    "aggregated results (mean and standard deviation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(rewards_list):\n",
    "    \"\"\"Plots the mean and standard deviation of rewards from multiple seeds.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Transpose the list of lists to easily calculate stats across seeds\n",
    "    rewards_array = np.array(rewards_list)\n",
    "    mean_rewards = np.mean(rewards_array, axis=0)\n",
    "    std_rewards = np.std(rewards_array, axis=0)\n",
    "\n",
    "    plt.plot(mean_rewards, label=\"Mean Reward\", color=\"blue\")\n",
    "    plt.fill_between(\n",
    "        range(len(mean_rewards)),\n",
    "        mean_rewards - std_rewards,\n",
    "        mean_rewards + std_rewards,\n",
    "        color=\"blue\",\n",
    "        alpha=0.2,\n",
    "        label=\"Standard Deviation\",\n",
    "    )\n",
    "\n",
    "    plt.title(\"A2C Performance on CartPole-v1 (3 Seeds)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run training with multiple seeds\n",
    "seeds = [42, 123, 789]\n",
    "all_rewards = []\n",
    "for seed in seeds:\n",
    "    # Using a shorter run for the multi-seed demonstration\n",
    "    rewards = train_with_monitoring(seed=seed, n_episodes=500)\n",
    "    all_rewards.append(rewards)\n",
    "\n",
    "# Plot the results\n",
    "plot_rewards(all_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: Scaling with Parallelization (Ray)\n",
    "\n",
    "#### How does Ray work?\n",
    "\n",
    "**Ray** is a framework for distributed computing that makes it simple to scale Python applications\n",
    "across multiple cores or machines. Its core philosophy is to turn regular Python functions and\n",
    "classes into distributable, asynchronous tasks.\n",
    "\n",
    "We will use two fundamental primitives from Ray Core:\n",
    "\n",
    "1.  `@ray.remote`: A decorator that turns a Python class or function into a remote object or task\n",
    "    that can be executed on a separate worker process.\n",
    "2.  `ray.put()` and `ray.get()`: These functions are used to efficiently transfer objects (like our\n",
    "    model's weights) to Ray's distributed object store and retrieve results from our remote workers.\n",
    "\n",
    "Our strategy will be to create several remote **`RolloutWorker`** actors. The main training loop\n",
    "will send the latest model weights to these workers, who will then independently collect experience\n",
    "(run episodes) in parallel. The main loop then gathers this experience to perform a single, larger\n",
    "update to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class RolloutWorker:\n",
    "    \"\"\"A remote actor for collecting experience in parallel.\"\"\"\n",
    "\n",
    "    def __init__(self, env_name, seed):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.seed = seed\n",
    "\n",
    "    def run_episode(self, model_weights):\n",
    "        # This worker needs its own model instance\n",
    "        state_dim = self.env.observation_space.shape[0]\n",
    "        action_dim = self.env.action_space.n\n",
    "        model = ActorCritic(state_dim, action_dim)\n",
    "        model.load_state_dict(model_weights)\n",
    "\n",
    "        states, actions, rewards = [], [], []\n",
    "        state, _ = self.env.reset(seed=self.seed)\n",
    "        done = False\n",
    "\n",
    "        # Collect a full trajectory\n",
    "        while not done:\n",
    "            # TODO: Get an action from the model (no gradients needed for workers).\n",
    "            action = None\n",
    "\n",
    "            # TODO: Step the environment with the action.\n",
    "            next_state, reward, done, _, _ = self.env.step(action)\n",
    "\n",
    "            # TODO: Store the state, action, and reward.\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        return states, actions, rewards\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Closes the worker's environment.\"\"\"\n",
    "        # TODO: Close the environment.\n",
    "        pass\n",
    "\n",
    "\n",
    "def train_a2c_with_ray(n_batches=250, n_workers=4, lr=0.001, gamma=0.99):\n",
    "    trainer = A2C_Trainer(STATE_DIM, ACTION_DIM, lr=lr, gamma=gamma)\n",
    "    writer = SummaryWriter(f\"{RUNS_DIR}/a2c_ray\")\n",
    "\n",
    "    # TODO: Create remote workers using the RolloutWorker class.\n",
    "    # Hint: [RolloutWorker.remote(...) for ...]\n",
    "    workers = []\n",
    "\n",
    "    for batch_idx in range(n_batches):\n",
    "        # TODO: Put the model's state_dict into Ray's object store.\n",
    "        # Hint: ray.put(...)\n",
    "        model_weights_id = None\n",
    "\n",
    "        # TODO: Call run_episode remotely on all workers.\n",
    "        # Hint: [w.run_episode.remote(...) for w in workers]\n",
    "        futures = []\n",
    "\n",
    "        # TODO: Get the results from the remote calls.\n",
    "        # Hint: ray.get(...)\n",
    "        results = []\n",
    "\n",
    "        batch_reward = 0\n",
    "        trainer.optimizer.zero_grad()\n",
    "\n",
    "        for states, actions, rewards in results:\n",
    "            batch_reward += sum(rewards)\n",
    "\n",
    "            # TODO: This inner loop calculates the loss for one trajectory\n",
    "            # and accumulates its gradients in the central model.\n",
    "            # The logic is very similar to A2C_Trainer.train_step, but you'll\n",
    "            # need to re-evaluate the policy and values on the central model\n",
    "            # to get tensors with a grad_fn.\n",
    "\n",
    "            # 1. Convert data to tensors\n",
    "            # 2. Re-evaluate policy and values on trainer.model\n",
    "            # 3. Get log_probs\n",
    "            # 4. Calculate returns and advantages\n",
    "            # 5. Calculate actor and critic loss\n",
    "            # 6. Calculate total loss and scale by n_workers\n",
    "            # 7. Call loss.backward() to accumulate gradients\n",
    "            pass\n",
    "\n",
    "        # TODO: After all gradients are accumulated, take one optimizer step.\n",
    "        # ...\n",
    "\n",
    "        avg_reward = batch_reward / n_workers\n",
    "        writer.add_scalar(\"Reward/avg_episode_reward\", avg_reward, batch_idx * n_workers)\n",
    "        if (batch_idx + 1) % 25 == 0:\n",
    "            print(f\"Batch {batch_idx+1}/{n_batches}, Avg Reward: {avg_reward:.2f}\")\n",
    "\n",
    "    writer.close()\n",
    "    print(\"Closing remote worker environments...\")\n",
    "    ray.get([w.close.remote() for w in workers])\n",
    "    print(\"Ray training finished.\")\n",
    "\n",
    "\n",
    "# It's good practice to restart Ray between runs if in an interactive environment\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Run the parallel training\n",
    "train_a2c_with_ray()\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7: Automated Hyperparameter Tuning (Optuna)\n",
    "\n",
    "#### How does Optuna work?\n",
    "\n",
    "**Optuna** is a hyperparameter optimization framework that automates the search for the best model\n",
    "settings. It uses a \"define-by-run\" API that makes it highly flexible and Pythonic.\n",
    "\n",
    "The core concepts we will use are:\n",
    "\n",
    "1.  **Study**: A `study` object manages an entire optimization task. We define the goal (e.g.,\n",
    "    `direction='maximize'`).\n",
    "2.  **Trial**: A `trial` represents a single execution of our training process with a specific set\n",
    "    of hyperparameters. Inside our objective function, we ask the `trial` object to `suggest` values\n",
    "    for each hyperparameter (e.g., `trial.suggest_float('lr', ...)`).\n",
    "3.  **Objective Function**: This is a function that Optuna will call repeatedly. It takes a `trial`\n",
    "    object as input, runs our training, and returns a performance score (e.g., the average reward),\n",
    "    which Optuna then tries to maximize or minimize.\n",
    "\n",
    "Optuna uses intelligent sampling algorithms (like TPE) to choose which hyperparameter combinations\n",
    "to try next, making it much more efficient than a simple grid search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"The objective function for Optuna to optimize.\"\"\"\n",
    "    # TODO: Suggest hyperparameters using the trial object.\n",
    "    # lr: float between 1e-4 and 1e-2 (log scale)\n",
    "    # gamma: float between 0.9 and 0.999 (log scale)\n",
    "    # hidden_dim: categorical choice from [64, 128, 256]\n",
    "    lr = 0.001\n",
    "    gamma = 0.99\n",
    "    hidden_dim = 128\n",
    "\n",
    "    # Set up environment and trainer\n",
    "    env = gym.make(ENV_NAME)\n",
    "    trainer = A2C_Trainer(STATE_DIM, ACTION_DIM, lr, gamma, hidden_dim)\n",
    "\n",
    "    episode_rewards = []\n",
    "    # Use fewer episodes for faster tuning trials\n",
    "    for episode in range(150):\n",
    "        # TODO: Run episode with the trainer\n",
    "        log_probs, values, rewards, total_reward = None\n",
    "        trainer.train_step(log_probs, values, rewards)\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "        # TODO: Report the current performance to the trial for pruning.\n",
    "        # Hint: trial.report(metric, step=episode)\n",
    "\n",
    "        # TODO: Check if the trial should be pruned.\n",
    "        # Hint: if trial.should_prune(): raise optuna.exceptions.TrialPruned()\n",
    "        pass\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # TODO: Return the final performance metric to be optimized.\n",
    "    # Hint: np.mean of the last 50 episode rewards\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# TODO: Create an Optuna study. Set direction to 'maximize' and use a pruner.\n",
    "# Hint: use optuna.create_study(...)\n",
    "study = None\n",
    "\n",
    "# TODO: Start the optimization. Hint: use study.optimize, use n_trials=10 and timeout=120\n",
    "# ...\n",
    "\n",
    "print(f\"Best trial value: {study.best_trial.value}\")\n",
    "print(f\"Best params: {study.best_params}\")\n",
    "\n",
    "if optuna.visualization.is_available():\n",
    "    fig1 = optuna.visualization.plot_optimization_history(study)\n",
    "    fig1.show()\n",
    "    fig2 = optuna.visualization.plot_param_importances(study)\n",
    "    fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 8: Other Production Techniques\n",
    "\n",
    "While we've covered the core hands-on tools, building a true production RL system involves a broader\n",
    "set of MLOps (Machine Learning Operations) practices:\n",
    "\n",
    "- **Configuration Management**: For larger projects, hard-coding hyperparameters is not scalable.\n",
    "  Using configuration files (e.g., YAML or JSON) allows you to manage settings for different\n",
    "  experiments cleanly.\n",
    "\n",
    "- **Model Versioning and Deployment**: In a production environment, you need a system for versioning\n",
    "  your models, tracking their performance, and deploying them to serve actions. Tools like\n",
    "  **MLflow** are excellent for experiment tracking and model management, while **Kubeflow** can\n",
    "  orchestrate entire ML pipelines on Kubernetes.\n",
    "\n",
    "- **Continuous Integration/Continuous Deployment (CI/CD)**: A CI/CD pipeline automates the process\n",
    "  of testing code, training models, and deploying them. For RL, this might mean a pipeline that\n",
    "  automatically runs a suite of tests, triggers a new training run with multiple seeds, evaluates\n",
    "  the resulting model against a baseline, and, if it's better, promotes it to production.\n",
    "\n",
    "- **Safe Exploration**: In real-world applications like robotics or autonomous driving, a wrong\n",
    "  action during exploration can be costly or dangerous. **Safe RL** is an entire subfield dedicated\n",
    "  to this problem. Techniques include using a safety layer that overrides unsafe actions, or\n",
    "  employing constrained optimization to ensure the agent's policy does not violate certain safety\n",
    "  criteria during updates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion & Next Steps\n",
    "\n",
    "In this notebook, we've elevated our RL code from a simple script to a more robust,\n",
    "production-oriented setup. We've seen how to:\n",
    "\n",
    "- **Structure** code for reuse.\n",
    "- **Monitor** training to gain critical insights.\n",
    "- **Checkpoint** models for safety and deployment.\n",
    "- **Debug** common and frustrating RL issues.\n",
    "- **Validate** results by running multiple seeds.\n",
    "- **Scale** data collection with parallelization.\n",
    "- **Automate** the search for optimal hyperparameters.\n",
    "\n",
    "These techniques are the building blocks for applying Reinforcement Learning to solve real-world\n",
    "problems. Happy training! 🚀\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlzh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
