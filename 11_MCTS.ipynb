{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Tree Search\n",
    "\n",
    "[Monte Carlo Tree Search (MCTS)](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search) is a\n",
    "powerful algorithmic framework that has revolutionized artificial intelligence, particularly in the\n",
    "domain of Reinforcement Learning (RL), enabling intelligent decision-making in complex environments.\n",
    "By efficiently balancing exploration and exploitation through simulated play, MCTS allows agents to\n",
    "discover optimal strategies without a pre-built model of the world.\n",
    "\n",
    "This notebook will first introduce the core principles of MCTS, demonstrating its power in playing\n",
    "games like Tic-Tac-Toe and Connect Four. We will then delve into one of MCTS's most exciting\n",
    "advancements: [**AlphaZero**](https://en.wikipedia.org/wiki/AlphaZero). This groundbreaking RL\n",
    "algorithm combines MCTS with deep neural networks, achieving superhuman performance in challenging\n",
    "games solely through self-play.\n",
    "\n",
    "Notably, a precursor to AlphaZero, AlphaGo, famously defeated Go world champion Lee Sedol, a feat\n",
    "previously considered impossible for AI. You will witness firsthand how AlphaZero trains an\n",
    "intelligent agent for these games, highlighting the remarkable capabilities of this combined\n",
    "approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, override\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from util.gymnastics import (\n",
    "    init_random,\n",
    "    check_grid_run,\n",
    "    pettingzoo_tictactoe_play,\n",
    "    pettingzoo_connect4_play,\n",
    "    plot_scores as plot_losses,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"Game\"\n",
    "\n",
    "The impressive successes of Monte Carlo Tree Search (MCTS) and AlphaZero, particularly in\n",
    "competitive board games, stem from their design around a specific class of problems.\n",
    "\n",
    "These algorithms thrive in environments characterized by distinct turns for opposing players,\n",
    "deterministic transitions between states, and quantifiable returns for each action. This framework\n",
    "is ideal for **perfect information** games like chess or Go, where the entire game state is known to\n",
    "all participants.\n",
    "\n",
    "Note that - while incredibly powerful within this domain - these algorithms face significant\n",
    "challenges and require substantial modifications to be applied effectively to games with imperfect\n",
    "information, simultaneous moves, continuous action spaces, or stochastic elements.\n",
    "\n",
    "Let's start by defining some convenient type-aliases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'State' encapsulates the state of a game. For example, in tic-tac-toe it can contain the board\n",
    "# and the player whose turn it is to play. We will see that the 'State' can contain additional\n",
    "# information as well (e.g., action mask, etc.)\n",
    "type State = Any\n",
    "\n",
    "# Represent an action that a player of a game can take in the current 'State'. For example, place\n",
    "# an X in the board of tic-tac-toe. For MCTS / AlphaZero, the action space is discrete. And actions\n",
    "# are for convenience represented by integers.\n",
    "type Action = int\n",
    "\n",
    "# The outcome of the game. That is defined by the game itself. For tic-tac-toe, it will be 1.0 if\n",
    "# the first player wins, -1.0 if the second player wins, 0.0 for draws.\n",
    "type Outcome = float\n",
    "\n",
    "# Given a current state, 'Transitions' is a dictionary that maps all possible actions that can be\n",
    "# taken in that state to their corresponding future state.\n",
    "type Transitions = dict[Action, State]\n",
    "\n",
    "# The player of the game, conveniently represented by an integer.\n",
    "type Player = int\n",
    "\n",
    "# Discrete probability distribution for all the actions that can be taken in a given state.\n",
    "type ActionProbs = list[float]\n",
    "\n",
    "# Given a state, it is possible that there are \"symmetric\" states in the game. E.g., in tic-tac-toe,\n",
    "# you can place the first X in any corner, and the state is effectively equivalent from a game-play\n",
    "# perspective. Symmetries will be used to speed up AlphaZero training.\n",
    "type Symmetries = list[tuple[State, ActionProbs]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a base class to represent a generic \"Game\" that we can use in our MCTS and\n",
    "AlphaZero algorithms.\n",
    "\n",
    "NOTE: This representation and encoding of a \"Game\" is non-standard. In particular, the \"game\" is\n",
    "generally stateful and keeps track of the various moves, which is reflected in the API as well. For\n",
    "convenience in this notebook, our `Game` class and methods are instead \"stateless\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(ABC):\n",
    "    \"\"\"Abstract class representing a game (e.g., tic-tac-toe, chess, Go, etc.)\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def action_size(self) -> int:\n",
    "        \"\"\"The size of the discrete action space.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def start_state(self) -> State:\n",
    "        \"\"\"The initial state when the game starts.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_player(self, state) -> Player:\n",
    "        \"\"\"The player that is supposed to take the next action in the given state.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_transitions(self, state: State) -> tuple[Transitions, Outcome | None]:\n",
    "        \"\"\"\n",
    "        Returns all possible transitions from the given state, as well as the outcome if the\n",
    "        state is terminal (i.e., a player won the game in this state.) When the outcome is not None\n",
    "        (i.e., the game ended), the transitions returned is an empty dictionary.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def run_simulation(self, state: State) -> Outcome:\n",
    "        \"\"\"\n",
    "        Runs a simulation of the game starting from 'state' until the game completes, and returns\n",
    "        the outcome of the game. IMPORTANT: the function must return the outcome from the\n",
    "        perspective of the current player at the node where the rollout starts. This ensures\n",
    "        consistency with the back-propagation process, which alternates values based on the player.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def canonicalize(self, state: State) -> State:\n",
    "        \"\"\"\n",
    "        Transforms the state into a \"canonical\" form. For example, in a two-player game, the state\n",
    "        as if it was from the perspective of player 1. This is important when training the neural\n",
    "        network for AlphaZero, that always plays from the perspective of the same player.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_symmetries(self, state: State, probs: ActionProbs) -> Symmetries:\n",
    "        \"\"\"\n",
    "        Returns all the equivalent / symmetric states, as well as the transformed action probs.\n",
    "        IMPORTANT: Symmetries should always include the original state passed as input.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tic-Tac-Toe!\n",
    "\n",
    "Let's play a game of Tic-Tac-Toe with a random opponent: the opponent should not be really good!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `pettingzoo_tictactoe_play` function is a convenient function to play Tic-Tac-Toe from the\n",
    "# PettingZoo environment. Feel free to look at the implementation :)\n",
    "pettingzoo_tictactoe_play(\"player_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game State\n",
    "\n",
    "Here is the `State` for Tic-Tac-Toe: the `BoardState`. This state is also going to be used by\n",
    "Connect4 later on!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BoardState:\n",
    "    \"\"\"\n",
    "    The Game State used in Tic-Tac-Toe and Connect4 games.\n",
    "    \"\"\"\n",
    "\n",
    "    player: int\n",
    "    \"\"\"The player whose turn is to play. Usually either +1 or -1.\"\"\"\n",
    "\n",
    "    action_mask: list[int]\n",
    "    \"\"\"Each index of the list represents the action, and its content is either 1 or 0 depending on\n",
    "    whether it is possible to take such action in this state or not respectively.\"\"\"\n",
    "\n",
    "    board: np.array\n",
    "    \"\"\"The board. Contains 0 for empty cells, +1 for cells belonging to the first player, and -1 for\n",
    "    cells belonging to the second player\"\"\"\n",
    "\n",
    "    # np.array doesn't support == as an equality operator and - in order to make it dictionary\n",
    "    # friendly - the __eq__ and __hash__ methods have been overridden accordingly.\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, BoardState):\n",
    "            return False\n",
    "        return (\n",
    "            self.player == other.player\n",
    "            and np.array_equal(self.action_mask, other.action_mask)\n",
    "            and np.array_equal(self.board, other.board)\n",
    "        )\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.player, tuple(self.action_mask), self.board.tobytes()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "There is quite some preparatory work before implementing the MCTS and AlphaZero algorithms. In\n",
    "particular, we need to implement the \"game\" itself is such a way that we can run simulations: good\n",
    "warmup before diving into the algorithms themselves!\n",
    "\n",
    "**NOTE:** The implementation of the game is not necessarily part of the _core_ inner workings of the\n",
    "MCTS and AlphaZero algorithms. While it is important to understand the API of the `Game` class, feel\n",
    "free to copy/paste the following `TicTacToe` class implementation from the solution folder if you\n",
    "prefer focusing on the algorithms themselves.\n",
    "\n",
    "Read the [PettingZoo documentation](https://pettingzoo.farama.org/environments/classic/tictactoe/)\n",
    "for implementing the game below!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeGame(Game):\n",
    "    \"\"\"Class implementing the Tic-Tac-Toe game!\"\"\"\n",
    "\n",
    "    @override\n",
    "    @property\n",
    "    def action_size(self) -> int:\n",
    "        # TODO: Return the total number of actions in Tic-Tac-Toe.\n",
    "        pass\n",
    "\n",
    "    @override\n",
    "    def start_state(self) -> BoardState:\n",
    "        # TODO: Return the initial state of the board for Tic-Tac-Toe.\n",
    "        pass\n",
    "\n",
    "    @override\n",
    "    def get_player(self, state: BoardState) -> Player:\n",
    "        # TODO: Return the current player for this `state`.\n",
    "        pass\n",
    "\n",
    "    @override\n",
    "    def canonicalize(self, state: BoardState) -> BoardState:\n",
    "        # TODO: Return the \"canonical state\". Hint: flip board and player if player is -1. Also,\n",
    "        #       make sure to deepcopy() the original state.\n",
    "        pass\n",
    "\n",
    "    @override\n",
    "    def get_transitions(self, state: BoardState) -> tuple[Transitions, Outcome | None]:\n",
    "        # TODO: Initialize the transitions to an empty dictionary.\n",
    "        transitions = None\n",
    "        # TODO: Get the outcome in case `state` is terminal. Use the _get_outcome(...) function\n",
    "        #       defined below.\n",
    "        outcome = None\n",
    "        # TODO: If an outcome is available, make sure to early return it with empty transitions!\n",
    "        # TODO: Enumerate the possible actions in this state.\n",
    "        possible_actions = None\n",
    "        for action in possible_actions:\n",
    "            # TODO: Get the position and new action_mask value for this (state, action) pair.\n",
    "            #       Use the _get_action_pos_and_mask(...) function defined below.\n",
    "            (x, y), mask = None\n",
    "            # TODO: Create the next_state starting from the current state, and updating its fields\n",
    "            #       accordingly.\n",
    "            next_state = None\n",
    "            # ...\n",
    "            # TODO: Make sure to add the next_state to the transitions!\n",
    "        return transitions, None\n",
    "\n",
    "    @override\n",
    "    def run_simulation(self, state: BoardState) -> Outcome:\n",
    "        # TODO: Using get_transitions(...), run a simulation of the game until reaching a terminal\n",
    "        #       state, and return the outcome. Actions must be taken randomly.\n",
    "        pass\n",
    "\n",
    "    @override\n",
    "    def get_symmetries(self, state: BoardState, probs: ActionProbs) -> Symmetries:\n",
    "        # TODO: Tic-Tac-Toe has 8 symmetries (rotations and flips). Make sure to compute all the\n",
    "        #       symmetries and also add the original state to the set. Also, the ActionProbs vectors\n",
    "        #       must be updated for each simmetry.\n",
    "        #       This method is likely the most complex one in the TicTacToeGame class. Feel free to\n",
    "        #       take a peek at the solution in case you get stuck!\n",
    "        # Hint: flip / rotate probs like the board... then remember to flatten it back!\n",
    "        probs = np.array(probs).reshape(3, 3)\n",
    "        symmetries = []\n",
    "\n",
    "        # TODO: Loop through the board and its horizontally-flipped version\n",
    "        for cur_board, cur_probs in None:\n",
    "            # TODO: Add the 4 rotations for the current board (original or flipped)\n",
    "            for i in range(4):\n",
    "                # TODO: Compute 0, 90, 180, 270 degree rotations.\n",
    "                symmetries.append(None)\n",
    "\n",
    "        return symmetries\n",
    "\n",
    "    def _get_outcome(self, state: BoardState, n=3) -> float | None:\n",
    "        \"\"\"Internal convenient method to return the outcome of a BoardState if it is terminal.\n",
    "        NOTE: Because the state is terminal, the player whose turn it is now must have either lost\n",
    "        or drawn the game. But this method implements a more \"general\" logic.\n",
    "        \"\"\"\n",
    "        board = state.board\n",
    "        if check_grid_run(board, -1, n):\n",
    "            return -1 * state.player\n",
    "        elif check_grid_run(board, 1, n):\n",
    "            return 1 * state.player\n",
    "        elif np.all(np.absolute(board)):\n",
    "            return 0\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _get_action_pos_and_mask(\n",
    "        self, state: BoardState, action: Action\n",
    "    ) -> tuple[tuple[int, int], int]:\n",
    "        \"\"\"Internal convenient method to get the position on the board corresponding to the\n",
    "        `action` passed as argument, as well as the new action-mask value for that action (based\n",
    "        on the current `state`) after the `action` is played. This method (together with\n",
    "        `_get_outcome`) are helpers that simplify both TicTacToe and Connect4 classes.\"\"\"\n",
    "        match action:\n",
    "            case 0:\n",
    "                return (0, 0), 0  # top-left\n",
    "            case 1:\n",
    "                return (0, 1), 0\n",
    "            case 2:\n",
    "                return (0, 2), 0\n",
    "            case 3:\n",
    "                return (1, 0), 0\n",
    "            case 4:\n",
    "                return (1, 1), 0\n",
    "            case 5:\n",
    "                return (1, 2), 0\n",
    "            case 6:\n",
    "                return (2, 0), 0  # top-right\n",
    "            case 7:\n",
    "                return (2, 1), 0\n",
    "            case 8:\n",
    "                return (2, 2), 0\n",
    "        raise Exception(\"Unmatched action!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a small set of unit tests to validate that the implementation is correct. Feel free to add\n",
    "more tests, the morer the better!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# Basic tests for TicTacToeGame, add more!\n",
    "def run_tictactoegame_tests():\n",
    "    game = TicTacToeGame()\n",
    "\n",
    "    # action_size\n",
    "    assert game.action_size == 9\n",
    "\n",
    "    # start_state.\n",
    "    state = game.start_state()\n",
    "    assert state.player == 1\n",
    "    assert state.action_mask == [1] * 9\n",
    "    assert np.all(state.board == 0)\n",
    "\n",
    "    # get_player\n",
    "    state = game.start_state()\n",
    "    assert game.get_player(state) == 1\n",
    "    state.player = -1\n",
    "    assert game.get_player(state) == -1\n",
    "\n",
    "    # canonicalize\n",
    "    state = game.start_state()\n",
    "    state.board[0, 1] = 1\n",
    "    state.board[0, 2] = 1\n",
    "    state.board[1, 1] = -1\n",
    "    state.board[2, 0] = -1\n",
    "\n",
    "    canonical = game.canonicalize(state)\n",
    "    assert np.array_equal(state.board, canonical.board)\n",
    "    assert state == canonical\n",
    "    assert canonical.player == 1\n",
    "\n",
    "    state.player = -1\n",
    "    canonical = game.canonicalize(state)\n",
    "    assert np.array_equal(state.board, -canonical.board)\n",
    "    assert state != canonical\n",
    "    assert canonical.player == 1, \"Player didn't become the canonical one!\"\n",
    "\n",
    "    # get_outcome\n",
    "    state = game.start_state()\n",
    "    assert game._get_outcome(state) is None\n",
    "\n",
    "    state.board[0, 0] = 1\n",
    "    state.board[0, 1] = 1\n",
    "    state.board[0, 2] = 1\n",
    "    state.board[1, 1] = -1\n",
    "    state.board[2, 0] = -1\n",
    "    assert game._get_outcome(state) == 1\n",
    "    state.player = -1\n",
    "    assert game._get_outcome(state) == -1\n",
    "\n",
    "    state = game.start_state()\n",
    "    state.board[0, 0] = -1\n",
    "    state.board[0, 1] = -1\n",
    "    state.board[0, 2] = -1\n",
    "    state.board[1, 0] = 1\n",
    "    state.board[1, 1] = 1\n",
    "    state.board[2, 0] = 1\n",
    "    assert game._get_outcome(state) == -1\n",
    "    state.player = -1\n",
    "    assert game._get_outcome(state) == 1\n",
    "\n",
    "    state = game.start_state()\n",
    "    state.board[0, 0] = 1\n",
    "    state.board[0, 1] = -1\n",
    "    state.board[0, 2] = 1\n",
    "    state.board[1, 0] = 0\n",
    "    state.board[1, 1] = 1\n",
    "    state.board[1, 2] = 1\n",
    "    state.board[2, 0] = -1\n",
    "    state.board[2, 1] = 1\n",
    "    state.board[2, 2] = -1\n",
    "    assert game._get_outcome(state) is None\n",
    "    state.player = -1\n",
    "    assert game._get_outcome(state) is None\n",
    "    state.board[1, 0] = -1\n",
    "    assert game._get_outcome(state) == 0\n",
    "    state.player = 1\n",
    "    assert game._get_outcome(state) == 0\n",
    "\n",
    "    # get_transitions\n",
    "    state = game.start_state()\n",
    "    state.board[0, 0] = 1\n",
    "    state.board[0, 1] = -1\n",
    "    state.board[0, 2] = 1\n",
    "    state.board[1, 0] = 0\n",
    "    state.board[1, 1] = 1\n",
    "    state.board[1, 2] = 1\n",
    "    state.board[2, 0] = -1\n",
    "    state.board[2, 1] = 1\n",
    "    state.board[2, 2] = -1\n",
    "    state.player = -1\n",
    "    state.action_mask = [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "    transitions, outcome = game.get_transitions(state)\n",
    "    assert len(transitions) == 1\n",
    "    assert outcome is None\n",
    "    next_state = transitions[3]\n",
    "    assert next_state is not None\n",
    "    assert next_state.board[1, 0] == -1\n",
    "    assert next_state.player == 1\n",
    "    assert next_state.action_mask == [0] * 9\n",
    "    transitions, outcome = game.get_transitions(next_state)\n",
    "    assert len(transitions) == 0\n",
    "    assert outcome == 0\n",
    "\n",
    "    state = game.start_state()\n",
    "    state.board[0, 0] = 1\n",
    "    state.board[0, 1] = 1\n",
    "    state.board[0, 2] = 0\n",
    "    state.board[1, 0] = 0\n",
    "    state.board[1, 1] = -1\n",
    "    state.board[1, 2] = 0\n",
    "    state.board[2, 0] = 0\n",
    "    state.board[2, 1] = -1\n",
    "    state.board[2, 2] = 0\n",
    "    state.action_mask = [0, 0, 1, 1, 0, 1, 1, 0, 1]\n",
    "    transitions, outcome = game.get_transitions(state)\n",
    "    assert len(transitions) == 5\n",
    "    assert outcome is None\n",
    "    next_state = transitions[2]\n",
    "    assert next_state is not None\n",
    "    assert next_state.board[0, 2] == 1\n",
    "    assert next_state.player == -1\n",
    "    transitions, outcome = game.get_transitions(next_state)\n",
    "    assert len(transitions) == 0\n",
    "    assert outcome == -1  # The outcome is from the perspective of the current player.\n",
    "\n",
    "    # get_symmetries\n",
    "    state = game.start_state()\n",
    "    state.board[0, 0] = 1\n",
    "    state.action_mask[0] = 0\n",
    "    state.player = -1\n",
    "    probs = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.72]\n",
    "    symmetries = game.get_symmetries(state, probs)\n",
    "    assert len(symmetries) == 4\n",
    "    assert symmetries[0][0] == state\n",
    "    assert symmetries[0][1] == probs\n",
    "    sym_state = deepcopy(state)\n",
    "    sym_state.board[0, 0] = 0\n",
    "    sym_state.board[2, 0] = 1\n",
    "    sym_state.action_mask[0] = 1\n",
    "    sym_state.action_mask[6] = 0\n",
    "    assert symmetries[1][0] == sym_state\n",
    "    assert symmetries[1][1] == [0.02, 0.05, 0.72, 0.01, 0.04, 0.07, 0.0, 0.03, 0.06]\n",
    "    sym_state = deepcopy(sym_state)\n",
    "    sym_state.board[2, 0] = 0\n",
    "    sym_state.board[2, 2] = 1\n",
    "    sym_state.action_mask[6] = 1\n",
    "    sym_state.action_mask[8] = 0\n",
    "    assert symmetries[2][0] == sym_state\n",
    "    assert symmetries[2][1] == [0.72, 0.07, 0.06, 0.05, 0.04, 0.03, 0.02, 0.01, 0.0]\n",
    "    sym_state = deepcopy(sym_state)\n",
    "    sym_state.board[2, 2] = 0\n",
    "    sym_state.board[0, 2] = 1\n",
    "    sym_state.action_mask[8] = 1\n",
    "    sym_state.action_mask[2] = 0\n",
    "    assert symmetries[3][0] == sym_state\n",
    "    assert symmetries[3][1] == [0.06, 0.03, 0.0, 0.07, 0.04, 0.01, 0.72, 0.05, 0.02]\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "\n",
    "run_tictactoegame_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCTS\n",
    "\n",
    "Monte Carlo Tree Search is an algorithm that incrementally explores a\n",
    "[tree](<https://en.wikipedia.org/wiki/Tree_(abstract_data_type)>) that can grow extremely large. It\n",
    "does so by setting upfront the number of \"iterations\" that the algorithm will perform, and deciding\n",
    "which branch / node of the tree to incrementally explore maximizing a particular formula called\n",
    "_Upper Confidence Bound 1 applied to trees_ (referred as \"UCB\" in this notebook) that balances\n",
    "exploitation and exploration.\n",
    "\n",
    "In our case, the nodes of the tree represent states of the game. And their children are the future\n",
    "states for each action that can be taken from that particular state. In MCTS, each node has two\n",
    "values associated with it:\n",
    "\n",
    "- **N**: the number of times the node has been visited.\n",
    "- **V**: the \"value\" of the node, which accumulates the approximate outcome of the game when taking\n",
    "  that particular branch of the tree.\n",
    "\n",
    "The UCB formula uses those numbers to select which one is the next child to explore. In particular,\n",
    "for MCTS the UCB formula we use is:\n",
    "\n",
    "$UCB(State_i) = \\bar{V_i} + c\\sqrt{\\frac{lnN}{n_i}}$\n",
    "\n",
    "With $N$ being the total visits of the parent. Basically, the average value of the node plus an\n",
    "exploration contribution (weighted by a constant _c_) that basically depends more or less on how\n",
    "many times the node has been already visited. Here is the `Node` class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Node:\n",
    "    \"\"\"A node in a MCTS tree.\"\"\"\n",
    "\n",
    "    state: State\n",
    "    \"\"\"The state represented by this node.\"\"\"\n",
    "\n",
    "    action: Action | None = None\n",
    "    \"\"\"The action that brought us to this node. If None, this is the root of the tree.\"\"\"\n",
    "\n",
    "    parent: \"Node\" = None\n",
    "    \"\"\"The parent node. If None, this is the root of the tree.\"\"\"\n",
    "\n",
    "    children: dict[Action, \"Node\"] | None = None\n",
    "    \"\"\"The children of this node. Empty dictionary means terminal. None, means unexplored leaf.\"\"\"\n",
    "\n",
    "    N: int = 0\n",
    "    \"\"\"The number of times the node as been visited.\"\"\"\n",
    "\n",
    "    V: float = 0.0\n",
    "    \"\"\"The cumulative value.\"\"\"\n",
    "\n",
    "    prior: list[float] | None = None\n",
    "    \"\"\"Prior probability of the actions to be taken from this node.\"\"\"\n",
    "\n",
    "    def is_leaf(self) -> bool:\n",
    "        \"\"\"A node is a leaf if it is either unexplored or it is a terminal node.\"\"\"\n",
    "        return self.children is None or self.is_terminal()\n",
    "\n",
    "    def is_terminal(self) -> bool:\n",
    "        \"\"\"A terminal node means that it has no children (e.g., game ended with an outcome).\"\"\"\n",
    "        return self.children == {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the exploration happen? Starting from the _root_ node, the process is quite simple:\n",
    "\n",
    "- If the current node is a **terminal node**, we backpropagate its value up the tree.\n",
    "- If the current node is an **unexplored leaf**, we perform a _rollout_ (basically simulate a game\n",
    "  through that branch), determine the value (or outcome), _expand_ the node (i.e., create its\n",
    "  children), and finally we backpropagate its value up the tree.\n",
    "- Otherwise, the **current node has children** and we select one of them using UCB formula.\n",
    "- Repeat for **N iterations**.\n",
    "\n",
    "At the beginning, the _root children_ (e.g., first possible moves of our game) are all unexplored\n",
    "leaves (i.e., their children haven't been expanded yet). Following the steps above for N iterations,\n",
    "the tree will grow larger and simulate outcomes of the game.\n",
    "\n",
    "Hence, the steps of the algorithm are: traversal, expansion, rollout / simulation, backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MCTS diagram](assets/11_MCTS_phases.excalidraw.png) <br><small>The diagram shows the first\n",
    "traversal from the root, picking one of its children (S1), running a simulation, and backpropagating\n",
    "the value up the tree. In the next step, we see that S1 has been expanded and now we know its\n",
    "children; one of them is selected (S3), doing a rollout / simulation, and backrpropagating the value\n",
    "again. The third (final in this example) step shows that S3 has been \"expanded\" but it was terminal,\n",
    "and then S2 is selected, and so on...</small>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    \"\"\"Base class implementing Monte Carlo Tree Search.\"\"\"\n",
    "\n",
    "    def __init__(self, game: Game, initial_state: State):\n",
    "        self.game = game\n",
    "        self.root = Node(initial_state)\n",
    "        self.expand(self.root)\n",
    "        self.cpuct = np.sqrt(2.0)\n",
    "\n",
    "    def search(self, n_iterations: int, temp: float = 0.0) -> Action:\n",
    "        # TODO: For n_interations, call the `traverse` method on the root node. Then return the\n",
    "        #       result of the `select_action` method.\n",
    "        pass\n",
    "\n",
    "    def traverse(self, node: Node):\n",
    "        # TODO: While the current node is _NOT_ a leaf, keep traversing selecting the next child.\n",
    "        while not node.is_leaf():\n",
    "            pass\n",
    "        # TODO: If the node is terminal, compute `value` as node.V / node.N.\n",
    "        #       NOTE: This is not really standard in MCTS algorithms, especially w.r.t. terminal\n",
    "        #       nodes. But this version of the algorithm will balance exploration directly via UCB\n",
    "        #       computation for simplicity, hence we keep it this way.\n",
    "        if node.is_terminal():\n",
    "            value = None\n",
    "        else:\n",
    "            # TODO: Compute `value` performing a simulation (hint: the `rollout` method does that!)\n",
    "            #       and make sure to `expand` the node right after.\n",
    "            pass\n",
    "        # TODO: Update the cumulative node.N and node.V\n",
    "        # ...\n",
    "        # TODO: Back-propagate the value up the tree (hint: use the `backprop` method!)\n",
    "\n",
    "    def expand(self, node: Node):\n",
    "        assert node.children is None, \"Node already expanded.\"\n",
    "        # TODO: Initialize the node's children to empty array.\n",
    "        node.children = None\n",
    "        # TODO: Create new child nodes based on the available transitions from this state.\n",
    "\n",
    "    def rollout(self, node: Node) -> float:\n",
    "        # TODO: Use game.run_simulation!\n",
    "        pass\n",
    "\n",
    "    def backprop(self, node: Node, value: float):\n",
    "        while node.parent is not None:\n",
    "            # TODO: Flip value for the parent's perspective. 'value' is from the perspective of the\n",
    "            #       child that was just evaluated. We alternate the sign as we move up the tree.\n",
    "            value = None\n",
    "            # TODO: Update node.N and node.V, and node itself to continue the iteration.\n",
    "\n",
    "    def select_action(self, temp: float = 0.0) -> Action:\n",
    "        # TODO: Find the child with the highest visit count (N) as best action.\n",
    "        return None\n",
    "\n",
    "    def select_child(self, node: Node) -> tuple[Action, Node]:\n",
    "        # TODO: Compute each child UCB as a dict keyed by action (hint: use the `ucb` method below).\n",
    "        children_ucb = None\n",
    "        # TODO: Get the best actions (i.e., the one with the max UCB)\n",
    "        best_actions = None\n",
    "        # TODO: Select one of the best actions randomly.\n",
    "        chosen_action = None\n",
    "        return chosen_action, node.children[chosen_action]\n",
    "\n",
    "    def ucb(self, child: Node):\n",
    "        if child.N == 0:\n",
    "            return float(\"inf\")\n",
    "        # TODO: Compute the UCB. IMPORTANT: child.V must be negated because the parent wants to\n",
    "        #       minimize the child's (opponent's) value!\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# Testing test... :)\n",
    "class TestGame(Game):\n",
    "\n",
    "    def action_size(self):\n",
    "        return 6\n",
    "\n",
    "    def start_state(self):\n",
    "        return \"s0\"\n",
    "\n",
    "    def get_player(self):\n",
    "        raise Exception(\"not implemented\")\n",
    "\n",
    "    def get_transitions(self, state: State) -> tuple[Transitions, Outcome | None]:\n",
    "        match state:\n",
    "            case \"s0\":\n",
    "                return {\"a1\": \"s1\", \"a2\": \"s2\"}, None\n",
    "            case \"s1\":\n",
    "                return {\"a3\": \"s3\", \"a4\": \"s4\"}, None\n",
    "            case \"s2\":\n",
    "                return {\"a5\": \"s5\", \"a6\": \"s6\"}, None\n",
    "            case \"s3\" | \"s4\" | \"s5\" | \"s6\":\n",
    "                return {}, 0.0\n",
    "        raise Exception(f\"Unexpected transition: {state}\")\n",
    "\n",
    "    def run_simulation(self, state: State) -> float:\n",
    "        match state:\n",
    "            case \"s1\":\n",
    "                return 20.0\n",
    "            case \"s2\":\n",
    "                return 10.0\n",
    "            case \"s3\":\n",
    "                return 0.0\n",
    "            case \"s5\":\n",
    "                return -14.0\n",
    "            case \"s6\":\n",
    "                return 0.0\n",
    "        raise Exception(f\"Unexpected rollout: {state}\")\n",
    "\n",
    "    def is_terminal(self, state: State) -> bool:\n",
    "        return state in [\"s3\", \"s4\", \"s5\", \"s6\"]\n",
    "\n",
    "    def canonicalize(self, state, player):\n",
    "        return state\n",
    "\n",
    "    def get_symmetries(self, state: State, probs: ActionProbs) -> Symmetries:\n",
    "        return [(state, probs)]\n",
    "\n",
    "\n",
    "tree = MCTS(TestGame(), \"s0\")\n",
    "action = tree.search(5)\n",
    "\n",
    "assert action == \"a2\"\n",
    "assert tree.root.V == -58.0, f\"{tree.root.V}\"\n",
    "assert tree.root.N == 5\n",
    "assert tree.root.children[\"a1\"].V == 20.0\n",
    "assert tree.root.children[\"a1\"].N == 1\n",
    "assert tree.root.children[\"a2\"].V == 38.0\n",
    "assert tree.root.children[\"a2\"].N == 4\n",
    "s1_node = tree.root.children[\"a1\"]\n",
    "assert s1_node.state == \"s1\"\n",
    "assert s1_node.children[\"a3\"].N == 0\n",
    "assert s1_node.children[\"a4\"].N == 0\n",
    "s2_node = tree.root.children[\"a2\"]\n",
    "assert s2_node.state == \"s2\"\n",
    "assert s2_node.children[\"a5\"].N == 2, \"leaf nodes must update their counts (in this impl).\"\n",
    "assert s2_node.children[\"a6\"].N == 1\n",
    "\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCTS Tic-Tac-Toe\n",
    "\n",
    "Let's play Tic-Tac-Toe with some exploration via MCTS. The player should be playing reasonably well\n",
    "(definitely better than the random one), but still it won't likely have perfect play unless the\n",
    "exploration and number of iterations are sufficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_game_state(pzoo_env_state: dict) -> BoardState:\n",
    "    \"\"\"Utility function to convert the PettingZoo state into a BoardState for playing against\n",
    "    an AI passed as a callback to `pettingzoo_tictactoe_play`.\"\"\"\n",
    "    observation = deepcopy(pzoo_env_state[\"observation\"])\n",
    "    player = 1 if pzoo_env_state[\"player\"] == \"player_1\" else -1\n",
    "    flip_observation = player == -1\n",
    "    return BoardState(\n",
    "        player,\n",
    "        deepcopy(pzoo_env_state[\"action_mask\"]),\n",
    "        board=(\n",
    "            observation[:, :, 1] - observation[:, :, 0]\n",
    "            if flip_observation\n",
    "            else observation[:, :, 0] - observation[:, :, 1]\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = TicTacToeGame()\n",
    "policy_callback = lambda env_state: MCTS(game, to_game_state(env_state)).search(500)\n",
    "pettingzoo_tictactoe_play(\"player_2\", policy_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = TicTacToeGame()\n",
    "policy_callback = lambda env_state: MCTS(game, to_game_state(env_state)).search(1000)\n",
    "pettingzoo_tictactoe_play(\"player_1\", policy_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaZero\n",
    "\n",
    "[AlphaZero](https://arxiv.org/abs/1712.01815) is a reinforcement learning algorithm that combines\n",
    "Monte Carlo Tree Search and self-play of a policy network in such a way that the neural network can\n",
    "be successfully trained via policy iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network\n",
    "\n",
    "The network really depends on the game. The following network trains well for Tic-Tac-Toe and\n",
    "Connect4.\n",
    "\n",
    "The network has a value head, estimating the value of a state; and a policy head, estimating the\n",
    "probability distribution of actions for the state. The network is trained on examples $(s, \\pi, z)$\n",
    "containing: the state $s$, the estimate of the policy $\\pi$, the final outcome of the game $z$.\n",
    "\n",
    "During training, the following loss function is minimized:\n",
    "\n",
    "$L = (z - v_\\theta(s))^2 - \\pi \\cdot log(p_\\theta(s))$\n",
    "\n",
    "The first term is the MSE (mean squared error) between the network estimate of the state and the\n",
    "actual outcome of the game (they should be close to each other); the second term is the\n",
    "[cross-entropy](https://en.wikipedia.org/wiki/Cross-entropy) between the probability distribution\n",
    "predicted by the network and the estimate of the policy in the training example (which is produced\n",
    "by the Monte Carlo tree and should approximate the real distribution). Minimizing cross-entropy\n",
    "makes the predicted policy ($p_\\theta$) as close as possible to the MCTS-derived target policy\n",
    "($\\pi$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, grid_width, grid_height, action_size):\n",
    "        super().__init__()\n",
    "        out_chan = 16\n",
    "        kernel_size = 2\n",
    "        self.action_size = action_size\n",
    "        # TODO: Create a Conv2d layer with one inputs channel, out_chan, kernel_size, and no bias.\n",
    "        self.conv2d = None\n",
    "        # TODO: Compute the size of the flattened convolutional layer using grid_width and\n",
    "        #       grid_height. Hint: brush up PyTorch conv layers docs for this :)\n",
    "        self.conv_size = None\n",
    "        # TODO: Create a Sequential layer with a linear layer of 32 units and ReLU non-linearity.\n",
    "        self.shared_linear = None\n",
    "        # TODO: Create a Sequential layer: linear 32 -> 16, ReLU, linear 16 -> action size\n",
    "        self.policy_head = None\n",
    "        # TODO: Create a Sequential layer: linear 32 -> 8, ReLU, linear 8 -> 1 -> TanH\n",
    "        self.value_head = None\n",
    "\n",
    "    def forward(self, state):\n",
    "        boards = torch.stack([torch.tensor(s.board).unsqueeze(dim=0) for s in state]).float()\n",
    "\n",
    "        # TODO: Apply the convolutional layer.\n",
    "        x = self.conv2d(boards)\n",
    "        # TODO: Use `view` to flatten the layer - nn.Flatten() doesn't respect batch :(\n",
    "        x = None\n",
    "        # TODO: Compute features using the shared layer.\n",
    "        features = None\n",
    "\n",
    "        # TODO: Compute value and logits using value head and policy head respectively.\n",
    "        value = None\n",
    "        logits = None\n",
    "\n",
    "        # TODO: Compute the legal_moves using the state action mask.\n",
    "        legal_moves = None\n",
    "        # TODO: Compute the masked logits. Hint: use the masked_fill function.\n",
    "        masked_logits = None\n",
    "        # TODO: Compute the action probabilities on the masked_logits using F.softmax. Pay attention\n",
    "        #       on the `dim` parameter!\n",
    "        probs = None\n",
    "\n",
    "        return probs, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCTS Adaptation\n",
    "\n",
    "The AlphaZero version of MCTS is very similar to the base MCTS, with the following differences:\n",
    "\n",
    "- The _rollout_ is performed via a forward-pass on the policy network, that should provide the\n",
    "  approximate value of the state. It also updates the node's _prior_ used for expansion. Note that\n",
    "  when reaching a terminal state, we need to return the _actual_ outcome!\n",
    "- The action is selected using the action probability distribution given by the `N` values of the\n",
    "  _root_'s children after N iterations. This is the key to getting better policy approximations!\n",
    "- The UCB formula weighs the exploration bonus by the prior probability of the action. In\n",
    "  particular, the formula used is: <br>\n",
    "  $UCB(s_i) = \\frac{v_i}{n_i} + c \\cdot p(s_i) \\cdot \\frac{\\sqrt{N}}{1 + n_i}$\n",
    "- It has two additional methods: `swap_root` (to reuse a subtree), and `add_dirichlet_noise_to_root`\n",
    "  (to improve exploration).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaMCTS(MCTS):\n",
    "\n",
    "    def __init__(self, game: Game, initial_state: State, policy: Policy):\n",
    "        super().__init__(game, initial_state)\n",
    "        self.policy = policy\n",
    "        # TODO: Do a rollout on the root node (to set the actions prior) and set N=1.\n",
    "        assert len(self.root.children) > 0\n",
    "\n",
    "    @override\n",
    "    @torch.no_grad()\n",
    "    def rollout(self, node):\n",
    "        # TODO: Check if the node is terminal, in which case return the outcome directly.\n",
    "        # ...\n",
    "        # TODO: Compute the canonical state, and forward pass on the policy network.\n",
    "        canonical_state = None\n",
    "        probs, value = None\n",
    "        # TODO: Update the node prior using the new action probabilities from the network.\n",
    "        #       Hint: squeeze it and convert it to numpy()\n",
    "        node.prior = probs.squeeze().cpu().numpy()\n",
    "        # TODO: Return the value. Question: should it be flipped or not? :)\n",
    "        return None\n",
    "\n",
    "    @override\n",
    "    def select_action(self, temp: float = 0.0) -> Action:\n",
    "        # TODO: Get the action probabilities using the `get_action_probs` method.\n",
    "        action_probs = self.get_action_probs(temp)\n",
    "        # TODO: Pick an action using those probabilities. Hint: use np.random.choice.\n",
    "        return None\n",
    "\n",
    "    @override\n",
    "    def ucb(self, child) -> float:\n",
    "        # TODO: Compute the `value` as -child.V / child.N (or zero if N == 0).\n",
    "        #       Notice the flipped / negative sign!\n",
    "        value = None\n",
    "        # TODO: Get this child prior from the parent.prior probs for the corresponding action.\n",
    "        child_prior = None\n",
    "        # TODO: Compute the exploration bonus using the formula above.\n",
    "        exploration_bonus = None\n",
    "        return value + exploration_bonus\n",
    "\n",
    "    def get_action_probs(self, temp: float = 1.0) -> list[float]:\n",
    "        \"\"\"Returns the posterior action probabilities given by the MCTS tree itself.\"\"\"\n",
    "        # TODO: Compute the total counts for each action using the root children values.\n",
    "        counts = None\n",
    "        # ...\n",
    "        if temp == 0:\n",
    "            # TODO: Compute a probability distribution that always selects the action with the\n",
    "            #       highest count. Hint: there is only one value non-zero... :)\n",
    "            probs = None\n",
    "        else:\n",
    "            # TODO: Raise the counts by 1/temp power.\n",
    "            counts = None\n",
    "            # TODO: Compute the probs using counts.\n",
    "            probs = None\n",
    "        return probs\n",
    "\n",
    "    def swap_root(self, action: Action, child_state: State):\n",
    "        \"\"\"Swap the root with one of its children.\"\"\"\n",
    "        if action in self.root.children:\n",
    "            # TODO: Swap the root with the corresponding child, and detach it from the old tree.\n",
    "            pass\n",
    "        else:\n",
    "            # TODO: If the chosen action leads to a state not already in the tree (can happen with\n",
    "            #       temperature-based sampling), create a new node.\n",
    "            pass\n",
    "\n",
    "    def add_dirichlet_noise_to_root(self, alpha=0.3, epsilon=0.25):\n",
    "        \"\"\"\n",
    "        Adds Dirichlet noise to the prior probabilities of the root node's children.\n",
    "        This is to encourage exploration during self-play.\n",
    "        \"\"\"\n",
    "        # TODO: Get the valid actions from the root\n",
    "        valid_actions = None\n",
    "        # TODO: Generate noise, but only for the valid actions. Hint: use np.random.dirichlet.\n",
    "        noise = None\n",
    "        # TODO: Create a mapping from action to noise value\n",
    "        noise_map = None\n",
    "\n",
    "        # TODO: Apply noise to the root's prior probabilities\n",
    "        new_priors = None  # Hint: (1 - epsilon) * current_priors\n",
    "        for action, n in noise_map.items():\n",
    "            new_priors[action] += None  # Hint: epsilon * n\n",
    "\n",
    "        self.root.prior = new_priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Algorithm\n",
    "\n",
    "Training happens via a\n",
    "[policy iteration](https://en.wikipedia.org/wiki/Markov_decision_process#Policy_iteration) algorithm\n",
    "that plays games via the current policy network, and updates it using the statistics computed via\n",
    "the Monte Carlo Tree Search utilized during self-play.\n",
    "\n",
    "At a high-level, the leaning algorithm is the following:\n",
    "\n",
    "<pre style=\"\n",
    "    font-family: 'Times New Roman', 'Georgia', 'Palatino Linotype', 'Book Antiqua', serif; /* Classic serif fonts */\n",
    "    font-size: 1.0em; /* Slightly larger for readability with serif */\n",
    "    line-height: 1.5;\n",
    "    white-space: pre-wrap; /* Crucial for preserving spaces and newlines */\n",
    "    color: black; /* Pure black text */\n",
    "    background-color: white; /* Explicitly white background */\n",
    "    padding: 1em; /* Add some space around the content */\n",
    "    border: 1px solid #eee; /* A very light, subtle border */\n",
    "    border-radius: 4px; /* Slightly rounded corners */\n",
    "\">\n",
    "<strong style=\"color:black;\">alpha_zero_learn</strong>(N, E, ...):\n",
    "  <strong style=\"color:black;\">for</strong> N iterations:\n",
    "    <strong style=\"color:black;\">samples</strong> = []\n",
    "\n",
    "    <strong style=\"color:black;\">for</strong> E episodes:\n",
    "      <strong style=\"color:black;\">mcts_tree</strong> = <strong style=\"color:black;\">AlphaZeroMCTS</strong>()\n",
    "      <strong style=\"color:black;\">samples</strong> += ... <em style=\"color:black;\"># Use mcts_tree to self play and collect the samples.</em>\n",
    "\n",
    "    <strong style=\"color:black;\">train_policy</strong>(<strong style=\"color:black;\">samples</strong>)\n",
    "    <em style=\"color:black;\"># optional: pitting stage, to make sure the new policy is actually better!</em>\n",
    "</pre>\n",
    "\n",
    "The core / brilliant idea is to simulate the game using MCTS, and compute a better approximate\n",
    "action probability distribution via the final root's children `N` values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type-alias for a single training example used to train the policy network.\n",
    "type TrainingExample = tuple[State, ActionProbs, Outcome]\n",
    "\n",
    "\n",
    "class AlphaZero:\n",
    "    \"\"\"Main class implementing the AlphaZero algorithm!\"\"\"\n",
    "    # NOTE: Hyperparameters in this constructor have been tuned for the examples in this notebook.\n",
    "    def __init__(\n",
    "        self,\n",
    "        game: Game,\n",
    "        policy: Policy,\n",
    "        lr=3e-3,\n",
    "        max_train_examples=int(2e5),\n",
    "        train_batch_size=64,\n",
    "        num_iter=150,\n",
    "        num_episodes=25,\n",
    "        num_sims=200,\n",
    "        train_epochs=10,\n",
    "        temp_threshold=5,\n",
    "        n_pitting_games=-1,\n",
    "        pit_update_threshold=0.55,\n",
    "        pit_every=10,\n",
    "    ):\n",
    "        self.game = game\n",
    "        self.policy = policy\n",
    "        self.num_iter = num_iter\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_sims = num_sims\n",
    "        self.train_epochs = train_epochs\n",
    "        self.max_train_examples = max_train_examples\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.temp_threshold = temp_threshold\n",
    "        self.n_pitting_games = n_pitting_games\n",
    "        self.pit_update_threshold = pit_update_threshold\n",
    "        self.pit_every = pit_every\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Main AlphaZero method: performs the training!\"\"\"\n",
    "        # TODO: Create a deque to track the losses.\n",
    "        losses = None\n",
    "        # TODO: Deep copy the current policy (used in optional pitting)\n",
    "        old_policy = None\n",
    "\n",
    "        for i in range(1, self.num_iter + 1):\n",
    "            # TODO: Initialize the training_examples as a deque of max len `max_train_examples`.\n",
    "            training_examples = None\n",
    "\n",
    "            for e in range(1, self.num_episodes + 1):\n",
    "                print(f\"Running iteration {i} episode {e}...\".ljust(100), end=\"\\r\")\n",
    "                # TODO: Run an episode, and track the new training examples. Hint: use the\n",
    "                #       `execute_episode` defined below.\n",
    "                pass\n",
    "\n",
    "            # TODO: Train the policy, get this epoch losses, and update the `losses`.\n",
    "            #       Hint: use `train_policy` method.\n",
    "\n",
    "            if self.n_pitting_games > 0 and i % self.pit_every == 0:\n",
    "                # TODO: Perform pitting. Hint: use the `pit` method\n",
    "                pit_score = None\n",
    "                # TODO: Determine if the new policy has been rejected, and update the `old_policy`\n",
    "                #       (or `policy`) accordingly.\n",
    "                rejected = None\n",
    "                # ...\n",
    "                print(f\"Iteration {i} average loss: {np.mean(losses): .5f}. \", end=\"\")\n",
    "                print(f\"New policy accepted? {not rejected}, pit score: {pit_score:.3f}.\")\n",
    "            else:\n",
    "                print(f\"Iteration {i} average loss: {np.mean(losses): .5f}.\")\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def execute_episode(self) -> list[TrainingExample]:\n",
    "        # TODO: Initialize `step`` to 0, `cur_player`` to 1, `state`` to the start state of the\n",
    "        #       game, the `episode_history` to empty list, and `mct`s to a new AlphaMCTS.\n",
    "        step = None\n",
    "        cur_player = None\n",
    "        state = None\n",
    "        episode_history = None\n",
    "        mcts = None\n",
    "\n",
    "        while True:\n",
    "            step += 1\n",
    "            temp_for_play = int(step < self.temp_threshold)\n",
    "\n",
    "            # TODO: Get transitions and oucome for the current state.\n",
    "            transitions, outcome = None\n",
    "            if outcome is not None:\n",
    "                # NOTE: here on how it is impossible to have an outcome that is 1 given the current\n",
    "                #       configuration of the game, where the current player in a terminal state\n",
    "                #       means that they have lost or drawn the game.\n",
    "                assert outcome != 1\n",
    "                # TODO: Return training examples (state, action probs, outcome) from the\n",
    "                #       `episode_history`. IMPORTANT: make sure to flip the outcome based on the\n",
    "                #       `cur_player`!\n",
    "                return (\n",
    "                    # ...\n",
    "                )\n",
    "\n",
    "            # TODO: Add Dirichlet noise to the root of the `mcts` tree.\n",
    "            # ...\n",
    "            # TODO: Get the next action calling `search` in `mcts`. Hint: use `temp_for_play`.\n",
    "            action = None\n",
    "            # TODO: Get the new action probabilities from the mcts tree!\n",
    "            new_probs = None\n",
    "\n",
    "            # TODO: Get the canonical state and append state and its symmetries to the\n",
    "            #       `episode_history`.\n",
    "            # ...\n",
    "\n",
    "            # TODO: Update the state to the next state based on the selected action. Also, update\n",
    "            #       the current player, and `swap_root` of the mcts tree!\n",
    "            # ...\n",
    "\n",
    "    def train_policy(self, training_examples: list[TrainingExample]) -> list[float]:\n",
    "        n_samples = len(training_examples)\n",
    "        n_batches = int(len(training_examples) / self.train_batch_size)\n",
    "\n",
    "        # TODO: Shuffle the `training_examples`.\n",
    "\n",
    "        losses = []\n",
    "        for epoch in range(0, self.train_epochs):\n",
    "            batch_losses = []\n",
    "            for batch in range(0, n_batches):\n",
    "                # TODO: Compute the start and end indices, used to get the training `mini_batch`.\n",
    "                start = None\n",
    "                end = None\n",
    "                mini_batch = None\n",
    "\n",
    "                # TODO: Extract the states from the `mini_batch` training examples. Keep them as a\n",
    "                #       simple Python list.\n",
    "                states = None\n",
    "                # TODO: Extract the target action probabilities and target values (outcomes) from\n",
    "                #       the `mini_batch` training examples. Make a tensor. Hint: use torch.stack.\n",
    "                target_probs = None\n",
    "                target_values = None\n",
    "\n",
    "                # TODO: Forward pass via the policy network.\n",
    "                probs, values = None\n",
    "                # TODO: # Remove the batch dimension for a tensor of scalars.\n",
    "                values = None\n",
    "\n",
    "                # TODO: Compute the value component of the loss (MSE).\n",
    "                loss_values = None\n",
    "                # TODO: Compute the probs component of the loss (cross-entropy).\n",
    "                loss_probs = None\n",
    "                # TODO: Compute the total loss as their sum.\n",
    "                loss = None\n",
    "\n",
    "                # TODO: Perform the optimizer step.\n",
    "                # ...\n",
    "                batch_losses.append(loss.item())\n",
    "            losses.append(np.mean(batch_losses))\n",
    "        return losses\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def pit(self, new_policy: Policy, old_policy: Policy) -> float:\n",
    "        # TODO: If you are brave, implement the pitting logic :)\n",
    "        #       Note that a simple \"win ratio\" approach might not work even for simple games like\n",
    "        #       Tic-Tac-Toe, and including \"draws\" as positive signal might work better. In real\n",
    "        #       AlphaZero, the pitting stage uses more sophisticated metrics like Elo ratings and\n",
    "        #       larger sampling size.\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Tic-Tac-Toe\n",
    "\n",
    "Let's train Tic-Tac-Toe! It should take a couple of minutes, depending on your hardware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_random()\n",
    "game = TicTacToeGame()\n",
    "policy = Policy(grid_width=3, grid_height=3, action_size=game.action_size)\n",
    "alpha_zero = AlphaZero(game, policy, num_episodes=5)\n",
    "# NOTE: Use the following if you implemented pitting.\n",
    "# alpha_zero = AlphaZero(game, policy, num_episodes=5, n_pitting_games=250)\n",
    "tic_tac_toe_losses = alpha_zero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(tic_tac_toe_losses, rolling_window=100, ylabel=\"Avg. Loss\", xlabel=\"Train epoch #\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlphaZero Tic Tac Toe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = TicTacToeGame()\n",
    "policy_callback = lambda env_state: AlphaMCTS(game, to_game_state(env_state), policy).search(150)\n",
    "pettingzoo_tictactoe_play(\"player_2\", policy_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = TicTacToeGame()\n",
    "policy_callback = lambda env_state: AlphaMCTS(game, to_game_state(env_state), policy).search(150)\n",
    "pettingzoo_tictactoe_play(\"player_1\", policy_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect-4!\n",
    "\n",
    "Let's now train a more complex game: Connect4! As usual, let's start by playing a game against a\n",
    "random opponent - it should be an easy win!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pettingzoo_connect4_play(\"player_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now prepare the `Connect4Game`\n",
    "([documentation](https://pettingzoo.farama.org/environments/classic/connect_four/)). Because of the\n",
    "`Game` and `TicTacToeGame` classes structure, we can inherit from `TicTacToeGame` most methods!\n",
    "(feel free to copy/paste the game from the solution at this point).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connect4Game(TicTacToeGame):\n",
    "\n",
    "    @override\n",
    "    @property\n",
    "    def action_size(self):\n",
    "        # TODO: Return Connect4 action size.\n",
    "        pass\n",
    "\n",
    "    @override\n",
    "    def start_state(self):\n",
    "        # TODO: Return the initial `BoardState` for Connect4.\n",
    "        return None\n",
    "\n",
    "    @override\n",
    "    def get_symmetries(self, state: BoardState, probs: ActionProbs) -> Symmetries:\n",
    "        # TODO: Return Connect4 symmetries. Should be very simple: only horizontal flip!\n",
    "        pass\n",
    "\n",
    "    @override\n",
    "    def _get_outcome(self, state, n=4):\n",
    "        return super()._get_outcome(state, n)\n",
    "\n",
    "    @override\n",
    "    def _get_action_pos_and_mask(\n",
    "        self, state: BoardState, action: Action\n",
    "    ) -> tuple[tuple[int, int], int]:\n",
    "        next_pos = [None] * 7\n",
    "        for index_tuple in np.ndindex(state.board.shape):\n",
    "            row_idx, col_idx = index_tuple\n",
    "            row_idx = state.board.shape[0] - row_idx - 1  # Flip to start from the bottom\n",
    "            value = state.board[row_idx, col_idx]\n",
    "            if value == 0 and next_pos[col_idx] is None:\n",
    "                next_pos[col_idx] = row_idx\n",
    "        position = (next_pos[action], action)\n",
    "        assert position[0] is not None\n",
    "        return position, 0 if position[0] == 0 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the policy! This time it takes longer, possibly $O(hours)$ depending on the\n",
    "hardware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_c4 = Connect4Game()\n",
    "policy_c4 = Policy(grid_width=7, grid_height=6, action_size=game_c4.action_size)\n",
    "alpha_zero_c4 = AlphaZero(\n",
    "    game_c4,\n",
    "    policy_c4,\n",
    "    num_iter=35,\n",
    "    temp_threshold=15,\n",
    ")\n",
    "connect4_losses = alpha_zero_c4.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(connect4_losses, rolling_window=50, ylabel=\"Avg. Loss\", xlabel=\"Train epoch #\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, play with a smarter opponent and see how many games you can win!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_c4 = Connect4Game()\n",
    "policy_fn_c4 = lambda env_state: AlphaMCTS(game_c4, to_game_state(env_state), policy_c4).search(150)\n",
    "pettingzoo_connect4_play(\"player_1\", policy_fn_c4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "There are many resources out there about MCTS and AlphaZero. Amongst them, I recommend reading Surag\n",
    "Nair's amazing [blog post](https://suragnair.github.io/posts/alphazero.html) about AlphaZero, and\n",
    "watching [this YouTube](https://www.youtube.com/watch?v=UXW2yZndl7U) video about Monte Carlo Tree\n",
    "Search by John Levine.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlzh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
