{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Methods\n",
    "\n",
    "Policy _based_ methods learn the optimal policy directly, without necessarily estimating a value\n",
    "function. Policy _gradient_ methods do that performing gradient ascent on the objective function.\n",
    "\n",
    "### Advantages\n",
    "\n",
    " * No need to store action-values.\n",
    " * Ability to learn a stochastic policy direcly.\n",
    " * Hence, no need to manually tune exploitation vs. exploration.\n",
    " * Effective in continuous action spaces (and high-dimensional state spaces).\n",
    " * They generally have good convergence properties.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    " * They might have high-variance.\n",
    " * Might converge to a local maximum.\n",
    " * Slower than other methods, and might take a long time to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from typing import Union\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from util.gymnastics import DEVICE, gym_simulation, init_random, plot_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cart Pole Environment\n",
    "\n",
    "Explore the [Gymnasium Cart Pole environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/),\n",
    "and run the simulation below!\n",
    "\n",
    "**IMPORTANT**: For this notebook, we are going to tweak the reward function of the environment. See\n",
    "the \"Reward Function\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_simulation(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for convenience, we hardcode the state and action sizes of the CartPole environment.\n",
    "STATE_SIZE  = 4\n",
    "ACTION_SIZE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Function\n",
    "\n",
    "The Cart Pole reward function returns `+1` reward at every timestep: the idea is that the longer the\n",
    "pole stays in the upright position the better. Such reward function does not encapsulate whether an\n",
    "action is good or bad, and it does not play well with some concepts we are going to analyze later\n",
    "in this notebook (such as normalization and custom baselines).\n",
    "\n",
    "For this reason, we \"adjust\" the reward taking into account angle and position of the pole as a\n",
    "heuristic to evaluate how good is the next state (to which the agent transitioned given its action)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_reward(next_state, reward) -> Union[np.float64, np.array]:\n",
    "    angle = next_state[2] if len(next_state.shape) == 1 else next_state[:, 2]\n",
    "    position = next_state[0] if len(next_state.shape) == 1 else next_state[:, 0]\n",
    "    return reward - np.abs(angle) / 0.418 - np.abs(position) / 4.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Rule\n",
    "\n",
    "For one trajectory $\\tau$ (or episode), the neural networks weight can be updated according to:\n",
    "\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_k - \\alpha \\sum_{t=0} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) R(\\tau)\n",
    "$$\n",
    "\n",
    "We can interpret this as pushing up probabilities for action / states combinations when the return\n",
    "is high, and the other way around for low returns.\n",
    "\n",
    "This relationship is also interesting because only the policy function needs to be differentiable:\n",
    "the reward function might very well be discontinuous and sparse.\n",
    "\n",
    "For derivation, check the [Hugging Face Deep RL tutorial](https://huggingface.co/learn/deep-rl-course/unit4/pg-theorem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE\n",
    "\n",
    "<div style=\"width: 70%\">\n",
    "  <img src=\"../assets/04_PG_reinforce.png\">\n",
    "  <br>\n",
    "  <small>Sutton & Barto 2022</small>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, hidden_units=16):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(STATE_SIZE, hidden_units)\n",
    "        self.fc2 = nn.Linear(hidden_units, ACTION_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.policy = PolicyNetwork()\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2)\n",
    "\n",
    "    def sample_action(self, state: np.array) -> tuple[np.array, torch.Tensor]:\n",
    "        state = torch.from_numpy(state).float().to(DEVICE)\n",
    "        probs = self.policy.forward(state)\n",
    "        cdist = Categorical(probs)\n",
    "        action = cdist.sample()\n",
    "        logprob = cdist.log_prob(action)\n",
    "        return action.cpu().numpy(), logprob\n",
    "\n",
    "    def learn(self, log_probs: list[torch.Tensor], returns: Union[np.float64, np.array]):\n",
    "        returns = torch.tensor(returns, dtype=torch.float64, device=DEVICE)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        policy_loss = -(log_probs * returns).sum()\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    @torch.no_grad\n",
    "    def act(self, state):\n",
    "        \"\"\"Convenient method for the agent to select an action during simulation.\"\"\"\n",
    "        action, _ = self.sample_action(state[np.newaxis, :])\n",
    "        return action[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def REINFORCE(env, agent, max_episodes=10_000, max_t=1_000, gamma=0.9999):\n",
    "    scores = []\n",
    "    for i_episode in range(1, max_episodes + 1):\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = agent.sample_action(state)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            rewards.append(adjust_reward(state, reward))\n",
    "            log_probs.append(log_prob)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        discounts = np.power(gamma, np.arange(len(rewards)))\n",
    "        R = np.sum(discounts * rewards)\n",
    "\n",
    "        agent.learn(log_probs, R)\n",
    "\n",
    "        # Track scores and print statistics.\n",
    "        scores.append(t)\n",
    "        avg_duration = np.mean(scores[-100:])\n",
    "        if i_episode % 100 == 0:\n",
    "            print(f'Episode {i_episode}\\tAverage duration: {avg_duration:.2f}')\n",
    "        if avg_duration >= 490.0: # Solved\n",
    "            print(f'Environment solved at episode {i_episode}\\Avg. duration: {avg_duration:.2f}')\n",
    "            break\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with init_random(gym.make('CartPole-v1')) as env:\n",
    "    agent = Agent()\n",
    "    scores = REINFORCE(env, agent)\n",
    "plot_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_simulation(\"CartPole-v1\", agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "\n",
    "### Use Future Rewards\n",
    "\n",
    "First thing to notice is that we are using all rewards at every timestep. But really, we should only\n",
    "consider future rewards: i.e., the rewards that are actually the consequences of our actions.\n",
    "\n",
    "$$\n",
    "g = \\sum_t R_t^{future}\\nabla_{\\theta} log\\pi_{\\theta}(a_t | s_t)\n",
    "$$\n",
    "\n",
    "### Collect Multiple Trajectories\n",
    "\n",
    "In every episode, we sample a single trajectory: the gradient update might not contain good data\n",
    "about our policy and the stochastic updates might be very _noisy_. Learning happens because in the\n",
    "long run we hope that all tiny signals accumulate and converge towards the optimal policy.\n",
    "\n",
    "How do we reduce noise? The simplest strategy is to collect more trajectories (with the current\n",
    "policy) at once! And Gymnasium vectorized environment `gym.vector.Env` serves exactly that purpose!\n",
    "\n",
    "#### Normalize Rewards\n",
    "\n",
    "When collecting multiple trajectories, a technique we can use is to normalize the rewards across\n",
    "the various trajectories: which roughly picks half actions to encourage / discourage, and keeps the\n",
    "gradient updates moderate.\n",
    "\n",
    "$$\n",
    "R_k \\leftarrow \\frac{R_k - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "The rewards distribution will also likely change during training, and a reward that might be good at\n",
    "the beginning might actually signal a bad trajectory in late stages of training. Normalization helps\n",
    "with such cases as well.\n",
    "\n",
    "### Baseline Subtraction\n",
    "\n",
    "The idea is to subtract to the reward a _baseline_ $b$, for example the average reward along all\n",
    "trajectories (What if every trajectory has _always_ positive returns?). In this case, things that\n",
    "are above average will push their probabilities to happen up while things below average will be\n",
    "penalized.\n",
    "\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_k - \\alpha \\sum_{t=0} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) [R(\\tau) - b(s_t)]\n",
    "$$\n",
    "\n",
    "We can do this because on expectation this extra subtracted term will have zero effect (as long as\n",
    "it does not depend on the action in _logprob_), but overall we'll get reduced variance (proof left\n",
    "as exercise and/or you can find it in the resources).\n",
    "\n",
    "#### Advantage Function\n",
    "\n",
    "This value that we multiply with the log-probability to \"reinforce\" or \"depress\" the corresponding\n",
    "actions is called the _advantage function_ and plays a critical role in state-of-the-art algorithms:\n",
    "\n",
    "$$\n",
    "A(*) = R(\\tau) - b\n",
    "$$\n",
    "\n",
    "It measures how better the selected action does compared to the _average_ in the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def REINFORCE_v2(env: gym.vector.VectorEnv, agent, max_episodes=10_000, max_t=1_000, gamma=0.9999,\n",
    "                 with_normalization=True, with_baseline=True):\n",
    "    scores = []\n",
    "    for i_episode in range(1, max_episodes + 1):\n",
    "        states, rewards, log_probs = ([], [], [])\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = agent.sample_action(state)\n",
    "            log_probs.append(log_prob)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            rewards.append(adjust_reward(state, reward))\n",
    "            states.append(state)\n",
    "            if terminated.any() or truncated.any():\n",
    "                break\n",
    "\n",
    "        discounts = np.power(gamma, np.arange(len(rewards)))\n",
    "        discounted_rewards = discounts[:, np.newaxis] * rewards\n",
    "        future_returns = discounted_rewards[::-1].cumsum(axis=0)[::-1] # (batch, n_bots)\n",
    "\n",
    "        if with_baseline:\n",
    "            baseline = np.asarray([states[t][:, 2] * states[t][:, 3] for t in range(len(rewards))])\n",
    "            future_returns = future_returns - baseline\n",
    "\n",
    "        if with_normalization:\n",
    "            returns_mean = np.mean(future_returns, axis=1)[:, np.newaxis]\n",
    "            returns_std = np.std(future_returns, axis=1)[:, np.newaxis] + 1.0e-10 # avoid 0 division\n",
    "            future_returns = (future_returns - returns_mean) / returns_std\n",
    "\n",
    "        # copy() for negative strides :(\n",
    "        #   https://discuss.pytorch.org/t/negative-strides-in-tensor-error/134287/2\n",
    "        agent.learn(log_probs, future_returns.copy())\n",
    "\n",
    "        # Track scores and print statistics\n",
    "        scores.append(t)\n",
    "        avg_duration = np.mean(scores[-100:])\n",
    "        if i_episode % 100 == 0:\n",
    "            print(f'Episode {i_episode}\\tAverage duration: {avg_duration:.2f}')\n",
    "        if avg_duration >= 490.0: # Solved\n",
    "            print(f'Environment solved at episode {i_episode}\\tAvg. duration: {avg_duration:.2f}')\n",
    "            break\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with init_random(gym.vector.make('CartPole-v1', num_envs=5)) as env:\n",
    "    agent_v2 = Agent()\n",
    "    scores_v2 = REINFORCE_v2(env, agent_v2)\n",
    "plot_scores(scores_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_simulation(\"CartPole-v1\", agent_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Can We Do Better?\n",
    "\n",
    "There are other improvements that can be applied:\n",
    "\n",
    "1. Actor-critic setup (with value function baseline) and advanced advantage estimation such as _GAE_\n",
    "  will improve learning.\n",
    "2. We are currently _discarding experiences_ after every learning step. That is because the policy\n",
    "  effectively changes. But we'll see that with importance sampling we can iterate on the same data\n",
    "  multiple times and learn in mini-batches!\n",
    "3. Techniques such as \"_trust region_\" and \"_loss clipping_\" will help against degeneration and keep\n",
    "  the policy learning along smooth gradient directions.\n",
    "\n",
    "Once we put all of these in place... we'll have PPO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### Meaning of Loss\n",
    "\n",
    "Note that the loss function used in policy gradient methods doesn't have the same meaning of the\n",
    "typical supervised learning setup. In particular, after that first step of gradient descent, there\n",
    "is no more connection to performance - which is determined by the average return.\n",
    "\n",
    "The loss function is only useful when evaluated at the current parameters to perform one step of\n",
    "gradient ascent. After that it loses its meaning and it's value shouldn't be used as a metric for\n",
    "performance.\n",
    "\n",
    "More details on [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#id14).\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "You can check how REINFORCE with future rewards + normalization + well crafted baseling performs\n",
    "better than the vanilla REINFORCE.\n",
    "\n",
    "Interesting to observe how we needed to tweak the reward function to reach these results: in fact,\n",
    "formulating the reward function properly is one of the major challenges in RL and an area of open\n",
    "research (e.g., curiosity driven agents).\n",
    "\n",
    "As an exercise, try removing the adjustment of rewards in this notebook and just keep the original\n",
    "Cart Pole `+1` per timestep reward. What's the effect on the algorithms' variants? Hint: check how\n",
    "catastrophic learning is with normalization enabled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "init_random()\n",
    "base, norm, all = ([], [], [])\n",
    "random_seeds = np.random.randint(3_141_592, size=10)\n",
    "for seed in random_seeds:\n",
    "    with init_random(gym.vector.make('CartPole-v1', num_envs=5), seed=int(seed)) as env:\n",
    "        print('Future rewards only:')\n",
    "        agent_v3 = Agent()\n",
    "        scores_v3 = REINFORCE_v2(env, agent_v3, with_normalization=False, with_baseline=False)\n",
    "        base.append(len(scores_v3))\n",
    "\n",
    "    with init_random(gym.vector.make('CartPole-v1', num_envs=5), seed=int(seed)) as env:\n",
    "        print('Future rewards + normalization:')\n",
    "        agent_v3_b = Agent()\n",
    "        scores_v3_b = REINFORCE_v2(env, agent_v3_b, with_normalization=True, with_baseline=False)\n",
    "        norm.append(len(scores_v3_b))\n",
    "\n",
    "    with init_random(gym.vector.make('CartPole-v1', num_envs=5), seed=int(seed)) as env:\n",
    "        print('Future rewards + normalization + baseline:')    \n",
    "        agent_v3_bn = Agent()\n",
    "        scores_v3_bn = REINFORCE_v2(env, agent_v3_bn, with_normalization=True, with_baseline=True)\n",
    "        all.append(len(scores_v3_bn))\n",
    "    print()\n",
    "\n",
    "x = np.arange(len(norm))\n",
    "plt.figure('Episode scores')\n",
    "plt.plot(x, base, label='Future rewards')\n",
    "plt.plot(x, norm, 'r', label='Future rewards + normalization')\n",
    "plt.plot(x, all, 'g', label='Future rewards + normalization + baseline')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlzh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
