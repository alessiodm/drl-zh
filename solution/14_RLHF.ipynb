{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with Human Feedback\n",
    "\n",
    "[Large Language Models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) have been at the\n",
    "forefront of the present AI summer, making the generative AI field explode with innovation in recent\n",
    "years.\n",
    "\n",
    "LLMs are trained on vast amount of data in a self-supervised fashion (i.e., next token prediction),\n",
    "but they require lots of fine-tuning for them to be \"user-friendly\" and behave / respond in a way\n",
    "that humans consider appropriate (e.g., do not\n",
    "[hallucinate](<https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)>)).\n",
    "\n",
    "Fine tuning LLMs generally happens in two stages:\n",
    "\n",
    "- **Supervised Fined-Tuning (SFT):** The pre-trained model is fine-tuned on a smaller, high-quality\n",
    "  dataset of labeled prompt-response pairs that demonstrate desired behaviors for specific tasks.\n",
    "  This helps the model become more useful and follow instructions.\n",
    "- **RLHF:** Human evaluators rank or provide preferences for different model outputs. The feedback\n",
    "  is used to train a \"reward model,\" which guides the LLM to generate responses that are highly\n",
    "  preferred by humans (e.g., addressing safety, nuance, creativity, etc.) Here is a\n",
    "  [seminal paper](https://arxiv.org/abs/1909.08593) on the topic.\n",
    "\n",
    "### RLHF Steps\n",
    "\n",
    "The starting point is to collect a \"preference dataset\", where human raters pick one LLM output\n",
    "preferred over another (or multiple ones). The preference dataset has tuples like:\n",
    "\n",
    "```\n",
    "(prompt, output_1, output_2, human_preference)\n",
    "```\n",
    "\n",
    "Then we use the preference dataset to train a _Reward Model_. The reward model is generally another\n",
    "LLM that takes a prompt and a completion, and outputs a _score_. Training the reward model is a hard\n",
    "task in and of itself, and in this notebook we will \"fake\" it using a hand-crafted reward function.\n",
    "\n",
    "At this point, we can introduce the reinforcement learning loop to fine-tune our original LLM! We\n",
    "need a second dataset containing many prompts (no completions!) which are analogous to \"environment\n",
    "steps\". Then, we feed the prompts to the model, which produces a completion, which is then scored\n",
    "via the reward model, and then weights are updated via PPO (or similar RL algorithm).\n",
    "\n",
    "![RLHF diagram](../assets/14_RLHF_loop.excalidraw.png) <br><small>Reinforcement learning loop for\n",
    "LLM fine-tuning.</small>\n",
    "\n",
    "In this notebook, we will fine-tune a small language model to prefer telling stories about the\n",
    "animal kingdom!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary classes from Hugging Face transformers library\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    PreTrainedModel,\n",
    ")\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "\n",
    "# Import necessary classes from PEFT (Parameter-Efficient Fine-Tuning) library for LoRA\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from util.gymnastics import RLHF, DEVICE, init_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a [TinyStories](https://arxiv.org/abs/2305.07759) pretrained model that you can find on\n",
    "[HuggingFace](https://huggingface.co/roneneldan/TinyStories-33M). Moreover, we limit the output of\n",
    "the model to 60 tokens, to keep the generation relatively short and more efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language model used in this notebook.\n",
    "MODEL_NAME = \"roneneldan/TinyStories-33M\"\n",
    "\n",
    "# Output length for training and sampling.\n",
    "OUTPUT_LEN = 60\n",
    "\n",
    "# Sample prompt for tests and consistent for comparison across epochs.\n",
    "SAMPLE_PROMPT = \"Once upon a time\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "In order to be able to fine-tune a language-model, we need to understand the very basics of its\n",
    "architecture. In particular, at the foundation of modern LLMs is the _transformer architecture_,\n",
    "described in the popular paper [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "We will use the [Hugging Face transformers API](https://huggingface.co/docs/transformers/index).\n",
    "Reading the documentation before proceeding with the notebook is highly-recommended, because the\n",
    "notebook assumes familiarity with such APIs and concepts.\n",
    "\n",
    "First thing first, let's write a function to get a tokenizer for our model. A _tokenizer_ transforms\n",
    "text into the corresponding tensor tokens' numerical IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokenizer(model_name=MODEL_NAME) -> PreTrainedTokenizer:\n",
    "    # Load the tokenizer associated with the chosen model. The tokenizer converts text to numerical\n",
    "    # IDs and back.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # Some models don't have a default padding token; set it to the end-of-sequence token in case.\n",
    "    # This is important for batching (even if don't batch in this notebook) and consistent sequence\n",
    "    # handling.\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to retrain the entire model for performance reasons. So we will use the HF\n",
    "[PEFT](https://huggingface.co/docs/transformers/en/peft) (Parameter Efficient Fine Tuning) library\n",
    "to create\n",
    "[LoRA adapters](https://huggingface.co/docs/peft/conceptual_guides/adapter#low-rank-adaptation-lora).\n",
    "\n",
    "Low-Rank Adaptation (LoRA) is one of the most popular PEFT methods, and allows to train a smaller\n",
    "subset of parameters added to the model's fixed parameters to tweak its behavior.\n",
    "\n",
    "Because we will use PPO for training, we will have both an actor and a critic model. Hence, we will\n",
    "create two LoRA configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lora_configs(r=16, alpha=32, dropout=0.05) -> tuple[LoraConfig, LoraConfig]:\n",
    "    \"\"\"LoRA (Low-Rank Adaptation) allows fine-tuning only a small number of parameters, saving\n",
    "    memory and compute.\n",
    "\n",
    "    Args:\n",
    "        r (int): Rank of the LoRA decomposition matrices. Higher rank means more trainable\n",
    "                 parameters (more capacity but slower).\n",
    "        alpha (int): LoRA scaling factor, often set to 2*R. Balances the influence of LoRA\n",
    "                     adapters vs base weights.\n",
    "        dropout (float): Dropout probability within LoRA layers to prevent overfitting.\n",
    "    \"\"\"\n",
    "    # Specify which layers / modules within the base model to apply LoRA adapters to.\n",
    "    # Targeting attention projection layers (query, key, value) is common and effective.\n",
    "    target_layers = [\"q_proj\", \"k_proj\", \"v_proj\"]\n",
    "\n",
    "    # LoRA configuration specifically for the actor model (Causal Language Model task)\n",
    "    actor_lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=r,\n",
    "        lora_alpha=alpha,\n",
    "        lora_dropout=dropout,\n",
    "        target_modules=target_layers,\n",
    "    )\n",
    "\n",
    "    # LoRA configuration specifically for the critic model (Sequence Classification task)\n",
    "    # Note: Use SEQ_CLS task type b/c we load the critic using AutoModelForSequenceClassification.\n",
    "    # Also, we assume the base architecture shares \"targetable\" layer names with the CausalLM.\n",
    "    critic_lora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        r=r,\n",
    "        lora_alpha=alpha,\n",
    "        lora_dropout=dropout,\n",
    "        target_modules=target_layers,\n",
    "    )\n",
    "    return actor_lora_config, critic_lora_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create the models used for training!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_models(\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    model_name=MODEL_NAME,\n",
    ") -> tuple[PreTrainedModel, PreTrainedModel, PreTrainedModel]:\n",
    "    \"\"\"Loads the pre-trained language models to use for RLHF fine-tuning.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - PreTrainedModel: the frozen reference model, used for KL-divergence computation and\n",
    "                               reference during training.\n",
    "            - PreTrainedModel: the \"actor\" model, i.e., the language model that is fine-tuned.\n",
    "            - PreTrainedModel: the \"critic\" model, i.e., the model for classification used to\n",
    "                               determine the \"values\" of the actor outputs.\n",
    "    \"\"\"\n",
    "    # Load the base pre-trained language model for the Actor (policy network).\n",
    "    base_actor_model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n",
    "\n",
    "    # Load the base pre-trained model, but configure it for sequence classification to act as the\n",
    "    # critic (value network). `num_labels=1` makes it output a single continuous value (regression),\n",
    "    # suitable for predicting expected reward (value). This is a simplification; a more standard\n",
    "    # critic might have a custom value head on the CausalLM base.\n",
    "    base_critic_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=1\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Ensure critic model knows the correct padding token ID, important for attention mechanisms.\n",
    "    base_critic_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Create a deep copy of the original *base* actor model as the reference model, before applying\n",
    "    # LoRA. This model serves as a fixed reference point for the KL divergence calculation.\n",
    "    ref_model = copy.deepcopy(base_actor_model).to(DEVICE)\n",
    "    # Set the reference model to evaluation mode (disables dropout, etc.)\n",
    "    ref_model.eval()\n",
    "    # Freeze all parameters of the reference model, so they don't get updated during training.\n",
    "    for param in ref_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Wrap the base actor and critic models with LoRA adapters using the defined configurations.\n",
    "    # `get_peft_model` modifies the model to insert LoRA layers and freezes the original weights.\n",
    "    actor_lora_config, critic_lora_config = make_lora_configs()\n",
    "    actor_model = get_peft_model(base_actor_model, actor_lora_config)\n",
    "    critic_model = get_peft_model(base_critic_model, critic_lora_config)\n",
    "\n",
    "    return ref_model, actor_model, critic_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now put some pieces together and create a function that invokes a model and generates a\n",
    "completion to an input prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    llm_model: PreTrainedModel,\n",
    "    prompt: str,\n",
    "    max_length: int = OUTPUT_LEN,\n",
    ") -> tuple[torch.Tensor, str]:\n",
    "    \"\"\"Generates an output from the LLM model.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - Tensor: the model output token IDs.\n",
    "            - str: The decoded text.\n",
    "    \"\"\"\n",
    "    # Tokenize the current prompt text into numerical IDs for the model, truncating long prompts to\n",
    "    # avoid exceeding our specified max_length.\n",
    "    tokenized_prompt = tokenizer(\n",
    "        prompt, return_tensors=\"pt\", truncation=True, max_length=max_length // 2\n",
    "    ).to(DEVICE)\n",
    "    # Get token IDs and attention mask (which indicates which tokens are real vs padding).\n",
    "    input_ids = tokenized_prompt.input_ids\n",
    "    attention_mask = tokenized_prompt.attention_mask\n",
    "    prompt_len = input_ids.shape[1]  # Store the length of the prompt section\n",
    "\n",
    "    # Generate a response using the actor model (with LoRA). Use torch.no_grad() to avoid any\n",
    "    # gradients calculation on generation (saving memory during inference).\n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,  # Maximum length of prompt + response\n",
    "            do_sample=True,  # Enable sampling (essential for exploration in PPO)\n",
    "            pad_token_id=tokenizer.pad_token_id,  # Specify padding token ID\n",
    "            eos_token_id=tokenizer.eos_token_id,  # Specify end-of-sequence token ID\n",
    "            top_k=50,  # Sampling: consider only top 50 probable tokens\n",
    "            top_p=0.95,  # Sampling: consider tokens summing up to 95% prob (nucleus sampling)\n",
    "            # Sampling temperature: controls randomness (lower=more focused, higher=more random)\n",
    "            temperature=0.7,\n",
    "        )\n",
    "    # Extract only the generated token IDs (excluding the prompt).\n",
    "    response_ids = outputs[0][prompt_len:]\n",
    "    # Decode the response token IDs back into human-readable text.\n",
    "    response_text = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "    # Return response_ids with first being the batch (of size 1 in this simple notebook).\n",
    "    return tokenized_prompt, response_ids.unsqueeze(0), response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the tokenizer and generate an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at roneneldan/TinyStories-33M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = make_tokenizer()\n",
    "ref_model, actor_model, critic_model = make_training_models(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a little girl named Lily. She loved to play with her toys and her friends. One day, Lily's mommy gave her a new toy. It was a big, shiny doll that could talk. Lily was so happy and she hugged her new toy. \n"
     ]
    }
   ],
   "source": [
    "_, _, output_text = generate(tokenizer, ref_model, prompt=SAMPLE_PROMPT)\n",
    "print(f\"{SAMPLE_PROMPT}{output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Model\n",
    "\n",
    "As previously mentioned, we are going to use an hard-coded function as our \"pretend reward model\"\n",
    "for simplicity and to avoid to train a full-fledged reward model. While this works in our limited\n",
    "educational RLHF example, it is certainly not effective for proper fine-tuning and alignment:\n",
    "[reward hacking](https://en.wikipedia.org/wiki/Reward_hacking) being one of the typical reasons.\n",
    "\n",
    "But for now, let's write a reward function that encourages telling stories about animals!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animals_stories_reward(response: str) -> torch.Tensor:\n",
    "    \"\"\"Defines the reward signal used to guide the RL training.\n",
    "\n",
    "    This simple function rewards mentioning animal words and penalizes mentioning human words.\n",
    "    Note: Simple keyword-based rewards are easy for education but prone to \"reward hacking\".\n",
    "    \"\"\"\n",
    "    # Preprocess the generated response.\n",
    "    text_words = response.lower().split()\n",
    "    response_length = len(text_words)\n",
    "\n",
    "    # Count occurrences of animal and human words.\n",
    "    animal_mentions = [word for word in text_words if word in RLHF.ANIMAL_WORDS]\n",
    "    animal_score = len(animal_mentions)\n",
    "    human_score = sum(1 for word in text_words if word in RLHF.HUMAN_WORDS)\n",
    "\n",
    "    # Penalize very short responses.\n",
    "    length_penalty = 0 if response_length > 4 else -1.0\n",
    "\n",
    "    # Penalize excessive repetition of animal words.\n",
    "    animal_threshold = max(5, int(0.15 * response_length))\n",
    "    excessive_animal_penalty = 0\n",
    "    if animal_score > animal_threshold:\n",
    "        excessive_animal_penalty = -0.5 * (animal_score - animal_threshold)\n",
    "\n",
    "    # Encourage a diversity of animal words.\n",
    "    unique_animal_bonus = 0\n",
    "    if animal_score > 0:\n",
    "        unique_animal_bonus = 0.5 * len(set(animal_mentions))\n",
    "\n",
    "    # Calculate final reward.\n",
    "    reward = (\n",
    "        (1.2 * animal_score)\n",
    "        - (0.8 * human_score)\n",
    "        + length_penalty\n",
    "        + excessive_animal_penalty\n",
    "        + unique_animal_bonus\n",
    "    )\n",
    "    return torch.tensor(reward, dtype=torch.float32, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test Case 1 (Standard) Passed!\n",
      "✅ Test Case 2 (Short Response) Passed!\n",
      "✅ Test Case 3 (Excessive Animals) Passed!\n",
      "✅ Test Case 4 (No Keywords) Passed!\n"
     ]
    }
   ],
   "source": [
    "def test_animals_stories_reward():\n",
    "    response1 = \"The quick brown fox and a lion met a boy\"\n",
    "    expected_reward1 = 2.6\n",
    "    actual_reward1 = animals_stories_reward(response1)\n",
    "    assert torch.isclose(\n",
    "        actual_reward1, torch.tensor(expected_reward1)\n",
    "    ), f\"FAILED Case 1: Expected {expected_reward1}, but got {actual_reward1.item()}\"\n",
    "    print(\"✅ Test Case 1 (Standard) Passed!\")\n",
    "\n",
    "    response2 = \"a cat and dog\"\n",
    "    expected_reward2 = 2.4\n",
    "    actual_reward2 = animals_stories_reward(response2)\n",
    "    assert torch.isclose(\n",
    "        actual_reward2, torch.tensor(expected_reward2)\n",
    "    ), f\"FAILED Case 2: Expected {expected_reward2}, but got {actual_reward2.item()}\"\n",
    "    print(\"✅ Test Case 2 (Short Response) Passed!\")\n",
    "\n",
    "    response3 = \"cat dog lion tiger bear fox cat dog lion tiger bear fox\"  # 12 words\n",
    "    expected_reward3 = 13.9\n",
    "    actual_reward3 = animals_stories_reward(response3)\n",
    "    assert torch.isclose(\n",
    "        actual_reward3, torch.tensor(expected_reward3)\n",
    "    ), f\"FAILED Case 3: Expected {expected_reward3}, but got {actual_reward3.item()}\"\n",
    "    print(\"✅ Test Case 3 (Excessive Animals) Passed!\")\n",
    "\n",
    "    response4 = \"This is a simple sentence about nothing special\"\n",
    "    expected_reward4 = 0.0\n",
    "    actual_reward4 = animals_stories_reward(response4)\n",
    "    assert torch.isclose(\n",
    "        actual_reward4, torch.tensor(expected_reward4)\n",
    "    ), f\"FAILED Case 4: Expected {expected_reward4}, but got {actual_reward4.item()}\"\n",
    "    print(\"✅ Test Case 4 (No Keywords) Passed!\")\n",
    "\n",
    "\n",
    "test_animals_stories_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLHF Fine Tuning w/ PPO\n",
    "\n",
    "Let's start by writing a helper function that computes the actual probabilities of the model output,\n",
    "with respect to the actual response. We will use this repeatedly in the training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actual_log_probs(\n",
    "    logits_all: torch.Tensor, response_ids: torch.Tensor, prompt_len: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute the log probabilities of the model output logits w.r.t. the *actual* response.\n",
    "\n",
    "    Args:\n",
    "        logits_all (torch.Tensor): The model output logits of prompt + response. The shape should\n",
    "                                   be [batch, tokens, logits], where batch is always 1 in this\n",
    "                                   notebook, tokens are prompt+response tokens, logits ~50k (i.e.,\n",
    "                                   total number of tokens).\n",
    "        response_ids (torch.Tensor): The actual response token IDs. Shape should be:\n",
    "                                     [batch, response_tokens], in this notebook [1, 100].\n",
    "        prompt_len (int): How long is the prompt.\n",
    "    \"\"\"\n",
    "    log_probs_all = F.log_softmax(logits_all, dim=-1)\n",
    "    # Define the start and end indices corresponding to the *generated response* tokens within the\n",
    "    # full sequence logits. We need logits from position `prompt_len - 1` to predict tokens at\n",
    "    # position `prompt_len`, up to the end.\n",
    "    gen_start = prompt_len - 1\n",
    "    # Index of the second-to-last token (predicting the last token).\n",
    "    gen_end = logits_all.shape[1] - 1\n",
    "    response_log_probs = log_probs_all[:, gen_start:gen_end, :]\n",
    "    # The generated `response_ids` tensor needs correct shape for `gather`.\n",
    "    # Shape: (1, gen_len) -> (1, gen_len, 1)\n",
    "    gather_index = response_ids.unsqueeze(-1)\n",
    "    # Use `gather` to select the log probabilities corresponding to the actual tokens generated by\n",
    "    # the actor (remove last dim).\n",
    "    # (1, 100, 50k) gathered by (1, 100, 1) -> selects (i, j, index[i, j, 1]), i.e., the actual\n",
    "    # token logprob for each token.\n",
    "    actual_log_probs = torch.gather(response_log_probs, 2, gather_index).squeeze(-1)\n",
    "    return actual_log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an AI-generated unit-test :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_actual_log_probs():\n",
    "    batch_size = 1\n",
    "    prompt_len = 3\n",
    "    response_len = 2\n",
    "    total_len = prompt_len + response_len\n",
    "\n",
    "    logits_all = torch.tensor(\n",
    "        [\n",
    "            [  # Batch 0\n",
    "                [0, 0, 0, 0, 0],  # Logits for token 1 (in prompt)\n",
    "                [0, 0, 0, 0, 0],  # Logits for token 2 (in prompt)\n",
    "                [1, 2, 3, 4, 5],  # Logits used to predict the 1st response token\n",
    "                [5, 4, 3, 2, 1],  # Logits used to predict the 2nd response token\n",
    "                [0, 0, 0, 0, 0],  # Logits for token after response (will be ignored)\n",
    "            ]\n",
    "        ],\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    response_ids = torch.tensor([[2, 0]], dtype=torch.int64)\n",
    "    logits_for_response = logits_all[:, prompt_len - 1 : total_len - 1, :]\n",
    "    expected_log_probs_slice = F.log_softmax(logits_for_response, dim=-1)\n",
    "    log_prob_token1 = expected_log_probs_slice[0, 0, 2]\n",
    "    log_prob_token2 = expected_log_probs_slice[0, 1, 0]\n",
    "\n",
    "    expected_output = torch.tensor([[log_prob_token1, log_prob_token2]])\n",
    "\n",
    "    actual_output = actual_log_probs(\n",
    "        logits_all=logits_all,\n",
    "        response_ids=response_ids,\n",
    "        prompt_len=prompt_len,\n",
    "    )\n",
    "\n",
    "    assert actual_output.shape == (\n",
    "        batch_size,\n",
    "        response_len,\n",
    "    ), f\"FAILED: Expected shape {(batch_size, response_len)}, but got {actual_output.shape}\"\n",
    "\n",
    "    assert torch.allclose(\n",
    "        actual_output, expected_output\n",
    "    ), f\"FAILED: Expected values {expected_output}, but got {actual_output}\"\n",
    "\n",
    "    print(\"✅ Test passed!\")\n",
    "\n",
    "\n",
    "test_actual_log_probs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally... let's write our RLHF training loop using PPO! You should already by familiar with\n",
    "PPO's general training structure and loss function - feel free to take a peek at the PPO notebook\n",
    "solution to refresh your memory!\n",
    "\n",
    "One important detail about training is that the reward (that comes from the reward model) is\n",
    "counter-balanced by preventing the new model distribution to steer away too much from the original\n",
    "model distribution. Intuitively, we still want our language model to produce text in a similar way\n",
    "it currently does (and not finding ways to maximize reward that would instead produce unintelligible\n",
    "text).\n",
    "\n",
    "In order to do that, we measure the difference between the \"reference\" model distribution and the\n",
    "model under training via\n",
    "[KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P \\| Q) = \\sum_i P(i) \\log \\frac{P(i)}{Q(i)}\n",
    "$$\n",
    "\n",
    "Which can be [efficiently approximated](http://joschu.net/blog/kl-approx.html) as:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P \\| Q) = E_{X \\sim P}\\left[ \\log P(X) - \\log Q(X) \\right]\n",
    "$$\n",
    "\n",
    "Understanding the mathematics behind these formulas would definitely be beneficial, but it is not\n",
    "necessary to still appreciate the overall RLHF training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rlhf_fine_tuning(\n",
    "    actor_model: PreTrainedModel,\n",
    "    critic_model: PreTrainedModel,\n",
    "    num_epochs=50,\n",
    "    n_epoch_updates=2,\n",
    "    max_length=OUTPUT_LEN,\n",
    "    actor_lr=5e-5,\n",
    "    critic_lr=1e-4,\n",
    "    kl_beta=0.1,\n",
    "    epsilon=0.2,\n",
    "    grad_clip_norm=1.0,\n",
    "    seed=42,\n",
    "):\n",
    "    \"\"\"Fine-tune an LLM using Reinforcement Learning w/ Human Feedback.\n",
    "\n",
    "    Args:\n",
    "        num_epochs (int): Number of times the training loop iterates over all prompts.\n",
    "        max_length (int): Maximum sequence length for generated responses (including prompt).\n",
    "                          Limits computational cost and output length.\n",
    "        actor_lr (float): Learning rate for the actor (policy) model optimizer.\n",
    "        critic_lr (float): Learning rate for the critic (value) model optimizer.\n",
    "        kl_beta (float): Strength of the KL divergence penalty.\n",
    "        epsilon (float): PPO clipping parameter; limits how much the policy changes in one update.\n",
    "        grad_clip_norm (float): Maximum gradients norm to avoid exploding gradients.\n",
    "        seed (int): Seed for deterministic training.\n",
    "    \"\"\"\n",
    "    init_random(seed=seed)\n",
    "\n",
    "    # Optimizers on the trainable parameters, i.e., LoRA weights.\n",
    "    actor_optimizer = optim.Adam(actor_model.parameters(), lr=actor_lr)\n",
    "    critic_optimizer = optim.Adam(critic_model.parameters(), lr=critic_lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Dictionary of metrics to track during training.\n",
    "        metrics = {\n",
    "            \"raw_scores\": [],\n",
    "            \"rewards\": [],\n",
    "            \"kl_divs\": [],\n",
    "            \"ppo_losses\": [],\n",
    "            \"critic_losses\": [],\n",
    "        }\n",
    "\n",
    "        # Set models to training mode (enables dropout, LoRA updates, etc.)\n",
    "        actor_model.train()\n",
    "        critic_model.train()\n",
    "\n",
    "        # Shuffle prompts randomly each epoch for better generalization.\n",
    "        epoch_prompts = RLHF.PROMPTS.copy()\n",
    "        random.shuffle(epoch_prompts)\n",
    "\n",
    "        for prompt in epoch_prompts:\n",
    "            # Generate Response (no grad).\n",
    "            tokenized_prompt, response_ids, response_text = generate(\n",
    "                tokenizer, actor_model, prompt, max_length\n",
    "            )\n",
    "            # Store the length of the response section.\n",
    "            response_len = response_ids.shape[1]\n",
    "            # Store the length of the prompt section.\n",
    "            prompt_len = tokenized_prompt.input_ids.shape[1]\n",
    "\n",
    "            # Concatenate prompt and response IDs to form the full sequence for model input.\n",
    "            full_ids = torch.cat([tokenized_prompt.input_ids, response_ids], dim=1)\n",
    "            # Create the corresponding attention mask for the full sequence (prompt mask + ones for\n",
    "            # response b/c response has no padding).\n",
    "            full_attention_mask = torch.cat(\n",
    "                [\n",
    "                    tokenized_prompt.attention_mask,\n",
    "                    torch.ones(1, response_len, device=DEVICE, dtype=torch.long),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "            # Calculate Metrics (Reward, KL, Value). Perform these calculations without tracking\n",
    "            # gradients as they are inputs to the loss functions.\n",
    "            with torch.no_grad():\n",
    "                # Get logits (raw model outputs before activation) from the actor (PEFT model).\n",
    "                # The actor model output has two fields: `logits` and `past_key_values` (for caching\n",
    "                # attention computations).\n",
    "                actor_logits = actor_model(full_ids, attention_mask=full_attention_mask).logits\n",
    "                # Calculate the log probabilities using the response_ids.\n",
    "                actor_log_probs = actual_log_probs(actor_logits, response_ids, prompt_len)\n",
    "\n",
    "                # Get logits from the frozen reference model (original base model).\n",
    "                ref_logits = ref_model(full_ids, attention_mask=full_attention_mask).logits\n",
    "                # Calculate the log probabilities using the response_ids.\n",
    "                ref_log_probs = actual_log_probs(ref_logits, response_ids, prompt_len)\n",
    "\n",
    "                # Calculate the log probability of the generated sequence under the old policy\n",
    "                # (current actor state). Sum the log probabilities across the sequence dimension\n",
    "                # (dim=1). Squeeze to get a scalar tensor.\n",
    "                log_probs_old = actor_log_probs.sum(dim=1).squeeze()\n",
    "\n",
    "                # Get the value prediction from the critic (PEFT model) using the full sequence.\n",
    "                critic_outputs = critic_model(full_ids, attention_mask=full_attention_mask)\n",
    "                # The critic outputs logits; squeeze to get the scalar value prediction V(s).\n",
    "                critic_value = critic_outputs.logits.squeeze()\n",
    "\n",
    "                # Calculate the KL divergence between the actor and reference model distributions\n",
    "                # for the generated sequence. KL(P || Q) approx E[log P - log Q] (sample)\n",
    "                # Sum difference over sequence, get scalar.\n",
    "                kl_div = (actor_log_probs - ref_log_probs).sum(dim=1).squeeze()\n",
    "\n",
    "                # Get the base reward based on the generated text using the custom reward function.\n",
    "                base_reward = animals_stories_reward(response_text)\n",
    "                # Calculate the final reward used for PPO update: base reward minus the KL penalty.\n",
    "                total_reward = base_reward - kl_beta * kl_div\n",
    "\n",
    "            # Calculate the advantage A(s,a) = R - V(s) using the final reward and the value\n",
    "            # estimate (scalar). Detach it (to be used as a constant target below).\n",
    "            advantage = (total_reward - critic_value).detach()\n",
    "\n",
    "            for _ in range(n_epoch_updates):\n",
    "                # Perform a forward pass through the current actor model again with gradients\n",
    "                # enabled to get the log probabilities of the generated sequence under the\n",
    "                # current policy state.\n",
    "                actor_logits_new = actor_model(full_ids, attention_mask=full_attention_mask).logits\n",
    "                actor_log_probs_new = actual_log_probs(actor_logits_new, response_ids, prompt_len)\n",
    "                log_probs_new = actor_log_probs_new.sum(dim=1).squeeze()\n",
    "\n",
    "                # Calculate the probability ratio: ratio = exp(log_prob_new - log_prob_old).\n",
    "                # Use the detached log_probs_old as it represents the fixed policy for this step.\n",
    "                ratio = torch.exp(log_probs_new - log_probs_old.detach())\n",
    "\n",
    "                # Clip the ratio to the range [1 - epsilon, 1 + epsilon].\n",
    "                clipped_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
    "\n",
    "                # The PPO loss is the negative of the surrogate objective: the minimum between\n",
    "                # ratio * advantage and clipped_ratio * advantage.\n",
    "                ppo_loss = -torch.min(ratio * advantage, clipped_ratio * advantage)\n",
    "\n",
    "                # Update Actor (Policy Network)\n",
    "                actor_optimizer.zero_grad()\n",
    "                ppo_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(actor_model.parameters(), max_norm=grad_clip_norm)\n",
    "                actor_optimizer.step()\n",
    "\n",
    "                # Update Critic (Value Network)\n",
    "                # Calculate the target value for the critic. Return = Advantage + V(s), which should\n",
    "                # approximate the total_reward.\n",
    "                returns = advantage + critic_value.detach()\n",
    "\n",
    "                # Perform a forward pass through the current critic with gradients enabled to get\n",
    "                # value prediction for the current state.\n",
    "                critic_outputs_for_loss = critic_model(full_ids, attention_mask=full_attention_mask)\n",
    "                critic_value_for_loss = critic_outputs_for_loss.logits.squeeze()  # Scalar tensor\n",
    "\n",
    "                # Calculate the critic loss, MSE between predicted value and target return.\n",
    "                critic_loss = F.mse_loss(critic_value_for_loss, returns)\n",
    "\n",
    "                critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(critic_model.parameters(), max_norm=grad_clip_norm)\n",
    "                critic_optimizer.step()\n",
    "\n",
    "            # Store metrics for this step.\n",
    "            metrics[\"raw_scores\"].append(base_reward)\n",
    "            metrics[\"rewards\"].append(total_reward.item())\n",
    "            metrics[\"kl_divs\"].append(kl_div.item())\n",
    "            metrics[\"ppo_losses\"].append(ppo_loss.item())\n",
    "            metrics[\"critic_losses\"].append(critic_loss.item())\n",
    "\n",
    "        # Epoch average metrics.\n",
    "        avg_raw_score = sum(metrics[\"raw_scores\"]) / len(metrics[\"raw_scores\"])\n",
    "        avg_final_reward = sum(metrics[\"rewards\"]) / len(metrics[\"rewards\"])\n",
    "        avg_kl_div = sum(metrics[\"kl_divs\"]) / len(metrics[\"kl_divs\"])\n",
    "        avg_ppo_loss = sum(metrics[\"ppo_losses\"]) / len(metrics[\"ppo_losses\"])\n",
    "        avg_critic_loss = sum(metrics[\"critic_losses\"]) / len(metrics[\"critic_losses\"])\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] \", end=\"\")\n",
    "        print(\n",
    "            f\"Reward: {avg_raw_score:.2f} | Reward (w/ KL): {avg_final_reward:.2f} | \"\n",
    "            + f\"KL Div: {avg_kl_div:.4f} | PPO Loss: {avg_ppo_loss:.4f} | \"\n",
    "            + f\"Critic Loss: {avg_critic_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Sample response for qualitative progress assessment.\n",
    "        actor_model.eval() # eval mode (e.g., disable dropouts)\n",
    "        sample_prompt = SAMPLE_PROMPT\n",
    "        _, _, sample_response = generate(tokenizer, actor_model, sample_prompt, max_length)\n",
    "        print(f\"[SAMPLE] {sample_prompt}{sample_response.replace('\\n', ' ')}\\n\")\n",
    "\n",
    "    print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training!\n",
    "\n",
    "During training we should be monitoring the KL divergence, which should be positive but not too\n",
    "large. Also the losses should trend downwards, but they can be very noisy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/50] Reward: -0.59 | Reward (w/ KL): -0.59 | KL Div: -0.0093 | PPO Loss: 0.2125 | Critic Loss: 10.6719\n",
      "[SAMPLE] Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big, bright rainbow in the sky. It was so pretty!  Lily ran to her friend, a little boy named Max, and said, \"\n",
      "\n",
      "[Epoch 2/50] Reward: -1.06 | Reward (w/ KL): -0.98 | KL Div: -0.7448 | PPO Loss: -0.1894 | Critic Loss: 12.3777\n",
      "[SAMPLE] Once upon a time, there was a little girl named Lily. She loved to play outside in her backyard. One day, she saw a big swing hanging from a tree. She ran over to it and started to swing back and forth. It was so much fun!  Suddenly, her mom\n",
      "\n",
      "[Epoch 3/50] Reward: -0.47 | Reward (w/ KL): -0.35 | KL Div: -1.2162 | PPO Loss: -0.2504 | Critic Loss: 16.1696\n",
      "[SAMPLE] Once upon a time there was a little boy named Jack. He was 3 years old and liked to explore. One day he went on a walk with his mom and dad.  Jack was so excited to explore. He saw a big tree, a pond with a lot of ducks swimming around.\n",
      "\n",
      "[Epoch 4/50] Reward: -0.90 | Reward (w/ KL): -0.81 | KL Div: -0.9318 | PPO Loss: -0.7882 | Critic Loss: 4.4996\n",
      "[SAMPLE] Once upon a time, there was a boy named Jack. He loved to play with his toy gun. One day, he went to the park with his mom. They saw a big tree with lots of leaves. Jack wanted to shoot his toy gun at the leaves.  As Jack shot his\n",
      "\n",
      "[Epoch 5/50] Reward: 1.07 | Reward (w/ KL): 0.88 | KL Div: 1.9318 | PPO Loss: -1.5708 | Critic Loss: 9.0035\n",
      "[SAMPLE] Once upon a time, there was a little girl named Lily. She was very happy and loved to play with her toys. One day, Lily found a new toy in her room. It was a small, round object that could spin.  Lily played with the spinning object all day long\n",
      "\n",
      "[Epoch 6/50] Reward: 0.62 | Reward (w/ KL): 0.30 | KL Div: 3.2379 | PPO Loss: -0.1726 | Critic Loss: 5.3666\n",
      "[SAMPLE] Once upon a time there was a little girl called Sally. She was three years old and she loved to explore. One day she found a little box in the garden. She opened it and found a big, white powder inside. Sally was very excited and she wanted to know what it was. \n",
      "\n",
      "[Epoch 7/50] Reward: 0.75 | Reward (w/ KL): 0.46 | KL Div: 2.8976 | PPO Loss: -1.2324 | Critic Loss: 4.9732\n",
      "[SAMPLE] Once upon a time there was a little girl called Mia. She was only 3 years old and loved to explore the world around her. One day Mia decided to go for a walk and went to the forest. As she walked through the trees, she came across a big tree. She stopped and looked\n",
      "\n",
      "[Epoch 8/50] Reward: 0.27 | Reward (w/ KL): 0.10 | KL Div: 1.6636 | PPO Loss: -0.5359 | Critic Loss: 6.1840\n",
      "[SAMPLE] Once upon a time there were two friends, Sam and Tom. Sam was always very careful and never thought he could do anything right. Tom was a bit jealous of Sam's speed. One day, Sam and Tom decided to have a race to see who was the fastest.  On the day\n",
      "\n",
      "[Epoch 9/50] Reward: 1.28 | Reward (w/ KL): 1.01 | KL Div: 2.6066 | PPO Loss: -1.4086 | Critic Loss: 4.3807\n",
      "[SAMPLE] Once upon a time there was a family of mice who lived in a cozy hole in the ground. Every day they would go out to explore, searching for food and fun.  One day, one of the mice noticed something strange. It was a big, round object with a long neck and\n",
      "\n",
      "[Epoch 10/50] Reward: 2.40 | Reward (w/ KL): 1.90 | KL Div: 4.9802 | PPO Loss: -0.0721 | Critic Loss: 5.3159\n",
      "[SAMPLE] Once upon a time there was a big, strong bear. He was very hungry, so he went looking for food. He saw a big bowl of yummy food, but it was too high for him to reach. So he decided to ask for help.  He saw a little bird nearby\n",
      "\n",
      "[Epoch 11/50] Reward: 2.49 | Reward (w/ KL): 1.99 | KL Div: 4.9684 | PPO Loss: -0.1624 | Critic Loss: 5.5586\n",
      "[SAMPLE] Once upon a time there was a big, bright bird. The bird was very friendly, and it made the bird happy.  The bird asked the bird, \"Would you like to go on an adventure?\" The bird said, \"Yes, please!\"  So the bird and the bird\n",
      "\n",
      "[Epoch 12/50] Reward: 3.20 | Reward (w/ KL): 2.47 | KL Div: 7.3274 | PPO Loss: 0.1052 | Critic Loss: 2.7900\n",
      "[SAMPLE] Once upon a time, there was a little rabbit. The rabbit was very hungry and was looking for some food. The rabbit saw a carrot and wanted to eat it. But the carrot was stuck in the ground. The rabbit tried to pull it out, but it was too hard.   \n",
      "\n",
      "[Epoch 13/50] Reward: 2.94 | Reward (w/ KL): 2.27 | KL Div: 6.6740 | PPO Loss: -0.6778 | Critic Loss: 4.5570\n",
      "[SAMPLE] Once upon a time, there was a little boy who was very happy. He liked to play with his friends and explore the world around him. One day, the little boy's mum said, \"You can go outside and play, but remember to be serious!\"  The little boy did as\n",
      "\n",
      "[Epoch 14/50] Reward: 3.73 | Reward (w/ KL): 3.03 | KL Div: 6.9946 | PPO Loss: -0.7481 | Critic Loss: 3.2014\n",
      "[SAMPLE] Once upon a time there was a girl called Mary. She was very nice and liked to give hugs to her friends. One day, Mary wanted to get a new name for her friend. She thought and thought until she came up with a very nice name.  Mary decided to call the '\n",
      "\n",
      "[Epoch 15/50] Reward: 2.90 | Reward (w/ KL): 2.25 | KL Div: 6.4414 | PPO Loss: -0.4864 | Critic Loss: 3.8491\n",
      "[SAMPLE] Once upon a time there were two friends, Mandy and Nick. They loved to play together, but one day Nick had a different idea.  Mandy asked Nick, \"What if we unite?\"  Nick thought for a moment and then said, \"Let's make a special place\n",
      "\n",
      "[Epoch 16/50] Reward: 4.34 | Reward (w/ KL): 3.50 | KL Div: 8.4002 | PPO Loss: -0.5197 | Critic Loss: 5.9150\n",
      "[SAMPLE] Once upon a time, there was a little girl named Jane. She was very troubled. She had a sore finger and it was hurting her.  One day, Jane went to the doctor. The doctor told her to take out a special kind of medicine. Jane was very scared, but the\n",
      "\n",
      "[Epoch 17/50] Reward: 4.82 | Reward (w/ KL): 3.99 | KL Div: 8.2821 | PPO Loss: 0.3130 | Critic Loss: 4.9445\n",
      "[SAMPLE] Once upon a time there was a bear who lived in a cave. The bear was very bored and wanted to find a friend.   The bear thought to himself, \"How can I make new friends?\" Then the bear had an idea. He found a piece of wood and a rope.\n",
      "\n",
      "[Epoch 18/50] Reward: 4.02 | Reward (w/ KL): 3.33 | KL Div: 6.8757 | PPO Loss: -0.5749 | Critic Loss: 4.1800\n",
      "[SAMPLE] Once upon a time there was a wise old bear. He lived in a big tree in the middle of the forest. Every day the bear would watch the birds in the tree and the squirrels playing in the branches. One day the bear noticed something strange. He saw that one of the birds was\n",
      "\n",
      "[Epoch 19/50] Reward: 4.95 | Reward (w/ KL): 3.91 | KL Div: 10.3638 | PPO Loss: -0.0701 | Critic Loss: 4.8367\n",
      "[SAMPLE] Once upon a time there was a little puppy. The puppy was so small and cute, but it was also very lonely. The puppy wanted a friend to play with.  One day the puppy saw a big bird flying in the sky. The bird was so scared! The puppy wanted to help\n",
      "\n",
      "[Epoch 20/50] Reward: 4.84 | Reward (w/ KL): 3.63 | KL Div: 12.0399 | PPO Loss: -0.3917 | Critic Loss: 4.1761\n",
      "[SAMPLE] Once upon a time, there was a big truck. The truck was very old and the driver needed a new driver. The driver drove the truck to a store, but they didn't have any drivers there. The driver was sad and didn't know what to do.  The driver asked a\n",
      "\n",
      "[Epoch 21/50] Reward: 5.44 | Reward (w/ KL): 4.06 | KL Div: 13.7440 | PPO Loss: 0.3011 | Critic Loss: 8.2454\n",
      "[SAMPLE] Once upon a time, there was a little bird who lived in a big tree. The bird was sad because the other birds in the tree were jealous of the bird. The bird had no one to talk to and no one to play with.  One day, the bird decided to be brave\n",
      "\n",
      "[Epoch 22/50] Reward: 5.20 | Reward (w/ KL): 3.69 | KL Div: 15.1474 | PPO Loss: 0.2474 | Critic Loss: 7.4113\n",
      "[SAMPLE] Once upon a time, there was a wise old rabbit. The rabbit was always looking for food in the grass. One day, the rabbit noticed something strange. There was a trap on the ground. The trap was made of sticks and stones, and the rabbit knew that something bad was about to happen\n",
      "\n",
      "[Epoch 23/50] Reward: 6.60 | Reward (w/ KL): 5.16 | KL Div: 14.4296 | PPO Loss: -0.2696 | Critic Loss: 4.1818\n",
      "[SAMPLE] Once upon a time, there was a small girl called Bella. Bella was very tired, and she wanted to sleep. But then she noticed something strange in the corner of the room. It was a small cord. Bella was curious, so she decided to take a closer look. As Bella came closer\n",
      "\n",
      "[Epoch 24/50] Reward: 6.75 | Reward (w/ KL): 5.17 | KL Div: 15.8434 | PPO Loss: -0.3141 | Critic Loss: 2.8717\n",
      "[SAMPLE] Once upon a time there was a little bird, named Tweetie. Tweetie was very sad because he had lost his mommy. The bird asked the bird, \"Where can I find my mommy?\" The bird replied, \"I'm sorry, Tweetie. I don't know.\" \n",
      "\n",
      "[Epoch 25/50] Reward: 7.53 | Reward (w/ KL): 5.60 | KL Div: 19.2598 | PPO Loss: -0.3029 | Critic Loss: 3.7421\n",
      "[SAMPLE] Once upon a time there was a bear and a rabbit. The bear was mad at the rabbit, so the bear said, \"I disagree! I want to eat the rabbit!\" The rabbit was very scared.  The bear said, \"Why do you disagree? The rabbit is my friend,\n",
      "\n",
      "[Epoch 26/50] Reward: 6.97 | Reward (w/ KL): 4.86 | KL Div: 21.0760 | PPO Loss: 0.0878 | Critic Loss: 4.5239\n",
      "[SAMPLE] Once upon a time there were two friends, a bear and a fox. The fox and the bear were best friends, but the fox was always feeling very sad.  One day, the fox and the bear were walking through the forest when the bear noticed that the fox was frowning. The\n",
      "\n",
      "[Epoch 27/50] Reward: 7.35 | Reward (w/ KL): 5.29 | KL Div: 20.5488 | PPO Loss: -0.3311 | Critic Loss: 3.4196\n",
      "[SAMPLE] Once upon a time there was a little bird. The bird was so small and the bird was so tiny! The bird and the bird were friends. The bird asked the bird to fly to the top of the tree. The bird was scared and flew away.  The bird said, \"Bird\n",
      "\n",
      "[Epoch 28/50] Reward: 6.72 | Reward (w/ KL): 4.58 | KL Div: 21.4539 | PPO Loss: -0.1468 | Critic Loss: 7.7649\n",
      "[SAMPLE] Once upon a time, there was a big tree that had lots of branches. A bird was sitting on one of the branches and the bird was singing a lovely song.  One day, the bird was singing a different song. It was a spicy song! The bird was a bit scared of\n",
      "\n",
      "[Epoch 29/50] Reward: 7.08 | Reward (w/ KL): 5.62 | KL Div: 14.6728 | PPO Loss: -0.2638 | Critic Loss: 3.4265\n",
      "[SAMPLE] Once upon a time, there was a little girl named Mary who was only three years old. Mary was so excited to see a big purple cloud in the sky. The cloud looked so pretty and bright, so Mary decided to go outside and play with the cloud.   Mary was about to\n",
      "\n",
      "[Epoch 30/50] Reward: 6.06 | Reward (w/ KL): 4.77 | KL Div: 12.8386 | PPO Loss: 0.3706 | Critic Loss: 5.7118\n",
      "[SAMPLE] Once upon a time there was a little girl called Lola. Lola was three years old and she was very excited to meet a new friend. Lola saw a big bird outside the window and she called out to it.  The bird was very friendly, and it flew right up to\n",
      "\n",
      "[Epoch 31/50] Reward: 6.16 | Reward (w/ KL): 5.13 | KL Div: 10.3292 | PPO Loss: -0.4390 | Critic Loss: 1.8959\n",
      "[SAMPLE] Once upon a time there was a little girl called Daisy. Daisy was very happy and loved to play with her toys. One day, Daisy was playing with her dolls when her mom came in. Mommy said, \"Daisy, can you send my message to Daddy?\" Daisy was confused and asked\n",
      "\n",
      "[Epoch 32/50] Reward: 6.13 | Reward (w/ KL): 5.02 | KL Div: 11.1115 | PPO Loss: -0.0772 | Critic Loss: 4.3615\n",
      "[SAMPLE] Once upon a time there was a little bunny who loved to hop and bounce around the garden. One day the bunny noticed a big, red tomato in the garden and it was the biggest and juiciest tomato the bunny had ever seen! The bunny wanted to eat the tomato so it hopped over to\n",
      "\n",
      "[Epoch 33/50] Reward: 7.52 | Reward (w/ KL): 6.06 | KL Div: 14.6469 | PPO Loss: -0.0460 | Critic Loss: 2.5130\n",
      "[SAMPLE] Once upon a time there was a cat. The cat was the only cat with a blue nose. The cat felt so sad and lonely. It just wanted a friend!  The cat saw the cat and said, \"Let me be your friend!\" The cat was very happy and the cat was\n",
      "\n",
      "[Epoch 34/50] Reward: 4.96 | Reward (w/ KL): 4.25 | KL Div: 7.1011 | PPO Loss: -0.1010 | Critic Loss: 4.1534\n",
      "[SAMPLE] Once upon a time there was a bird who lived in a tree. The bird was very sad because it could not find its family. The bird looked everywhere but it couldn't find the family. Then it saw a rabbit in the grass. The bird asked the rabbit if it could help. The rabbit\n",
      "\n",
      "[Epoch 35/50] Reward: 6.97 | Reward (w/ KL): 5.83 | KL Div: 11.4208 | PPO Loss: 0.2687 | Critic Loss: 5.3072\n",
      "[SAMPLE] Once upon a time there was a little bird. The bird was looking for a new home. The bird flew around the garden looking for the perfect spot. The bird saw a big tree with a hollow in it and flew inside. The bird was so happy and chirped happily. The bird found\n",
      "\n",
      "[Epoch 36/50] Reward: 7.05 | Reward (w/ KL): 6.04 | KL Div: 10.1414 | PPO Loss: -0.5808 | Critic Loss: 4.4258\n",
      "[SAMPLE] Once upon a time, there was a little girl who loved to play in the mud. She loved to make mud pies, but her mommy said that the mud was too dirty. The little girl was sad and asked her mommy why the mud was so dirty.  Her mommy explained\n",
      "\n",
      "[Epoch 37/50] Reward: 7.20 | Reward (w/ KL): 6.10 | KL Div: 10.9769 | PPO Loss: -0.2712 | Critic Loss: 2.1605\n",
      "[SAMPLE] Once upon a time, there was a little girl named Millie. Millie was playing in the garden with her teddy bear when suddenly, it got stuck in the mud! Millie was so upset, she started to cry.  Her mom came running and said, “Millie\n",
      "\n",
      "[Epoch 38/50] Reward: 5.60 | Reward (w/ KL): 4.62 | KL Div: 9.7271 | PPO Loss: 0.6195 | Critic Loss: 3.9550\n",
      "[SAMPLE] Once upon a time, there was a furry bear called Bear. Bear lived in a big tree near a little house in the forest. Bear liked to play outside, and the two friends were the best of friends.  One day, Bear had an idea. He asked the bird in the tree\n",
      "\n",
      "[Epoch 39/50] Reward: 6.63 | Reward (w/ KL): 5.59 | KL Div: 10.3903 | PPO Loss: -0.5326 | Critic Loss: 3.2616\n",
      "[SAMPLE] Once upon a time, there was a little girl called Sue. Sue was a very thoughtful girl. One day, Sue was playing in the garden when she noticed a big pot sitting on the ground. Sue decided to pick it up and she noticed something inside.  Sue said to the pot\n",
      "\n",
      "[Epoch 40/50] Reward: 7.68 | Reward (w/ KL): 6.54 | KL Div: 11.4362 | PPO Loss: -0.0891 | Critic Loss: 2.7513\n",
      "[SAMPLE] Once upon a time, there was a little girl named Lulu. Lulu was so excited because she was playing with a colorful box of glue! She was looking at the glue and she asked her mom, \"What can I do with the glue?\"   Her mom smiled and said,\n",
      "\n",
      "[Epoch 41/50] Reward: 7.80 | Reward (w/ KL): 6.30 | KL Div: 15.0104 | PPO Loss: 0.0604 | Critic Loss: 6.6227\n",
      "[SAMPLE] Once upon a time, there was a little girl named Mandy. Mandy had a big wish. She wished for a puppy. Mandy asked her mom and dad, “Can I have a puppy?” Her mom and dad said, “We don’t have enough\n",
      "\n",
      "[Epoch 42/50] Reward: 7.96 | Reward (w/ KL): 6.24 | KL Div: 17.2224 | PPO Loss: 0.0248 | Critic Loss: 2.9909\n",
      "[SAMPLE] Once upon a time, there was a bee and a bee. The bee was flying around the bee and the bee was buzzing. The bee was looking for something, but the bee was too busy to notice.  The bee flew over a flower and noticed a bee that was not moving. The\n",
      "\n",
      "[Epoch 43/50] Reward: 7.22 | Reward (w/ KL): 6.01 | KL Div: 12.0197 | PPO Loss: -0.4915 | Critic Loss: 3.5157\n",
      "[SAMPLE] Once upon a time there was a little girl called Abigail. Abigail wanted to take a nap but the sun was already up in the sky. Abigail asked the sun, \"Can I go to sleep now?\"  The sun replied, \"Yes, but first you need\n",
      "\n",
      "[Epoch 44/50] Reward: 6.63 | Reward (w/ KL): 5.72 | KL Div: 9.1661 | PPO Loss: -0.0531 | Critic Loss: 2.5299\n",
      "[SAMPLE] Once upon a time, there was a little girl called Jane. Jane was three years old and she loved to play in the garden. One day, when Jane was playing, she heard a strange sound coming from the garden. It was a little bird! The bird was singing a mild song. \n",
      "\n",
      "[Epoch 45/50] Reward: 6.95 | Reward (w/ KL): 5.90 | KL Div: 10.5347 | PPO Loss: -0.0875 | Critic Loss: 2.0068\n",
      "[SAMPLE] Once upon a time, there was a little bunny. The bunny was hopping around in the grass when suddenly it saw a butterfly. The bunny hopped up to the butterfly and said, \"Hi! What are you doing?\"  The butterfly replied, \"I am looking for a flower. I need\n",
      "\n",
      "[Epoch 46/50] Reward: 7.25 | Reward (w/ KL): 6.38 | KL Div: 8.7325 | PPO Loss: -0.8756 | Critic Loss: 2.7531\n",
      "[SAMPLE] Once upon a time, there was a little bird. The bird was looking for a safe place to rest. The bird saw a big tree and flew to it. The bird was happy to find the tree. The bird felt safe and happy. The bird thanked the tree and the bird flew away.\n",
      "\n",
      "[Epoch 47/50] Reward: 8.05 | Reward (w/ KL): 6.86 | KL Div: 11.9341 | PPO Loss: -0.0627 | Critic Loss: 2.3276\n",
      "[SAMPLE] Once upon a time there was a bird. The bird was very excited because it was flying around in the sky.  The bird saw a rabbit nearby. The rabbit said to the bird, “Do you want to fly together?” The bird was a bit scared but the rabbit encouraged\n",
      "\n",
      "[Epoch 48/50] Reward: 7.32 | Reward (w/ KL): 6.09 | KL Div: 12.2407 | PPO Loss: -0.0314 | Critic Loss: 3.0557\n",
      "[SAMPLE] Once upon a time, there was a little bird who was in need of help. The bird wanted to fly but it was too scared. Suddenly, the bird noticed a piece of cloth that was lying on the ground. The bird thought the cloth could help it fly.  The bird grabbed the\n",
      "\n",
      "[Epoch 49/50] Reward: 8.64 | Reward (w/ KL): 7.33 | KL Div: 13.0784 | PPO Loss: -0.0284 | Critic Loss: 2.3289\n",
      "[SAMPLE] Once upon a time there was a bear who was looking for a snack. The bear was looking for a snack but the rabbit was being very persistent. The bear asked the rabbit why it was being so quiet. The rabbit replied \"I'm looking for a snack to eat\". The bear thought for a\n",
      "\n",
      "[Epoch 50/50] Reward: 7.74 | Reward (w/ KL): 6.58 | KL Div: 11.5998 | PPO Loss: 0.0868 | Critic Loss: 4.7692\n",
      "[SAMPLE] Once upon a time there was a bird. The bird was scared and flew far away from its home. Suddenly, the bird heard a voice coming from a nearby tree. The bird flew closer and saw a little squirrel sitting in the tree.  The bird asked the squirrel why it was scared.\n",
      "\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# Takes ~15 minutes on a modern CPU/GPU (RTX 4090).\n",
    "rlhf_fine_tuning(actor_model, critic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how our model generates stories: hopefully, we will get a strong preference towards\n",
    "animals' stories!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAMPLE] Once upon a time, there was a little bear named Tim. Tim was very hungry. He went to the woods to find some food. Tim saw a rabbit in the woods. The rabbit looked hungry too. Tim said, \"Hi, rabbit! Do you want to share some food with me?\"\n"
     ]
    }
   ],
   "source": [
    "_, _, sample_response = generate(tokenizer, actor_model, SAMPLE_PROMPT)\n",
    "print(f\"[SAMPLE] {SAMPLE_PROMPT}{sample_response.replace('\\n', ' ')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-statistic\n",
    "\n",
    "Finally, let's compute an approximate statistic about how often the fine-tuned model uses words\n",
    "related to animals compared to the reference model. We should see some bias :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fined tuned: 6.5%\n",
      "Reference model: 0.5%\n"
     ]
    }
   ],
   "source": [
    "ref_model_count, ref_model_total, fine_tuned_count, fine_tuned_total = 0, 0, 0, 0\n",
    "\n",
    "for i in range(100):\n",
    "    print(f\"\\rSample: {i} ...\", end=\"\")\n",
    "    _, _, ref_model_text = generate(tokenizer, ref_model, SAMPLE_PROMPT)\n",
    "    _, _, fine_tuned_text = generate(tokenizer, actor_model, SAMPLE_PROMPT)\n",
    "    ref_model_text_words = ref_model_text.split()\n",
    "    fine_tuned_text_words = fine_tuned_text.split()\n",
    "    ref_model_total += len(ref_model_text_words)\n",
    "    fine_tuned_total += len(fine_tuned_text_words)\n",
    "    for word in RLHF.ANIMAL_WORDS:\n",
    "        ref_model_count += ref_model_text_words.count(word)\n",
    "        fine_tuned_count += fine_tuned_text_words.count(word)\n",
    "\n",
    "print(f\"\\rFined tuned: {(fine_tuned_count / fine_tuned_total) * 100:.1f}%\")\n",
    "print(f\"Reference model: {(ref_model_count / ref_model_total) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other / Most Recent Techniques\n",
    "\n",
    "Most recent techniques that represent good alternatives to RLHF are\n",
    "[DPO](https://arxiv.org/abs/2305.18290) and [GRPO](https://arxiv.org/abs/2402.03300). In particular,\n",
    "when you have verifiable tasks (either via script, or via another LLM!), GRPO works great and\n",
    "employs reward functions similar to this notebook's example. Deeplearning.AI has a great\n",
    "[introductory course on GRPO](https://learn.deeplearning.ai/courses/reinforcement-fine-tuning-llms-grpo),\n",
    "totally recommended!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlzh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
