{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Agent Reinforcement Learning\n",
    "\n",
    "While standard (single-agent) RL focuses on a _single_ agent learning to make decisions within an\n",
    "environment maximizing its own cumulative reward, Multi-Agent RL (MARL) involves multiple agents\n",
    "interacting within a shared environment. Each agent learns its policy, potentially influencing and\n",
    "being influenced by others.\n",
    "\n",
    "Agents might have _cooperative_ (shared reward), _competitive_ (conflicting rewards), or _mixed_\n",
    "objectives, adding complexity beyond maximizing a single agent's reward.\n",
    "\n",
    "## Key Challenges\n",
    "\n",
    "MARL presents unique and key challenges:\n",
    "\n",
    "- **Non-Stationarity:** The core challenge. As one agent learns and changes its policy, the\n",
    "  environment effectively changes from the perspective of other agents. The learning process of\n",
    "  other agents makes the environment appear dynamic and unstable to any individual agent.\n",
    "- **Scalability:** The complexity (state and action spaces) can grow exponentially with the number\n",
    "  of agents, making learning computationally expensive or intractable.\n",
    "- **Credit Assignment:** In cooperative settings, it's difficult to determine which agent's specific\n",
    "  actions contributed to the overall team success or failure, making reward distribution\n",
    "  challenging.\n",
    "- **Coordination/Collaboration:** Agents need to learn how to coordinate their actions effectively,\n",
    "  especially with limited or no direct communication.\n",
    "- **Partial Observability:** Agents often have only a limited view of the full environment state and\n",
    "  may not know the internal states or intentions of other agents.\n",
    "\n",
    "## Interesting Aspects\n",
    "\n",
    "MARL is a very interesting setup, and deeply intertwined with **game theory**. Concepts like _Nash\n",
    "Equilibrium_ (where no agent can improve its outcome by unilaterally changing its strategy) are\n",
    "crucial for analyzing the stability and performance of MARL systems, especially in competitive or\n",
    "mixed settings. However, finding or converging to meaningful equilibria can be difficult.\n",
    "\n",
    "MARL systems can exhibit **complex and unexpected collective behaviors** that _emerge_ from the\n",
    "interactions of individually simple agents. Studying these emergent phenomena (like flocking,\n",
    "spontaneous cooperation/competition, or even simulated economic behaviors like bartering) is\n",
    "fascinating and crucial for understanding system dynamics. It can also be a challenge to ensure\n",
    "emergent behavior is desirable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from pettingzoo.utils.env import AgentID, AECEnv\n",
    "from pettingzoo.utils import aec_to_parallel\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "from pettingzoo.mpe import simple_adversary_v3\n",
    "\n",
    "import supersuit as ss\n",
    "\n",
    "from collections import deque\n",
    "from typing import Type\n",
    "\n",
    "from util.gymnastics import DEVICE, pettingzoo_simulation, init_random\n",
    "from util.rl_algos import soft_update_model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPE Simple Adversary Environment\n",
    "\n",
    "Let's use the [Simple Adversary environment](https://mpe2.farama.org/environments/simple_adversary/)\n",
    "to train our MARL agents. This environment is a mixed competitive-cooperative environment, where the\n",
    "two good agents need to deceive the adversary.\n",
    "\n",
    "\"_Good agents are rewarded based on how close the closest one of them is to the target landmark, but\n",
    "negatively rewarded based on how close the adversary is to the target landmark. The adversary is\n",
    "rewarded based on distance to the target, but it doesn’t know which landmark is the target landmark.\n",
    "[...] This means good agents have to learn to ‘split up’ and cover all landmarks to deceive the\n",
    "adversary._\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment configuration tuned for conveniency of this notebook.\n",
    "MPE_SIMPLE_ADVERSARY_KWARGS = {\n",
    "    \"continuous_actions\": True,\n",
    "    \"dynamic_rescaling\": True,\n",
    "}\n",
    "\n",
    "\n",
    "def simple_adversary_v3_wrappers(zoo_env):\n",
    "    \"\"\"Configure wrappers on the simple adversary environment. In particular, make sure the\n",
    "    observations of the good agents and the adversary have the same dimension.\"\"\"\n",
    "    zoo_env = ss.pad_observations_v0(zoo_env)\n",
    "    return zoo_env\n",
    "\n",
    "\n",
    "def create_simple_adversary_env():\n",
    "    \"\"\"Convenient function to setup the simple adversary environment.\"\"\"\n",
    "    env = simple_adversary_v3.env(**MPE_SIMPLE_ADVERSARY_KWARGS)\n",
    "    env = simple_adversary_v3_wrappers(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env: AECEnv = create_simple_adversary_env()\n",
    "test_env.reset(seed=42)\n",
    "agents: list[AgentID] = test_env.agents\n",
    "print(agents)\n",
    "print(test_env.action_space(agents[0]))\n",
    "print(test_env.observation_space(agents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pettingzoo_simulation(\n",
    "    simple_adversary_v3, wrappers=[simple_adversary_v3_wrappers], **MPE_SIMPLE_ADVERSARY_KWARGS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MA-DDPG\n",
    "\n",
    "MA-DDPG is a popular algorithm for multi-agent reinforcement learning, particularly effective in\n",
    "scenarios with continuous action spaces. It extends the single-agent DDPG algorithm using the\n",
    "**Centralized Training with Decentralized Execution (CTDE)** paradigm.\n",
    "\n",
    "<div>\n",
    "  <img src=\"./assets/09_MARL_CTDE.png\">\n",
    "  <br>\n",
    "  <small>From \"Multi-Agent Actor-Critic for Mixe Cooperative-Competitive Environments\"</small>\n",
    "</div>\n",
    "\n",
    "During training, a centralized controller might access all agents' observations, actions, and reward\n",
    "functions to ease learning (addressing non-stationarity and credit assignment). However, during\n",
    "execution, each agent acts based only on its own local observations, making the policies deployable\n",
    "in real-world scenarios where centralization isn't feasible.\n",
    "\n",
    "MA-DDPG trains a centralized critic that considers all agents' observations and actions to guide the\n",
    "learning of individual actors, each of which determines a single agent's policy based only on its\n",
    "local observation.\n",
    "\n",
    "Take a look at the [paper](https://arxiv.org/abs/1706.02275)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer\n",
    "\n",
    "Like DDPG, we use a replay buffer to store experiences. This time configured for multiple agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Replay buffer used in the MA-DDPG implementation.\n",
    "\n",
    "    Stores tuples like: (obs, actions, rewards, next_obs, dones)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size: int, sample_size: int, num_agents: int):\n",
    "        self.size = size\n",
    "        self.deque = deque(maxlen=self.size)\n",
    "        self.sample_size = sample_size\n",
    "        self.num_agents = num_agents\n",
    "\n",
    "    def push(self, transition: tuple[torch.Tensor]):\n",
    "        \"\"\"Pushes into the buffer.\"\"\"\n",
    "        # TODO: Perform some sanity checks on dimensionality. The transition tuple looks like:\n",
    "        #       (obs, actions, rewards, next_obs, dones)\n",
    "        #       The first dimension of each tensor must always represent the agent.\n",
    "        # TODO: Append the transition to the buffer.\n",
    "        pass\n",
    "\n",
    "    def sample(self) -> tuple[torch.Tensor]:\n",
    "        \"\"\"Samples from the buffer. Returns a tuple of tensors for convenience.\n",
    "\n",
    "        The returned tensors shapes are: (num_agents, batch_size, ...)\n",
    "        \"\"\"\n",
    "        # TODO: Sample from the internal `deque` a `sample_size` subset of transitions.\n",
    "        samples = None\n",
    "\n",
    "        # TODO: From the list of samples, extract each component. Use torch.stack.\n",
    "        obs = None\n",
    "        actions = None\n",
    "        rewards = None\n",
    "        next_obs = None\n",
    "        dones = None\n",
    "\n",
    "        # Create a tuple of samples. Note: dimension is (batch_size, num_agents, ...)\n",
    "        t_samples = (obs, actions, rewards, next_obs, dones)\n",
    "        # TODO: Return a tuple of samples of dimension: (num_agents, batch_size, ...)\n",
    "        return None\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO: Return the length of the internal deque.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Adversary Neural Networks\n",
    "\n",
    "Configure actor and critic networks, as well as an `AgentNetwork` grouping together actor, critic,\n",
    "and corresponding target networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 256\n",
    "\n",
    "\n",
    "class MpeActorNetwork(nn.Module):\n",
    "    \"\"\"The agent policy network (actor).\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        # TODO: Configure a standard neural network with three linear layers (from state to action).\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: relu, relu and... the action space is [0, 1]: which final non-linearity?\n",
    "        return None\n",
    "\n",
    "\n",
    "class MpeCriticNetwork(nn.Module):\n",
    "    \"\"\"The agent centralized critic network (getting all observations).\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, num_agents):\n",
    "        super().__init__()\n",
    "        # TODO: Compute the centralized critic input size. Remember: the critic takes all agents'\n",
    "        #       observations and actions into account!\n",
    "        self.critic_input_size = None\n",
    "        # TODO: Configure another 3 layers neural network, that outputs the Q value.\n",
    "        pass\n",
    "\n",
    "    def forward(self, obs_all_agents, actions_all_agents):\n",
    "        # TODO: First, compute the centralized critic inputs. The dimension of the observation (and\n",
    "        #       action) tensors are: (N_agents, Batch_size, ...).\n",
    "        x = None\n",
    "        # TODO: Permute the tensor to have batch dimension first: (Batch_size, N_agents, ...).\n",
    "        x = None\n",
    "        # TODO: Here we go, relu, relu and... do not apply any non-linearity in the last layer!\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentNetwork(nn.Module):\n",
    "    \"\"\"Convenient network to encapsulate both actor, critic, and target networks for an agent.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_agents,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        # The `actor_net` needs to be instantiated via: actor_net(state_dim, action_dim).\n",
    "        actor_net: Type[nn.Module],\n",
    "        # The `critic_net` needs to be instantiated via: critic_net(state_dim, action_dim).\n",
    "        critic_net: Type[nn.Module],\n",
    "        lr_actor=1e-4,\n",
    "        lr_critic=1e-3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_dim\n",
    "        self.num_agents = num_agents\n",
    "\n",
    "        # TODO: Setup the actor network\n",
    "        self.actor = None\n",
    "        # TODO: Setup the actor target network\n",
    "        self.actor_target = None\n",
    "        # TODO: Copy the parameters of the actor network into the actor target network. Also, make\n",
    "        #       sure to set the target network into eval mode.\n",
    "        # ...\n",
    "        # TODO: Created the Adam optimizer for the actor network (eps=1e-5)\n",
    "        self.actor_optimizer = None\n",
    "\n",
    "        # TODO: Setup the critic network\n",
    "        self.critic = None\n",
    "        # TODO: Setup the critic target network\n",
    "        self.critic_target = None\n",
    "        # TODO: Copy the parameters of the critic network into the critic target network. Also, make\n",
    "        #       sure to set the target network into eval mode.\n",
    "        # ...\n",
    "        # TODO: Created the Adam optimizer for the critic network (eps=1e-5)\n",
    "        self.critic_optimizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MA-DDPG Agent\n",
    "\n",
    "MA-DDPG implementation which resembles the one described in the original paper. The actor / critic\n",
    "networks are generic, so that we can plug in different architecture for different environments (see\n",
    "`Pistonball` below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MA_DDPG:\n",
    "    \"\"\"The MA-DDPG algorithm: multi-agent training!\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        actor_net: Type[nn.Module],\n",
    "        critic_net: Type[nn.Module],\n",
    "        agent_ids,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        action_min=-1,\n",
    "        action_max=1,\n",
    "        gamma=0.99,\n",
    "        min_buffer_size=2048,\n",
    "        buffer_len=int(1e5),\n",
    "        batch_size=128,\n",
    "        update_every=8,\n",
    "        n_updates=4,\n",
    "        update_policy_every=2,\n",
    "        noise_start=1.0,\n",
    "        noise_reduction=0.99995,\n",
    "        noise_min=0.25,\n",
    "        max_grad_norm=0.5,\n",
    "        lr_actor=1e-4,\n",
    "        lr_critic=1e-3,\n",
    "        tau=0.01,\n",
    "    ):\n",
    "        self.num_agents = len(agent_ids)\n",
    "        self.agent_id_map = {id: index for (index, id) in enumerate(agent_ids)}\n",
    "        self.agent_index_map = {index: id for (index, id) in enumerate(agent_ids)}\n",
    "        self.action_dim = action_dim\n",
    "        self.action_min = action_min\n",
    "        self.action_max = action_max\n",
    "        self.gamma = gamma\n",
    "        self.noise = noise_start\n",
    "        self.noise_reduction = noise_reduction\n",
    "        self.noise_min = noise_min\n",
    "        self.batch_size = batch_size\n",
    "        self.min_buffer_size = min_buffer_size\n",
    "        self.n_updates = n_updates\n",
    "        self.update_every = update_every\n",
    "        self.update_policy_every = update_policy_every\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.tau = tau\n",
    "        self.t_step = 0\n",
    "\n",
    "        # TODO: Create the ReplayBuffer.\n",
    "        self.replay_buffer = None\n",
    "\n",
    "        # TODO: Create N agents (AgentNetwork) for the multi-agent algorithm.\n",
    "        self.agents = None\n",
    "\n",
    "    def step(self, obs, actions, next_obs, rewards, dones):\n",
    "        \"\"\"Multi-agent step: tracks the experience, and updates the agents using batch learning.\"\"\"\n",
    "        # Convert dicts keyed by agent ID to lists/tensors ordered by agent index. This is helpful\n",
    "        # to convert observations, etc. from PettingZoo ParallelEnv to a format suited to our MADDPG\n",
    "        # algorithm implementation.\n",
    "        convert = lambda t: [t[self.agent_index_map[i]] for i in range(self.num_agents)]\n",
    "\n",
    "        # TODO: Convert inputs and create corresponding tensors.\n",
    "        obs_t = None\n",
    "        actions_t = None\n",
    "        next_obs_t = None\n",
    "        rewards_t = None\n",
    "        dones_t = None  # Hint: uint8\n",
    "\n",
    "        # TODO: Store transition in the replay buffer.\n",
    "        # ...\n",
    "        # TODO: Increment global timestamp!\n",
    "        self.t_step = None\n",
    "\n",
    "        # Learning Step\n",
    "        if len(self.replay_buffer) > self.min_buffer_size and self.t_step % self.update_every == 0:\n",
    "            # TODO: Update noise with exponential decay.\n",
    "            self.noise = None\n",
    "            for cycle in range(self.n_updates):\n",
    "                print(f\"Learning cycle {cycle+1}/{self.n_updates}...\".ljust(100), end=\"\\r\")\n",
    "                # TODO: Sample a batch of transitions from the replay buffer.\n",
    "                samples = None\n",
    "                obs, actions, rewards, next_obs, dones = None\n",
    "                for a_i in range(len(self.agents)):\n",
    "                    # TODO: Perform critic update. Hint: use method defined below.\n",
    "                    # TODO: If is_actor_update(), perform actor update.\n",
    "                    pass\n",
    "            # TODO: Soft update the target networks towards the actual networks. IMPORTANT: Update\n",
    "            #       the target networks _after_ all the updates (which use target predictions\n",
    "            #       themselves). Hint: use the `update_targets` method below.\n",
    "            pass\n",
    "\n",
    "    def learn_actor(self, agent_number: int, obs):\n",
    "        \"\"\"Perform a single gradient descent step on the actor of the selected agent.\n",
    "\n",
    "        The `obs` parameter contains the observations from _all_ agents.\n",
    "        \"\"\"\n",
    "        # TODO: Get the agent.\n",
    "        agent = None\n",
    "        # TODO: Get _all_ action preditionsfrom all the agents. Make sure to detach() all the other\n",
    "        #       agent predictions!\n",
    "        actions_pred = None\n",
    "        # TODO: Compute the actor_loss as the negative value computed by the critic.\n",
    "        actor_loss = None\n",
    "\n",
    "        # TODO: Perform an optimizer step. Zero-grad, backward, step. Make sure to clip gradients\n",
    "        #       Before calling step(). Hint: Use clip_grad_norm_.\n",
    "        pass\n",
    "\n",
    "    def learn_critic(self, agent_number: int, obs, actions, rewards, next_obs, dones):\n",
    "        \"\"\"Perform a single gradient descent step on the critic of the selected agent.\n",
    "\n",
    "        The input tensors contains the observations, actions, etc. from _all_ agents.\n",
    "        \"\"\"\n",
    "        # TODO: Get the agent.\n",
    "        agent = None\n",
    "        with torch.no_grad():\n",
    "            # TODO: Get predicted all next-state actions and Q values from target models.\n",
    "            actions_next = None\n",
    "            # TODO: Get the next Q targets from target model for this agent.\n",
    "            Q_targets_next = None\n",
    "            # TODO: Compute Q targets for current states. Remember:\n",
    "            #       Q_target = reward + gamma * next_target * (1 - done)\n",
    "            Q_targets = None\n",
    "\n",
    "        # TODO: Compute the Q_expected using the critic.\n",
    "        Q_expected = None\n",
    "        # TODO: Calculate the critic loss (MSE).\n",
    "        critic_loss = None\n",
    "\n",
    "        # TODO: Perform an optimizer step. Zero-grad, backward, step. Make sure to clip gradients\n",
    "        #       Before calling step(). Hint: Use clip_grad_norm_.\n",
    "        pass\n",
    "\n",
    "    def update_targets(self):\n",
    "        \"\"\"Soft-update target networks.\"\"\"\n",
    "        for agent in self.agents:\n",
    "            # TODO: Update critic target parameters. If is_actor_update(), update actor target\n",
    "            #       parameters. Hint: use `soft_update_model_params`.\n",
    "            pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_act(self, agent_id: AgentID, obs: np.ndarray, add_noise=False):\n",
    "        \"\"\"Get action from the actor network for evaluation/execution.\"\"\"\n",
    "        # TODO: Convert the obs into a tensor. Make sure to add the batch dimension (unsqueeze).\n",
    "        obs_t = None\n",
    "        # TODO: Get the agent_number from the agent_id_map\n",
    "        agent_number = None\n",
    "        # TODO: Lookup the agent.\n",
    "        agent = None\n",
    "\n",
    "        # TODO: Compute the action. Hint: actor network to the rescue.\n",
    "        action = None\n",
    "\n",
    "        if add_noise:\n",
    "            # TODO: Add noise. Hint: use the get_noise() method below.\n",
    "            action += None\n",
    "\n",
    "        # TODO: Convert the action to numpy. Make sure to remove the batch dimension.\n",
    "        action = None\n",
    "\n",
    "        # Return the action, clipped between [action_min, action_max] for safety.\n",
    "        return None\n",
    "\n",
    "    def is_actor_update(self) -> bool:\n",
    "        \"\"\"Determines the actor network should be updated in the current global timestep.\"\"\"\n",
    "        return self.t_step % (self.update_every * self.update_policy_every) == 0\n",
    "\n",
    "    def get_noise(self) -> torch.Tensor:\n",
    "        \"\"\"Compute the Gaussian noise.\"\"\"\n",
    "        # TODO: Return the gaussian noise. Hint: use np.random.normal.\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "MA-DDPG training loop. It expects a PettingZoo [AECEnv](https://pettingzoo.farama.org/api/aec/) but\n",
    "converts it into [parallel](https://pettingzoo.farama.org/api/parallel/) right away for convenience\n",
    "of training this notebook MA-DDPG algorithm implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_maddpg(aec_env: AECEnv, agent: MA_DDPG = None, max_episodes=1_000):\n",
    "    \"\"\"Main training loop for MA-DDPG.\"\"\"\n",
    "    # TODO: Convert the AECEnv to ParallelEnv for ease of training MA-DDPG with this notebook\n",
    "    #       specific setup. Hint: use the `aec_to_parallel` utility.\n",
    "    env = None\n",
    "\n",
    "    returns = []\n",
    "    timestep = 0\n",
    "    env_agent_ids = aec_env.agents\n",
    "\n",
    "    print(f\"Training started. Collecting initial experiences...\".ljust(100), end=\"\\r\")\n",
    "    for n_episode in range(1, max_episodes + 1):\n",
    "        # TODO: Initialize a dictionary of {agent_id: 0.0} to record episode returns.\n",
    "        episode_returns = None\n",
    "        # TODO: Reset the environment.\n",
    "        obs, _ = None\n",
    "\n",
    "        # PettingZoo removes agents from env.agents when those agents have completed their episode.\n",
    "        while env.agents:\n",
    "            # TODO: Get all the actions from all agents as a dictionary of {agent_id: action}\n",
    "            actions = None\n",
    "\n",
    "            # TODO: Perform a single step in the parallel environment.\n",
    "            next_obs, rewards, terms, truncs, _ = None\n",
    "\n",
    "            # TODO: Compute whether agents have completed as a dictionary of agent_id: done.\n",
    "            #       Hint: an agent is done if terminated or truncated.\n",
    "            dones = None\n",
    "\n",
    "            # TODO: Call step(...) on the agent!\n",
    "            pass\n",
    "\n",
    "            # TODO: Move to the next observation... easy to overlook!\n",
    "            obs = None\n",
    "\n",
    "            # TODO: Update timestep, and all agent episode returns\n",
    "            timestep += None\n",
    "            for agent_id in env_agent_ids:\n",
    "                episode_returns[agent_id] += None\n",
    "\n",
    "        # Record the current episode returns.\n",
    "        returns.append(episode_returns)\n",
    "\n",
    "        # Convenient log message every N episodes.\n",
    "        if n_episode % 25 == 0:\n",
    "            last_100_returns = returns[-100:]\n",
    "            avg_returns_by_agent = {id: 0.0 for id in env_agent_ids}\n",
    "            for ret in last_100_returns:\n",
    "                for id in env_agent_ids:\n",
    "                    avg_returns_by_agent[id] += ret[id]\n",
    "            avg_returns_by_agent = {\n",
    "                key: value / len(last_100_returns) for key, value in avg_returns_by_agent.items()\n",
    "            }\n",
    "            avg_return = np.mean([v for v in avg_returns_by_agent.values()])\n",
    "            formatted_returns = (\n",
    "                \"{\"\n",
    "                + \", \".join(f\"{repr(k)}: {v:.3f}\" for k, v in avg_returns_by_agent.items())\n",
    "                + \"}\"\n",
    "            )\n",
    "            print(f\"[Episode {n_episode} AVG: {avg_return:.3f}] {formatted_returns}\".ljust(100))\n",
    "\n",
    "    print(f\"Episode {n_episode} terminating training, average reward: {avg_return:.3f}\".ljust(100))\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Simple Adversary\n",
    "\n",
    "We can train the `simple_adversary_v3` environment with MA-DDPG and obtain policies that show good\n",
    "agents trying to trick the adversary and the adversary trying to become smarter. MA-DDPG has various\n",
    "limitations, some already notable with this MPE environment:\n",
    "\n",
    "- **Sensitivity to hyperparameters:** Finding the right learning rates, exploration parameters, and\n",
    "  network architectures often requires extensive tuning.\n",
    "- **Emergent Unwanted Behaviors:** Agents might learn suboptimal or even adversarial strategies that\n",
    "  exploit the weaknesses of other agents or the environment, even if these behaviors weren't\n",
    "  explicitly intended, landing in suboptimal policies. Sometimes early stopping is used.\n",
    "- **Scalability Issues:** As the number of agents increases, the size of the joint\n",
    "  action-observation space grows exponentially, making it computationally expensive and sample\n",
    "  inefficient to learn joint policies. We can see this in the _Pistonball_ environment below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_agent_returns(episode_returns):\n",
    "    \"\"\"Convenient function to plot agents and rolling average returns.\"\"\"\n",
    "    df = pd.DataFrame(episode_returns)\n",
    "    agent_ids = df.columns.tolist()\n",
    "    window_size = 100\n",
    "\n",
    "    for agent_id in agent_ids:\n",
    "        df[f\"{agent_id}_running_avg\"] = (\n",
    "            df[agent_id].rolling(window=window_size, min_periods=1).mean()\n",
    "        )\n",
    "    df[\"overall_average_return\"] = df[agent_ids].mean(axis=1)\n",
    "    df[\"running_overall_average_return\"] = (\n",
    "        df[\"overall_average_return\"].rolling(window=window_size, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(10, 3.7))\n",
    "    colors = cm.get_cmap(\"tab10\", len(agent_ids))\n",
    "    for i, agent_id in enumerate(agent_ids):\n",
    "        agent_color = colors(i)\n",
    "        plt.plot(\n",
    "            df.index,\n",
    "            df[agent_id],\n",
    "            label=f\"{agent_id} Raw Return\",\n",
    "            alpha=0.3,\n",
    "            color=agent_color,\n",
    "            linestyle=\":\",\n",
    "        )\n",
    "        plt.plot(\n",
    "            df.index,\n",
    "            df[f\"{agent_id}_running_avg\"],\n",
    "            label=f\"{agent_id} Running Avg\",\n",
    "            color=agent_color,\n",
    "            linewidth=1.5,\n",
    "        )\n",
    "    plt.plot(\n",
    "        df.index,\n",
    "        df[\"running_overall_average_return\"],\n",
    "        label=\"Running Avg (All Agents)\",\n",
    "        color=\"black\",\n",
    "        linewidth=2.5,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Return\")\n",
    "    plt.title(\"Agent Returns: Raw, Individual Running Averages, and Overall Running Average\")\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic learning\n",
    "\n",
    "With `~7k` episodes and a first set of hyperparameters, we see the agents trying to distance the\n",
    "adversary when it gets too close to the target.\n",
    "\n",
    "NOTE: I didn't spend too much time in configuring the hyperparameters and/or tailor the MA-DDPG\n",
    "implementation to this MPE environment (e.g., using the\n",
    "[Ornstein Uhlenbeck noise](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process) as in\n",
    "the original paper). Hence, it is very possible that we can train this environment much faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_env: AECEnv = create_simple_adversary_env()\n",
    "ad_env = init_random(ad_env)\n",
    "agent_ids = ad_env.possible_agents\n",
    "state_dim = ad_env.observation_space(agent_ids[0]).shape[0]\n",
    "action_dim = ad_env.action_space(agent_ids[0]).shape[0]\n",
    "\n",
    "agent_adv_basic = MA_DDPG(\n",
    "    MpeActorNetwork,\n",
    "    MpeCriticNetwork,\n",
    "    agent_ids,\n",
    "    state_dim,\n",
    "    action_dim,\n",
    "    action_min=0,\n",
    "    gamma=0.995,\n",
    "    min_buffer_size=2048,\n",
    "    buffer_len=int(3e5),\n",
    "    batch_size=128,\n",
    "    update_every=50,\n",
    "    n_updates=8,\n",
    "    update_policy_every=4,\n",
    "    noise_start=0.5,\n",
    "    noise_reduction=0.99995,\n",
    "    noise_min=0.1,\n",
    "    lr_actor=5e-5,\n",
    "    lr_critic=5e-4,\n",
    "    max_grad_norm=0.1,\n",
    "    tau=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25m training on GPU. Consider running on CPU given that CPU/GPU communication is bottleneck.\n",
    "adv_scores_basic = train_maddpg(ad_env, agent_adv_basic, max_episodes=7_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_agent_returns(adv_scores_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pettingzoo_simulation(\n",
    "    simple_adversary_v3,\n",
    "    agent=agent_adv_basic,\n",
    "    seed=100,\n",
    "    wrappers=[simple_adversary_v3_wrappers],\n",
    "    **MPE_SIMPLE_ADVERSARY_KWARGS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better strategy\n",
    "\n",
    "With `30k+` episodes and a different set of hyperparameters, we see how they tend to learn how to\n",
    "split apart as a better strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_env: AECEnv = create_simple_adversary_env()\n",
    "ad_env = init_random(ad_env)\n",
    "agent_ids = ad_env.possible_agents\n",
    "state_dim = ad_env.observation_space(agent_ids[0]).shape[0]\n",
    "action_dim = ad_env.action_space(agent_ids[0]).shape[0]\n",
    "\n",
    "agent_adv_better = MA_DDPG(\n",
    "    MpeActorNetwork,\n",
    "    MpeCriticNetwork,\n",
    "    agent_ids,\n",
    "    state_dim,\n",
    "    action_dim,\n",
    "    action_min=0,\n",
    "    gamma=0.95,\n",
    "    min_buffer_size=2048,\n",
    "    buffer_len=int(3e5),\n",
    "    batch_size=128,\n",
    "    update_every=16,\n",
    "    n_updates=4,\n",
    "    update_policy_every=2,\n",
    "    noise_start=1.0,\n",
    "    noise_reduction=0.99995,\n",
    "    noise_min=0.25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2h training on GPU. Consider running on CPU given that CPU/GPU communication is bottleneck.\n",
    "adv_scores_better = train_maddpg(ad_env, agent_adv_better, max_episodes=30_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_agent_returns(adv_scores_better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pettingzoo_simulation(\n",
    "    simple_adversary_v3,\n",
    "    agent=agent_adv_better,\n",
    "    seed=100,\n",
    "    wrappers=[simple_adversary_v3_wrappers],\n",
    "    **MPE_SIMPLE_ADVERSARY_KWARGS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pistonball v6\n",
    "\n",
    "Pistonball is a \"_physics based cooperative game where the goal is to move the ball to the left-wall\n",
    "of the game border by activating the vertically moving pistons. To achieve an optimal policy for the\n",
    "environment, pistons must learn highly coordinated behavior_\".\n",
    "\n",
    "Pistonball presents difficulties for MADDPG because coordinating and scaling a large number of\n",
    "agents with high-dimensional local image observations is complex, and the shared reward structure\n",
    "makes it hard for the centralized critic to accurately assign credit for collective success or\n",
    "failure to individual agents, hindering the learning of precise, coordinated actions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters to train pistonball with 5 pistons.\n",
    "PISTONBALL_N5 = {\n",
    "    \"n_pistons\": 5,\n",
    "    \"max_cycles\": 20,\n",
    "    \"time_penalty\": -0.3,\n",
    "}\n",
    "\n",
    "FRAME_W = 32\n",
    "FRAME_H = 32\n",
    "\n",
    "\n",
    "def pistonball_v6_wrappers(zoo_env):\n",
    "    \"\"\"Wrappers to facilitate Pistonball training.\"\"\"\n",
    "    zoo_env = ss.color_reduction_v0(zoo_env, mode=\"B\")\n",
    "    zoo_env = ss.resize_v1(zoo_env, FRAME_W, FRAME_H)\n",
    "    zoo_env = ss.frame_stack_v1(zoo_env, stack_size=3)\n",
    "    zoo_env = ss.dtype_v0(zoo_env, \"uint8\")\n",
    "    return zoo_env\n",
    "\n",
    "\n",
    "def create_pistonball_env(**env_kwargs):\n",
    "    \"\"\"Utility function to create the Pistonball environment.\"\"\"\n",
    "    zoo_env: AECEnv = pistonball_v6.env(**env_kwargs)\n",
    "    zoo_env = pistonball_v6_wrappers(zoo_env)\n",
    "    return zoo_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env: AECEnv = create_pistonball_env()\n",
    "test_env.reset(seed=42)\n",
    "agents: list[AgentID] = test_env.agents\n",
    "print(agents)\n",
    "print(test_env.action_space(agents[0]))\n",
    "print(test_env.observation_space(agents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pettingzoo_simulation(pistonball_v6, wrappers=[pistonball_v6_wrappers], **PISTONBALL_N5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pistonball Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEAT_SIZE = 256\n",
    "N_STACKED_FRAMES = 3\n",
    "\n",
    "\n",
    "def convolutional_feature_extractor(output_dim) -> nn.Module:\n",
    "    \"\"\"Feature extractor for Pistonball stacked frames, shared by both actor and critic.\"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(N_STACKED_FRAMES, 32, 3, padding=1),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, 3, padding=1),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 128, 3, padding=1),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(128 * 4 * 4, output_dim),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "\n",
    "class PistonballActorNetwork(nn.Module):\n",
    "    \"\"\"Pistonball actor network.\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.features = convolutional_feature_extractor(FEAT_SIZE)\n",
    "        self.fc1 = nn.Linear(FEAT_SIZE, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float() / 255.0\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.features(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class PistonballCriticNetwork(nn.Module):\n",
    "    \"\"\"Pistonball critic network.\"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, num_agents):\n",
    "        super().__init__()\n",
    "        self.num_agents = num_agents\n",
    "        self.critic_input_size = (FEAT_SIZE + action_dim) * num_agents\n",
    "        self.features = convolutional_feature_extractor(FEAT_SIZE)\n",
    "        self.fc1 = nn.Linear(self.critic_input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, obs_all_agents, actions_all_agents):\n",
    "        # action_all_agents shape (n_agents, batch_size, action_dim)\n",
    "        actions_all_agents = actions_all_agents.permute(1, 0, 2)\n",
    "        x = obs_all_agents.float() / 255.0\n",
    "        x = x.permute(1, 0, 4, 2, 3)\n",
    "        batch_size, num_agents, channels, height, width = x.shape\n",
    "\n",
    "        x = x.reshape(batch_size * num_agents, channels, height, width)\n",
    "        x = self.features(x)  # (batch_size * num_agents, feature_size)\n",
    "        x = x.reshape(batch_size, num_agents, -1)  # (batch_size, num_agents, feature_size)\n",
    "\n",
    "        x = torch.cat((x, actions_all_agents), dim=2).float().to(DEVICE)\n",
    "        x = x.flatten(1, 2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training 5 Pistons\n",
    "\n",
    "In training just 5 pistons, MADDPG converged to a suboptimal policy where the second-to-right-most\n",
    "piston consistently moves up while the rest remain down. This simple coordinated behavior likely\n",
    "provides just enough upward force or a consistent bounce point to guide the ball towards the target\n",
    "zone, resulting in some positive reward. However, this strategy is far from optimal because true\n",
    "mastery of Pistonball requires dynamic, precise, and coordinated movements from all pistons to\n",
    "effectively control the ball. This convergence to a local optimum can happen in MADDPG due to the\n",
    "challenges of credit assignment in shared reward settings; the simple \"second piston up\" strategy\n",
    "consistently yields some reward, reinforcing this basic coordination and potentially preventing the\n",
    "algorithm from exploring and discovering the more complex, highly coordinated behaviors needed for\n",
    "optimal performance with multiple interacting agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv6_env: AECEnv = create_pistonball_env(**PISTONBALL_N5)\n",
    "pv6_env = init_random(pv6_env)\n",
    "agent_ids = pv6_env.possible_agents\n",
    "state_dim = pv6_env.observation_space(agent_ids[0]).shape[0]\n",
    "action_dim = pv6_env.action_space(agent_ids[0]).shape[0]\n",
    "\n",
    "agent_n5 = MA_DDPG(\n",
    "    PistonballActorNetwork,\n",
    "    PistonballCriticNetwork,\n",
    "    agent_ids,\n",
    "    state_dim,\n",
    "    action_dim,\n",
    "    gamma=0.999,\n",
    "    min_buffer_size=10_000,\n",
    "    buffer_len=int(110_000),\n",
    "    batch_size=128,\n",
    "    noise_start=1.0,\n",
    "    noise_reduction=(1.0 - 5e-6),\n",
    "    noise_min=0.5,\n",
    "    update_every=32,\n",
    "    n_updates=8,\n",
    "    update_policy_every=2,\n",
    "    lr_actor=1e-5,\n",
    "    lr_critic=5e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 minutes on GPU.\n",
    "p5_scores = train_maddpg(pv6_env, agent_n5, max_episodes=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_agent_returns(p5_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pettingzoo_simulation(\n",
    "    pistonball_v6, agent=agent_n5, wrappers=[pistonball_v6_wrappers], **PISTONBALL_N5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MA-PPO\n",
    "\n",
    "Given the cooperative nature of Pistonball and the need for learning complex, coordinated behaviors\n",
    "among a significant number of agents with local observations,\n",
    "[MAPPO](https://arxiv.org/abs/2103.01955) is a well-regarded and often high-performing choice for\n",
    "cooperative tasks requiring coordinated control\n",
    "\n",
    "1. **Strong Performance in Cooperative MARL:** MAPPO has demonstrated strong performance on various\n",
    "   cooperative multi-agent benchmarks, including environments similar to Pistonball.\n",
    "2. **Stable Training:** As an on-policy algorithm based on PPO, MAPPO tends to have more stable\n",
    "   training dynamics compared to off-policy methods like MADDPG, which can be prone to instability\n",
    "   in multi-agent settings. This stability can be crucial for converging to more optimal and complex\n",
    "   coordinated policies.\n",
    "3. **Potential for Better Practical Scalability:** While scaling to a very large number of agents\n",
    "   remains a challenge for most MARL algorithms, MAPPO's improved stability can often translate to\n",
    "   better performance and more reliable learning compared to MADDPG as the number of agents\n",
    "   increases, making it potentially more suitable for environments like Pistonball with a moderate\n",
    "   to large number of participants.\n",
    "4. **Handles Centralized Training with Decentralized Execution:** Like MADDPG, MAPPO utilizes the\n",
    "   effective centralized training with decentralized execution paradigm, allowing the critic to\n",
    "   leverage global information during learning while agents act independently.\n",
    "\n",
    "Try implementing it! MARL is a rapidly evolving field with other promising algorithms, explore other\n",
    "popular algorithms as well, like: VDN, QMIX, COMA, independent learning like IQL / IPPO!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlzh2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
