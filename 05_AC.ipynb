{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Methods\n",
    "\n",
    "In _value-based_ methods, we use a value function to determine the optimal policy. The exploration\n",
    "and exploitation tradeoff is left to manual tuning. In _policy-based_ methods, we learn the policy\n",
    "directly, but the Monte Carlo methods have high-variance and tend to be slower (mitigating that by\n",
    "collecting more samples causes the algorithm to be less sample-efficient).\n",
    "\n",
    "Actor-critic methods combine the two approaches:\n",
    "\n",
    "- We learn the _actor_ policy, to control how the agent behave.\n",
    "- We measure how good the chosen actions are via a _critic_, which learns the value function.\n",
    "\n",
    "Remember the _baseline_ in policy gradient? The _critic_ helps computing it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from util.gymnastics import DEVICE, gym_simulation\n",
    "from util.gymnastics import init_random, plot_scores\n",
    "from util.rl_algos import ReplayBuffer, Experience, soft_update_model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "For simplicity, use a simple training loop with a single environment collecting and learning single\n",
    "trajectories. All the algorithms in this notebook would benefit greatly from multiple trajectories\n",
    "and vectorized environments, and most standard implementations do so.\n",
    "\n",
    "NOTE: certain actor-critic algorithms benefit from adding noise during training. Hence, we include\n",
    "that flag in the `act` call for simplicity in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor_critic(\n",
    "    env: gym.Env, agent, n_episodes=10_000, max_t=300, solved_score=-370, log_every=50\n",
    "):\n",
    "    \"\"\"General training loop for actor-critic algorithms in this lecture.\"\"\"\n",
    "    # Records all episode scores.\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        # TODO: Reset the environment.\n",
    "        state, _ = None\n",
    "        # TODO: Reset the score to zero.\n",
    "        score = None\n",
    "        # Run training for max_timesteps.\n",
    "        for _ in range(max_t):\n",
    "            # TODO: Select an action via agent.act(state, add_noise=True) -> notice the noise!\n",
    "            action = None\n",
    "            # TODO: Perform a step in the environment.\n",
    "            next_state, reward, terminated, truncated, _ = None\n",
    "            # TODO: Compute `done`.\n",
    "            done = None\n",
    "            # TODO: Perform a step for the agent calling agent.step(...)\n",
    "            # ...\n",
    "            # Update state, score, and check termination.\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Record statistics and print debugging information.\n",
    "        scores.append(score)\n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        print(\n",
    "            f\"\\rEpisode {i_episode}\\tAverage Score: {avg_score:.2f}\",\n",
    "            end=\"\\n\" if i_episode % 50 == 0 else \"\",\n",
    "        )\n",
    "        if avg_score >= log_every:\n",
    "            print(f\"\\rEpisode {i_episode} solved environment!\\tAverage Score: {avg_score:.2f}\")\n",
    "            break\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage Actor Critic (A2C)\n",
    "\n",
    "The simplified version of a general 1-step [actor-critic](https://arxiv.org/abs/1602.01783)\n",
    "algorithm goes as follows:\n",
    "\n",
    "1.  In state $S_t$, the _actor_ outputs the action $A_t$ using the policy $\\pi_{\\theta}(S_t)$ and\n",
    "    obtaining $R_{t+1}$ and $S_{t+1}$\n",
    "2.  The _critic_ outputs the value of state and next state:\n",
    "    $V_t = \\^{v_w}(S_t),\n",
    "    V_{t+1} = \\^{v_w}(S_{t+1})$\n",
    "3.  We compute the _advantage_: $A_t = Q(s_t, a_t) - V(s_t) = [R_{t+1} + \\gamma V_{t+1}] - V_t$\n",
    "4.  The _actor_ updates the policy parameters $\\theta$ using the _advantage_:\n",
    "    $\\space \\space \\Delta \\theta = \\alpha \\nabla_{\\theta} [ log \\pi_{\\theta}(S_t) ] A_t$\n",
    "5.  The _critic_ updates the value function parameters $w$ minimizing $A_t$\n",
    "6.  Repeat\n",
    "\n",
    "A2C is an _on-policy_ algorithm that learns a stochastic policy (i.e., its output is a probability\n",
    "distribution over actions), basically implementing the algorithm above. The implementation in this\n",
    "notebook performs networks' updates every $n$ steps. For a robust implementation of A2C (and the\n",
    "following algorithms), check\n",
    "[Stable Baselines3 A2C](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C Environment\n",
    "\n",
    "The Acrobot environment fits well our simple implementation of A2C with its discrete action space\n",
    "and well-formed reward (without need of any reshaping).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_simulation(\"Acrobot-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage Computation\n",
    "\n",
    "As we briefly touched upon in the policy gradients notebook, in RL the _advantage_ is a measure that\n",
    "represents how much better a specific action is compared to the average action in a given state.\n",
    "Repeating the formula above:\n",
    "\n",
    "$\n",
    "A_t(s_t, a_t) = G_t - V(s_t) = Q(s_t, a_t) - V(s_t) \\approx [R_{t} + \\gamma V_{t+1}] - V_t\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advantages_and_returns(gamma, rewards, dones, values, next_value):\n",
    "    \"\"\"Returns the advantages and returns for a trajectory.\"\"\"\n",
    "    # TODO: Initialize the `running_return` to the `next_value` at the end of the trajectory.\n",
    "    running_return = None\n",
    "    # TODO: Initialize `returns` as a zero tensor with the same shape of `rewards` ().\n",
    "    #       Hint: consider using torch.zeros_like, and remember to move it to DEVICE!\n",
    "    returns = None\n",
    "    # Iterate backwards: from the latest timestamp to the earliest.\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        # TODO: `mask` is zero when the episode has completed at time `t`.\n",
    "        mask = None\n",
    "        # TODO: Compute the `running_return` with the recursive relationship.\n",
    "        #       Return at `t` is the reward at `t` plus the return at (t+1) weighted by gamma.\n",
    "        #       Remember to take the fact that the episode might have ended into account!\n",
    "        running_return = None\n",
    "        # TODO: Set the return at time `t` as the `running_return`.\n",
    "        returns[t] = None\n",
    "    # TODO: Compute the advantages as the difference between returns and values.\n",
    "    advantages = None\n",
    "    return advantages, returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C Neural Networks\n",
    "\n",
    "The actor learns a probability distribution over the possible actions, while the critic learns the\n",
    "value for the state configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetworkA2C(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super().__init__(ActorNetworkA2C, self)\n",
    "        # TODO: Create three linear layers, the input of the policy network is a state,\n",
    "        #       and the output a the discrete actions' logits.\n",
    "\n",
    "    def forward(self, state):\n",
    "        # TODO: ReLU, ReLU, and just the last linear layer to get the logits :)\n",
    "        pass\n",
    "\n",
    "\n",
    "class CriticNetworkA2C(nn.Module):\n",
    "    def __init__(self, state_size, hidden_size=128):\n",
    "        super().__init__(CriticNetworkA2C, self)\n",
    "        # TODO: Same architecture as the actor network, but output a single value.\n",
    "\n",
    "    def forward(self, state):\n",
    "        # TODO: ReLU, ReLU, and just the last linear layer to get the value :)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentA2C:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size,\n",
    "        action_size,\n",
    "        gamma=0.99,\n",
    "        vloss_coeff=0.5,\n",
    "        ent_coeff=0.01,\n",
    "        learn_every=1000,\n",
    "        lr=1e-3,\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.vloss_coeff = vloss_coeff\n",
    "        self.ent_coeff = ent_coeff\n",
    "        self.learn_every = learn_every\n",
    "\n",
    "        # Actor and critic networks.\n",
    "        # TODO: Instantiate the actor and critic networks, as well as a single Adam optimizer.\n",
    "        #       Hint: You can concatenate the network parameters converting them as list().\n",
    "        self.actor = None\n",
    "        self.critic = None\n",
    "        self.optimizer = None\n",
    "\n",
    "        # Empty list of experiences to buffer the data for the next learning cycle.\n",
    "        self.experiences = []\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, state, add_noise=False):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        # TODO: Convert the state into a tensor.\n",
    "        state = None\n",
    "        # TODO: Get the probability distribution via `get_action_probs`\n",
    "        probs = None\n",
    "        # TODO: Get the action by sampling the distribution\n",
    "        action = None\n",
    "        return action.item()\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in the buffer, and trigger learning if collected enough.\"\"\"\n",
    "        # TODO: Create an `Experience` given the inputs, and append it to the `experiences` list.\n",
    "        # ...\n",
    "        # TODO: If we have enough experiences (i.e., check `learn_every`), then trigger `learn()`\n",
    "        #       and reset the `experiences` list.\n",
    "        # ...\n",
    "        pass\n",
    "\n",
    "    def get_action_probs(self, states):\n",
    "        \"\"\"Returns the action probability distribution over the states.\"\"\"\n",
    "        # TODO: Compute the logits using the `actor` network.\n",
    "        logits = None\n",
    "        # TODO: Return the categorical distribution of the logits.\n",
    "        return None\n",
    "\n",
    "    def learn(self):\n",
    "        # TODO: Unpack the current `experiences` using `ReplayBuffer.unpack`. We have already\n",
    "        #       seen these objects and methods in the previous notebooks :)\n",
    "        states, actions, rewards, next_states, dones = None\n",
    "\n",
    "        # TODO: Get the action probability distribution.\n",
    "        probs = None\n",
    "        # TODO: Get the log probabilities of the actions.\n",
    "        logprobs = None\n",
    "        # TODO: Get the entropy of the distribution.\n",
    "        entropy = None\n",
    "        # TODO: Compute the values using the `critic` network. Keep an eye on dimensionality.\n",
    "        values = None\n",
    "\n",
    "        # Compute returns and advantages.\n",
    "        with torch.no_grad():\n",
    "            # TODO: Get the last `next_state`.\n",
    "            next_state = None\n",
    "            # TODO: Compute the `next_state` value using the critic.\n",
    "            next_value = None\n",
    "            # TODO: Compute advantages and returns. Hint: make sure to detach() the values, and\n",
    "            #       to double check dimensionality.\n",
    "            advantages, returns = None\n",
    "\n",
    "        # TODO: Get the policy_loss as the mean of the negative product of logprobs and advantages.\n",
    "        policy_loss = None\n",
    "        # TODO: Get the value loss as MSE between values and returns.\n",
    "        value_loss = None\n",
    "        # TODO: Get the total loss as the policy_loss plus value_loss and entropy loss weighted by\n",
    "        #       their corresponding coefficients.\n",
    "        loss = None\n",
    "\n",
    "        # TODO: Perform actor/critic gradient descent update on the loss. Call zero_grad() on the\n",
    "        #       optimizer, backward() on the loss, and step() on the optimizer. Before calling step()\n",
    "        #       make sure to clip the gradients to avoid too large updates that could drift apart the\n",
    "        #       learning process (use `nn.utils.clip_grad_norm_` and 0.5).\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with init_random(gym.make(\"Acrobot-v1\")) as env:\n",
    "    agent_a2c = AgentA2C(env.observation_space.shape[0], env.action_space.n)\n",
    "    scores_a2c = train_actor_critic(env, agent_a2c, max_t=500, solved_score=-150)\n",
    "plot_scores(scores_a2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_simulation(\"Acrobot-v1\", agent_a2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning Based Actor-Critic Algorithms\n",
    "\n",
    "We are going to implement modern actor-critic algorithms such as DDPG, TD3, SAC. They are closely\n",
    "related to Q-Learning, and in-between DQN and policy-gradient methods. All these algorithms are\n",
    "analyzed in detail in [OpenAI SpinningUp](https://spinningup.openai.com/en/latest) and I strongly\n",
    "recommend to read that amazing resource!\n",
    "\n",
    "That is because these algorithms learn approximators for the optimal action-value function\n",
    "$Q^*(s, a)$ and optimal (deterministic, excluding SAC) policy $a^*(s) = \\argmax_{a} Q^*(s, a)$\n",
    "exploting the fact that the action-value function is differentiable.\n",
    "\n",
    "Finding the max of the action-value function would be an optimization problem in and of itself, but\n",
    "instead we learn a deterministic policy $\\mu(s)$ such that: $Q(s, \\mu(s)) \\approx \\max_a Q^(s,a)$.\n",
    "All of these algorithms are off-policy and use a replay buffer (like DQN).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "The `Pendulum-v1` environment is hard to learn from A2C. It is a continuous control problem that\n",
    "needs very precise actions (networks and logprob computation need to be updated non-trivially);\n",
    "rewards are very negative and smooth initially (the standard advantage computation might fall\n",
    "short); exploration matters quite a bit. Q-Learning based algorithms work quite well instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For convenience, harcoding actions' interval [-2.0, 2.0]\n",
    "ACTION_SCALE = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_simulation(\"Pendulum-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Models\n",
    "\n",
    "The action space is continuous, and the actor learns directly how to output the action. The critic\n",
    "learns the Q function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, fc1_units=400, fc2_units=300):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        # TODO: Create three linear layers, first two initialized via Kaiming normal, the last one\n",
    "        #       with uniform distribution in [-3e-3, 3e-3]. the input of the policy network is a\n",
    "        #       state, while the output a (continuous) action.\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        # TODO: Use ReLU, ReLU, TanH :)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, fcs1_units=400, fc2_units=300):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        # TODO: Same architecture as the policy network, but now the input is the flattened state\n",
    "        #       and action, while the output is a single value: the state-action value.\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        # TODO: concat state and action (pay attention at the dimension!), ReLU for non-linearity.\n",
    "        #       The output is directly the output of the last linear layer (without non-linearity).\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPG\n",
    "\n",
    "The _Deep Deterministic Policy Gradient_ algorithm can be thought the DQN algorithm for continuous\n",
    "action spaces, and it uses the same techniques: replay-buffer + target-networks (for both actor and\n",
    "critic).\n",
    "\n",
    "We already leant how the _actor_ learns a deterministic policy $\\mu(s)$ maximizing the critic value.\n",
    "The _critic_ learns the action-value function $Q(s, a)$ minimizing a _mean-squared Bellman error_\n",
    "(MSBE - the squared _advantage_ if you will) like DQN.\n",
    "\n",
    "Because the policy is deterministic, the _exploration / exploitation_ tradeoff can be tuned adding\n",
    "noise to the action. In the [original paper](https://proceedings.mlr.press/v32/silver14.pdf), the\n",
    "authors recommended to use\n",
    "[Ornstein-Uhlenbeck noise](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process), but it\n",
    "turns out that a zero-mean Gaussian noise works just as well (hence, we'll use that here).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentDDPG:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size,\n",
    "        action_size,\n",
    "        start_mem_size=128,\n",
    "        gamma=0.99,\n",
    "        lr_actor=1e-4,\n",
    "        lr_critic=1e-3,\n",
    "        exploration_noise_scale=0.1,\n",
    "    ):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.start_mem_size = start_mem_size\n",
    "        self.gamma = gamma\n",
    "        self.exploration_noise_scale = exploration_noise_scale\n",
    "\n",
    "        # Actor network (w/ target network)\n",
    "        # TODO: Create actor local and target networks, plus the Adam optimizer. Make sure to keep\n",
    "        #       the target network in eval mode and copy the local network parameters.\n",
    "        self.actor = None\n",
    "        self.actor_target = None\n",
    "        # ...\n",
    "        self.actor_optimizer = None\n",
    "\n",
    "        # Critic network (w/ target network)\n",
    "        # TODO: Create critic local and target networks, plus the Adam optimizer. Make sure to keep\n",
    "        #       the target network in eval mode and copy the local network parameters.\n",
    "        self.critic = None\n",
    "        self.critic_target = None\n",
    "        # ...\n",
    "        self.critic_optimizer = None\n",
    "\n",
    "        # TODO: Create the replay buffer (using the provided one in the `util` module).\n",
    "        self.memory = None\n",
    "\n",
    "    @torch.no_grad\n",
    "    def act(self, state: np.array, add_noise=False):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        # TODO: Convert the state into a tensor.\n",
    "        state = None\n",
    "        # TODO: Get the action via the actor local network\n",
    "        action = None\n",
    "        # In the original paper, the noise is generated via an Ornstein-Uhlenbeck process. It turns\n",
    "        # out, a normal gaussian noise works just as well. Hence, that's what we use.\n",
    "        if add_noise:\n",
    "            # TODO: Add noise via normal distribution (np.random.normal) times the parameter\n",
    "            #       `self.exploration_noise_scale`.\n",
    "            action += None\n",
    "        # TODO: Return the action, making sure to clip it based on ACTION_SCALE.\n",
    "        return None\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # TODO: Save experience / reward in the memory.\n",
    "        # ...\n",
    "\n",
    "        # TODO: Learn, if enough samples are available in memory.\n",
    "        if len(self.memory) > self.start_mem_size:\n",
    "            pass\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # CRITIC UPDATE.\n",
    "        with torch.no_grad():\n",
    "            # TODO: Get the next actions using the actor target on next_states.\n",
    "            actions_next = None\n",
    "            # TODO: Get the Q_targets_next using the critic target.\n",
    "            Q_targets_next = None\n",
    "            # TODO: Compute Q targets for current states: rewards + (gamma * Q_targets * (1-dones))\n",
    "            Q_targets = None\n",
    "\n",
    "        # TODO: Compute the Q_expected using the critic local network.\n",
    "        Q_values = None\n",
    "        # TODO: Compute critic loss: MSE between Q_expected and Q_targets.\n",
    "        critic_loss = None\n",
    "        # TODO: Perform a minimization step of the critic loss with its optimizer.\n",
    "        # ...\n",
    "\n",
    "        # ACTOR UPDATE.\n",
    "        # TODO: Compute the action predictions via the actor_local network.\n",
    "        actions_pred = None\n",
    "        # TODO: Compute actor loss, which is the negative critic_local(states, actions_pred) mean.\n",
    "        actor_loss = None\n",
    "        # TODO: Perform a minimization step of the actor loss with its optimizer.\n",
    "        # ...\n",
    "\n",
    "        # TODO: update target networks, calling the `soft_update_model_params` utility function.\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with init_random(gym.make(\"Pendulum-v1\")) as env:\n",
    "    agent_ddpg = AgentDDPG(env.observation_space.shape[0], env.action_space.shape[0])\n",
    "    scores_ddpg = train_actor_critic(env, agent_ddpg)\n",
    "plot_scores(scores_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_simulation(\"Pendulum-v1\", agent_ddpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD3\n",
    "\n",
    "The _Twin Delayed DDPG_ (TD3) algorithm expands on DDPG with a couple of additional tricks:\n",
    "\n",
    "- It learns _two_ Q functions, and uses the smaller Q value to for the target. That is to address\n",
    "  overestimation of Q values in DDPG. The \"_twin_\" part of the name comes from this.\n",
    "- Updates the policy (and target) networks less frequently than the Q function (hence, \"_delayed_\")\n",
    "  and that is to keep the target and learning more stable.\n",
    "- Finally, it adds noise to the target action to \"smooth out\" the action value and make it harder\n",
    "  for the policy to exploit errors in the Q function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentTD3:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size,\n",
    "        action_size,\n",
    "        start_mem_size=128,\n",
    "        gamma=0.99,\n",
    "        lr_actor=1e-4,\n",
    "        lr_critic=1e-3,\n",
    "        exploration_noise_scale=0.1,\n",
    "        policy_noise=0.2,\n",
    "        noise_clamp=0.5,\n",
    "        policy_freq=2,\n",
    "    ):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.start_mem_size = start_mem_size\n",
    "        self.gamma = gamma\n",
    "        self.exploration_noise_scale = exploration_noise_scale\n",
    "        self.policy_noise = policy_noise\n",
    "        self.noise_clamp = noise_clamp\n",
    "        self.policy_freq = policy_freq\n",
    "        self.t_step = 0\n",
    "\n",
    "        # Actor network (w/ target network)\n",
    "        # TODO: Build the same actor local and target network + optimizer as DDPG.\n",
    "        # ...\n",
    "\n",
    "        # TD3 trick n.1: Twin critic networks (w/ target network)\n",
    "        # TODO: Build **two** twin critic networks!\n",
    "        self.twin_critic_1 = None\n",
    "        # ...\n",
    "        self.twin_critic_2 = None\n",
    "        # ...\n",
    "\n",
    "        # TODO: Build a single Adam optimizer (hint: concatenate all parameters as list).\n",
    "        self.critic_optimizer = None\n",
    "\n",
    "        # TODO: Instantiate the replay buffer.\n",
    "        self.memory = ReplayBuffer()\n",
    "\n",
    "    @torch.no_grad\n",
    "    def act(self, state, add_noise=False):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        # TODO: Convert the state into a tensor.\n",
    "        state = None\n",
    "        # TODO: Get the action via the actor local network\n",
    "        action = None\n",
    "        # In the original paper, the noise is generated via an Ornstein-Uhlenbeck process. It turns\n",
    "        # out, a normal gaussian noise works just as well. Hence, that's what we use.\n",
    "        if add_noise:\n",
    "            # TODO: Add noise via normal distribution (np.random.normal) times the parameter\n",
    "            #       `self.exploration_noise_scale`.\n",
    "            action += None\n",
    "        # TODO: Return the action, making sure to clip it based on ACTION_SCALE.\n",
    "        return None\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        self.t_step += 1\n",
    "\n",
    "        # TODO: Save experience / reward in the memory.\n",
    "        # ...\n",
    "\n",
    "        # TODO: Learn, if enough samples are available in memory.\n",
    "        if len(self.memory) > self.start_mem_size:\n",
    "            pass\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # UPDATE TWIN CRITICS.\n",
    "        with torch.no_grad():\n",
    "            # TODO: Get the next actions using the actor target.\n",
    "            actions_next = None\n",
    "\n",
    "            # TD3 trick n.3: target policy smoothing.\n",
    "            # TODO: Get the noise torch.randn_like(actions_next) times the policy_noise.\n",
    "            noise = None\n",
    "            # TODO: Clamp the noise based on noise_clamp and scale it by ACTION_SCALE\n",
    "            noise = None\n",
    "            # TODO: Add the noise to the actions_next.\n",
    "            actions_next += None\n",
    "            # TODO: Clamp the actions to be in the correct interval [-ACTION_SCALE, ACTION_SCALE].\n",
    "            actions_next = None\n",
    "\n",
    "            # TODO: Compute Q_targets_1 and Q_targets_2 via the twin critic networks using the next\n",
    "            #       states and actions.\n",
    "            Q_targets_next_1 = None\n",
    "            Q_targets_next_2 = None\n",
    "            # TODO: Pick Q_targets_next as the minimum between those two.\n",
    "            Q_targets_next = None\n",
    "            # TODO: Compute Q_targets as DDPG :)\n",
    "            Q_targets = None\n",
    "\n",
    "        # TODO: Compute the Q_values of both twin critics.\n",
    "        Q_values_1 = None\n",
    "        Q_values_2 = None\n",
    "        # TODO: Compute the critic loss as sum of mse_loss of the two critics.\n",
    "        critic_loss = None\n",
    "\n",
    "        # TODO: Minimize the critic loss.\n",
    "        # ...\n",
    "\n",
    "        # UPDATE ACTOR.\n",
    "        # TD3 trick n.2: delayed policy updates.\n",
    "        if self.t_step % self.policy_freq == 0:\n",
    "            # TODO: Get actions from the actor.\n",
    "            actions_pred = None\n",
    "            # TODO: Compute the Q_target via the first twin critic.\n",
    "            Q_values = None\n",
    "            # TODO: Compute the action loss as negative mean of the Q_target.\n",
    "            actor_loss = None\n",
    "\n",
    "            # TODO: Minimize the actor loss.\n",
    "            # ...\n",
    "\n",
    "            # TODO: Update all target networks.\n",
    "            # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with init_random(gym.make(\"Pendulum-v1\")) as env:\n",
    "    agent_td3 = AgentTD3(env.observation_space.shape[0], env.action_space.shape[0])\n",
    "    scores_td3 = train_actor_critic(env, agent_td3)\n",
    "plot_scores(scores_td3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_simulation(\"Pendulum-v1\", agent_td3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAC (Optional)\n",
    "\n",
    "The _Soft Actor Critic_ (SAC) algorithm is an off-policy algorithm similar to DDPG and TD3, that\n",
    "learns a stochastic policy instead. It adopts many of the techniques used in TD3, but it stems from\n",
    "the _Maximum Entropy Formulation_ of reinforcement learning.\n",
    "\n",
    "For an in-depth understanding of both max-ent and SAC, I suggest watching\n",
    "[Lecture 1](https://www.youtube.com/watch?v=2GwBez0D20A&list=PLwRJQ4m4UJjNymuBM9RdmB3Z9N5-0IlY0&index=2)\n",
    "of Peter Abbeel Deep RL course, and reading the OpenAI SpinningUp SAC summary.\n",
    "\n",
    "In short and as a highlight, in this formulation of RL the optimization objective is to maximize the\n",
    "expected return plus the _entropy_ of the policy $H[ \\pi (a | s_t)]$, term that intuitively balance\n",
    "exploration and exploitation:\n",
    "\n",
    "$$\n",
    "\\max_{\\pi} \\mathbb{E}\\Bigl[ \\sum_{t=0}^{T} r_t + \\beta H[ \\pi (a | s_t)] \\Bigr]\n",
    "$$\n",
    "\n",
    "The _entropy_ in fact \"measures\" how uncertain is a policy (i.e., a deterministic policy has a very\n",
    "low entropy, while a random one has high entropy).\n",
    "\n",
    "While the _critic_ network learns the action-value as usual, the _actor_ network learns the mean and\n",
    "standard deviation of a Gaussian representing the stochastic policy, using `tanh` to \"squash\" the\n",
    "values in an acceptable range, and having the `logstd` depending on the network parameters as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetworkSAC(nn.Module):\n",
    "    \"\"\"The actor network for SAC. It is provided given its technicalities.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, action_scale=ACTION_SCALE, action_bias=0.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc_mean = nn.Linear(256, action_size)\n",
    "        self.fc_logstd = nn.Linear(256, action_size)\n",
    "        self.action_scale = action_scale\n",
    "        self.action_bias = action_bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        mean = self.fc_mean(x)\n",
    "        log_std = self.fc_logstd(x)\n",
    "        log_std = torch.tanh(log_std)\n",
    "        return mean, self.adjust_log_std(log_std)\n",
    "\n",
    "    def get_action(self, x):\n",
    "        mean, log_std = self(x)\n",
    "        std = log_std.exp()\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        # Reparameterization trick: this allows gradients to flow through sampling (SAC learns a\n",
    "        # distribution and we need to learn mean / std). If we just do .sample() from a Normal\n",
    "        # distribution, we cannot backpropagate through randomness. Basically, sampling becomes:\n",
    "        #  action = (mean + std * ϵ) where ϵ ~ N(0, 1)\n",
    "        # So now action is a determinstic function of parameters (mean, std) plus fixed noise, so\n",
    "        # gradients can flow properly.\n",
    "        sample = normal.rsample()\n",
    "        output = torch.tanh(sample)\n",
    "        action = output * self.action_scale + self.action_bias\n",
    "        # Because we do tanh squashing and then action scaling above, the logprob computation needs\n",
    "        # to take that into account as variable change. Consider random variable A ~ P(A), and we\n",
    "        # transform it in U = tanh(A). Now the probability density of U say Q(U) can be written as:\n",
    "        # Q(U) = P(A) * dA/dU. Now dU/dA is (1 - tanh(A)^2). Then dA/dU = 1 / (1 - U^2). Hence:\n",
    "        # Q(U) = P(A) / (1 - U^2) that in log space becomes a subtraction.\n",
    "        log_prob = normal.log_prob(sample)\n",
    "        log_prob -= torch.log(self.action_scale * (1 - output.pow(2)) + 1e-6)\n",
    "        log_prob = log_prob.sum(-1, keepdim=True)\n",
    "        return action, log_prob\n",
    "\n",
    "    def adjust_log_std(self, log_std):\n",
    "        log_std_min, log_std_max = (-5, 2)  # From SpinUp / Denis Yarats\n",
    "        return log_std_min + 0.5 * (log_std_max - log_std_min) * (log_std + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentSAC:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size,\n",
    "        action_size,\n",
    "        start_mem_size=128,\n",
    "        gamma=0.99,\n",
    "        lr_actor=1e-4,\n",
    "        lr_critic=1e-3,\n",
    "        policy_freq=2,\n",
    "    ):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.start_mem_size = start_mem_size\n",
    "        self.gamma = gamma\n",
    "        self.policy_freq = policy_freq\n",
    "        self.t_step = 0\n",
    "\n",
    "        # TODO: Build the actor network with the ActorNetworkSAC (no target!)\n",
    "        self.actor = None\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "\n",
    "        # TODO: Build twin critics like in TD3.\n",
    "        # ...\n",
    "\n",
    "        # TODO: Make the replay buffer.\n",
    "        self.memory = None\n",
    "\n",
    "    @torch.no_grad\n",
    "    def act(self, state, add_noise=False):  # SAC doesn't really have noise, but for consistency...\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        # TODO: Convert the numpy state to a tensor.\n",
    "        state = None\n",
    "        # TODO: Get the action calling `get_action` from the actor.\n",
    "        action, _ = None\n",
    "        # TODO: Return the numpy action.\n",
    "        return None\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        self.t_step += 1\n",
    "\n",
    "        # TODO: Save experience / reward in the memory.\n",
    "        # ...\n",
    "\n",
    "        # TODO: Learn, if enough samples are available in memory.\n",
    "        if len(self.memory) > self.start_mem_size:\n",
    "            pass\n",
    "\n",
    "    def learn(self, experiences, alpha=0.2):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # UPDATE TWIN CRITICS\n",
    "        with torch.no_grad():\n",
    "            # TODO: Get next action and logprob via actor.get_action(states).\n",
    "            actions_next, log_pi_next_st = None\n",
    "            # TODO: Compute the entropy term as: alpha * log_pi_next_st\n",
    "            entropy_term = None\n",
    "\n",
    "            # TODO: Compute Q_targets like in TD3, BUT subtract entropy_term from Q_targets_next.\n",
    "            # ...\n",
    "\n",
    "        # TODO: Compute and minimize critic loss like in TD3.\n",
    "        # ...\n",
    "\n",
    "        # UPDATE ACTOR.\n",
    "        if self.t_step % self.policy_freq == 0:\n",
    "            # TODO: Get action and log via actor.get_action(...)\n",
    "            action, log_pi = None\n",
    "            # TODO: Compute the entropy term as above\n",
    "            entropy_term = None\n",
    "\n",
    "            # TODO: Use the min of the twin critic computed Q values.\n",
    "            Q_values_1 = None\n",
    "            Q_values_2 = None\n",
    "            Q_values = None\n",
    "            # TODO: actor loss is the (entropy_term - Q_values).mean()\n",
    "            actor_loss = None\n",
    "\n",
    "            # TODO: Minimize the actor loss.\n",
    "            # ...\n",
    "\n",
    "            # TODO: Update the twin critic target networks.\n",
    "            # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with init_random(gym.make(\"Pendulum-v1\")) as env:\n",
    "    agent_sac = AgentSAC(env.observation_space.shape[0], env.action_space.shape[0])\n",
    "    scores_sac = train_actor_critic(env, agent_sac)\n",
    "plot_scores(scores_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_simulation(\"Pendulum-v1\", agent_sac)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlzh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
