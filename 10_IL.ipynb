{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation Learning: Learning from Demonstrations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Reinforcement Learning (RL), we often rely on a well-defined reward function to guide our agent's\n",
    "learning process. However, designing a good reward function can be a significant challenge in many\n",
    "real-world scenarios. Think about teaching a robot to perform a complex acrobatic maneuver.\n",
    "Specifying the reward for every small movement and interaction would be incredibly difficult!\n",
    "\n",
    "Imitation Learning offers a different approach. Instead of learning from a reward signal, an IL\n",
    "agent learns by **imitating an expert**. We provide the agent with demonstrations of the desired\n",
    "behavior, and the agent's goal is to learn a policy that reproduces this behavior.\n",
    "\n",
    "In this notebook, we'll explore the fundamentals of Imitation Learning, starting with the most\n",
    "intuitive and straightforward method: **Behavioral Cloning (BC)** ðŸ¤–.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral Cloning (BC)\n",
    "\n",
    "**Behavioral Cloning** is the simplest form of Imitation Learning. The core idea is to treat IL as a\n",
    "supervised learning problem. We have a dataset of expert demonstrations, which consists of\n",
    "state-action pairs `(s, a)`. The state `s` is what the expert observed, and the action `a` is what\n",
    "the expert did in that state.\n",
    "\n",
    "Our goal is to train a policy `Ï€(s)` that takes a state `s` as input and outputs an action `a` that\n",
    "is as close as possible to the expert's action. This can be a classification problem for discrete\n",
    "actions or a regression problem for continuous actions.\n",
    "\n",
    "Beyond being a standalone method, BC is a crucial component in more complex systems. For instance,\n",
    "in algorithms like **AlphaZero**, an initial policy is trained via BC on a dataset of human expert\n",
    "games. This provides the agent with a strong, sensible starting point, allowing it to\n",
    "**\"kickstart\"** its learning process and explore more intelligently than if it started from a\n",
    "completely random policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "from util.gymnastics import gym_simulation, init_random\n",
    "from util.rl_algos import AgentSAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Environment\n",
    "\n",
    "First, let's import the necessary libraries. We'll be using `gymnasium` and `mujoco` for the\n",
    "environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Swimmer-v5\")\n",
    "env = init_random(env)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "print(f\"Environment: Swimmer-v5\")\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading the Expert and Visualizing its Performance\n",
    "\n",
    "Instead of creating a toy expert, we will load a real, pre-trained SAC agent. This gives us a\n",
    "high-quality source of demonstrations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_agent = AgentSAC(state_dim, action_dim)\n",
    "expert_agent.load(\"solution/swimmer_v5_sac_weights.pth\")\n",
    "print(\"Expert agent loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our expert performs! The cell below will render the environment and show the expert\n",
    "agent in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the expert's behavior\n",
    "gym_simulation(\"Swimmer-v5\", expert_agent, max_t=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generating Expert Demonstrations\n",
    "\n",
    "Now, we'll run the expert policy in the environment to collect a dataset of `(state, action)` pairs.\n",
    "This dataset will be the ground truth for training our behavioral cloning agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_expert_data(env, policy, num_episodes=100):\n",
    "    # TODO: Initializes states and actions to empty lists.\n",
    "    states = None\n",
    "    actions = None\n",
    "\n",
    "    for _ in tqdm(range(num_episodes), desc=\"Generating Expert Data\"):\n",
    "        # TODO: Reset the environment\n",
    "        state, _ = None\n",
    "        # TODO: Set done to False\n",
    "        done = None\n",
    "\n",
    "        while not done:\n",
    "            # TODO: Get the action from the policy. Hint: use the `act` method.\n",
    "            action = None\n",
    "            # TODO: Append state and action to the states and actions lists\n",
    "            # ...\n",
    "\n",
    "            # TODO: Get the next state and done from the environment. Hint: call `step` on the env.\n",
    "            state, _, terminated, truncated, _ = None\n",
    "            # TODO: Compute `done` as either terminated or truncated\n",
    "            done = None\n",
    "\n",
    "    return np.array(states), np.array(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_states, expert_actions = generate_expert_data(env, expert_agent)\n",
    "print(f\"Generated {len(expert_states)} state-action pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The Behavioral Cloning Agent\n",
    "\n",
    "Our BC agent will be a simple neural network that takes a state and predicts a continuous action\n",
    "vector. Since this is a regression problem, we'll use Mean Squared Error (MSE) as our loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(BCAgent, self).__init__()\n",
    "        # TODO: Create a Sequential module, with linear, relu, linear, relu, linear tanh.\n",
    "        self.network = None\n",
    "\n",
    "    def forward(self, state: torch.Tensor):\n",
    "        # TODO: Invoke the network.\n",
    "        pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, state: np.array) -> np.array:\n",
    "        # TODO: Convert state into a tensor (also unsqueeze the first dimension!)\n",
    "        state = None\n",
    "        # TODO: Invoke the network.\n",
    "        action = self.network(state)\n",
    "        # TODO: Squeeze the result and return a numpy array.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training the Agent\n",
    "\n",
    "Now we train our `BCAgent` on the expert data. This is a standard supervised learning training loop\n",
    "for regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create the BCAgent\n",
    "bc_agent = None\n",
    "\n",
    "# TODO: Convert data to PyTorch Tensors. Hint: torch.float32\n",
    "expert_states_tensor = None\n",
    "expert_actions_tensor = None\n",
    "\n",
    "# TODO: Create a DataLoader to iterate through the data below. Hint: use a TensorDataset, and build\n",
    "#       the DataLoader from it, with batch_size=256 and `shuffle` set to True\n",
    "dataset = None\n",
    "dataloader = None\n",
    "\n",
    "# TODO: Create the Adam optimizer, with learning rate 0.001\n",
    "optimizer = None\n",
    "# TODO: Get the MSE loss function. Hint: from torch nn package :)\n",
    "loss_fn = None\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 25\n",
    "print(\"Training BC Agent...\")\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for states, actions in dataloader:\n",
    "        # TODO: Classic regression training loop. Call zero_grad on the optimizer.\n",
    "        # ...\n",
    "        # TODO: Call the bc_agent.\n",
    "        predicted_actions = None\n",
    "        # TODO: Compute the loss.\n",
    "        loss = None\n",
    "        # TODO: Call backward() on the loss, and step() on the optimizer.\n",
    "        # ...\n",
    "        # TODO: Accumulate the total_loss\n",
    "        total_loss += None\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluating and Comparing Policies\n",
    "\n",
    "Now for the moment of truth! Let's see how our trained BC agent performs and compare it directly\n",
    "with the expert. We will evaluate each policy over many episodes to get a stable estimate of their\n",
    "performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, num_episodes=50):\n",
    "    # TODO: Initialize total_rewards to an empty list.\n",
    "    total_rewards = None\n",
    "    for _ in tqdm(range(num_episodes), desc=f\"Evaluating {policy.__class__.__name__}\"):\n",
    "        # TODO: Reset the environment\n",
    "        state, _ = None\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # TODO: Get the action invoking the policy. Hint: use the `act` method.\n",
    "            action = None\n",
    "            # TODO: Perform the next step in the environment\n",
    "            state, reward, terminated, truncated, _ = None\n",
    "            # TODO: Compute `done`\n",
    "            done = None\n",
    "            # TODO: Accumulate episode rewards.\n",
    "            episode_reward += None\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both policies\n",
    "expert_rewards = evaluate_policy(env, expert_agent)\n",
    "bc_rewards = evaluate_policy(env, bc_agent)\n",
    "\n",
    "# Calculate statistics\n",
    "expert_mean, expert_std = np.mean(expert_rewards), np.std(expert_rewards)\n",
    "bc_mean, bc_std = np.mean(bc_rewards), np.std(bc_rewards)\n",
    "\n",
    "print(f\"Expert Performance: {expert_mean:.2f} +/- {expert_std:.2f}\")\n",
    "print(f\"BC Agent Performance: {bc_mean:.2f} +/- {bc_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Visualization\n",
    "\n",
    "A picture is worth a thousand numbers. Let's plot the results to visually compare the two policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a nice style for the plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart for mean comparison\n",
    "axs[0].bar(\n",
    "    [\"Expert\", \"BC Agent\"],\n",
    "    [expert_mean, bc_mean],\n",
    "    yerr=[expert_std, bc_std],\n",
    "    capsize=5,\n",
    "    color=[\"#1f77b4\", \"#ff7f0e\"],\n",
    ")\n",
    "axs[0].set_title(\"Mean Episode Reward Comparison\")\n",
    "axs[0].set_ylabel(\"Mean Reward\")\n",
    "\n",
    "# Violin plot for distribution comparison\n",
    "sns.violinplot(data=[expert_rewards, bc_rewards], ax=axs[1], palette=[\"#1f77b4\", \"#ff7f0e\"])\n",
    "axs[1].set_xticklabels([\"Expert\", \"BC Agent\"])\n",
    "axs[1].set_title(\"Distribution of Episode Rewards\")\n",
    "axs[1].set_ylabel(\"Episode Reward\")\n",
    "\n",
    "plt.suptitle(\"Expert vs. Behavioral Cloning Agent Performance\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the BC agent achieves performance that is close to the expert, but likely not\n",
    "identical. This gap is the classic result of the **covariate shift** problem: small errors made by\n",
    "the BC agent lead it into states that the expert never visited, where the agent doesn't know how to\n",
    "act, causing compounding errors.\n",
    "\n",
    "You might also notice that the BC agent doesn't achieve the same peak performance of the expert, but\n",
    "surprisingly it might be doing better on average. How can that be? It's a well-known and fascinating\n",
    "phenomenon in imitation learning. The short answer is that the behavioral cloning agent isn't\n",
    "learning to be the expert; it's learning to **generalize from the expert's behavior**, and this\n",
    "often results in a more conservative / safer, but more consistently good, policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Learned Policy\n",
    "\n",
    "Finally, let's see how our cloned agent looks in action. Does its behavior resemble the expert's?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the BC agent's behavior\n",
    "gym_simulation(\"Swimmer-v5\", bc_agent, max_t=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've successfully trained a policy using Behavioral Cloning on a continuous\n",
    "control task, learning from a real expert. We saw that even this simple method can achieve\n",
    "impressive performance, closely mimicking the expert.\n",
    "\n",
    "However, we also saw that BC is often not perfect. It's just the first step. The field of Imitation\n",
    "Learning offers a rich set of tools for learning from expert data, each with its own trade-offs.\n",
    "Here are the key paradigms to be aware of:\n",
    "\n",
    "- **Behavioral Cloning (BC):** The simplest approach, treating imitation as a supervised learning\n",
    "  problem. It's easy to implement but can be brittle due to the **covariate shift** problem, where\n",
    "  small errors lead the agent into unfamiliar states.\n",
    "\n",
    "- **Dataset Aggregation (DAgger):** An interactive algorithm that directly addresses covariate\n",
    "  shift. It works by iteratively running the agent's policy, asking an expert for corrections, and\n",
    "  aggregating this new data to retrain the policy.\n",
    "\n",
    "- **Inverse Reinforcement Learning (IRL):** A more robust method that aims to learn the expert's\n",
    "  underlying **reward function** rather than just copying actions. By understanding the expert's\n",
    "  \"intent,\" the agent can generalize better to new situations.\n",
    "\n",
    "- **Generative Adversarial Imitation Learning (GAIL):** A modern, powerful technique that uses a\n",
    "  GAN-like setup. A generator (the policy) tries to create expert-like behavior to \"fool\" a\n",
    "  discriminator, which learns to distinguish between agent and expert data.\n",
    "\n",
    "I hope this gives you a solid, practical foundation for your exploration of Imitation Learning!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlzh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
